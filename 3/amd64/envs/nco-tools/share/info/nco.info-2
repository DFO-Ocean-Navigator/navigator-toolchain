This is nco.info, produced by makeinfo version 6.7 from nco.texi.

INFO-DIR-SECTION netCDF
START-INFO-DIR-ENTRY
* NCO::        User Guide for the netCDF Operator suite
END-INFO-DIR-ENTRY

This file documents NCO, a collection of utilities to manipulate and
analyze netCDF files.

   Copyright (C) 1995-2020 Charlie Zender

   This is the first edition of the 'NCO User Guide',
and is consistent with version 2 of 'texinfo.tex'.

   Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.  The
license is available online at <http://www.gnu.org/copyleft/fdl.html>

   The original author of this software, Charlie Zender, wants to
improve it with the help of your suggestions, improvements, bug-reports,
and patches.
Charlie Zender <surname at uci dot edu> (yes, my surname is zender)
3200 Croul Hall
Department of Earth System Science
University of California, Irvine
Irvine, CA 92697-3100


File: nco.info,  Node: Compression,  Next: Deflation,  Prev: Chunking,  Up: Shared features

3.30 Compression
================

Availability: 'ncbo', 'ncecat', 'nces', 'ncflint', 'ncks', 'ncpdq',
'ncra', 'ncrcat', 'ncwa'
Short options: None
Long options: '--ppc VAR1[,VAR2[,...]]=PRC',
'--precision_preserving_compression VAR1[,VAR2[,...]]=PRC',
'--quantize VAR1[,VAR2[,...]]=PRC'

   NCO implements or accesses four different compression algorithms, the
standard lossless DEFLATE algorithm and three lossy compression
algorithms.  All four algorithms reduce the on-disk size of a dataset
while sacrificing no (lossless) or a tolerable amount (lossy) of
precision.  First, NCO can access the lossless DEFLATE algorithm, a
combination of Lempel-Ziv encoding and Huffman coding, algorithm on any
netCDF4 dataset (*note Deflation::).  Because it is lossless, this
algorithm re-inflates deflated data to their full original precision.
This algorithm is accessed via the HDF5 library layer (which itself
calls the 'zlib' library also used by 'gzip'), and is unavailable with
netCDF3.

* Menu:

* Linear Packing::
* Precision-Preserving Compression::


File: nco.info,  Node: Linear Packing,  Next: Precision-Preserving Compression,  Prev: Compression,  Up: Compression

3.30.1 Linear Packing
---------------------

The three lossy compression algorithms are Linear Packing (*note Packed
data::), and two precision-preserving algorithms.  Linear packing
quantizes data of a higher precision type into a lower precision type
(often 'NC_SHORT') that thus stores a fewer (though constant) number of
bytes per value.  Linearly packed data unpacks into a (much) smaller
dynamic range than the floating-point data can represent.  The
type-conversion and reduced dynamic range of the data allows packing to
eliminate bits typically used to store an exponent, thus improving its
packing efficiency.  Packed data also can also be deflated for
additional space savings.

   A limitation of linear packing is that unpacking data stored as
integers into the linear range defined by 'scale_factor' and
'add_offset' rapidly loses precision outside of a narrow range of
floating-point values.  Variables packed as 'NC_SHORT', for example, can
represent only about 64000 discrete values in the range
-32768*scale_factor+add_offset to 32767*scale_factor+add_offset.  The
precision of packed data equals the value of 'scale_factor', and
'scale_factor' is usually chosen to span the range of valid data, not to
represent the intrinsic precision of the variable.  In other words, the
precision of packed data cannot be specified in advance because it
depends on the range of values to quantize.


File: nco.info,  Node: Precision-Preserving Compression,  Prev: Linear Packing,  Up: Compression

3.30.2 Precision-Preserving Compression
---------------------------------------

NCO implemented the final two lossy compression algorithms in version
4.4.8 (February, 2015).  These are both "Precision-Preserving
Compression" (PPC) algorithms and since standard terminology for
precision is remarkably imprecise, so is our nomenclature.  The
operational definition of "significant digit" in our precision
preserving algorithms is that the exact value, before rounding or
quantization, is within one-half the value of the decimal place occupied
by the "Least Significant Digit" (LSD) of the rounded value.  For
example, the value pi = 3.14 correctly represents the exact mathematical
constant PI to three significant digits because the LSD of the rounded
value (i.e., 4) is in the one-hundredths digit place, and the difference
between the exact value and the rounded value is less than one-half of
one one-hundredth, i.e., (3.14159265358979323844 - 3.14 = 0.00159 <
0.005).

   One PPC algorithm preserves the specified total "Number of Signifcant
Digits" (NSD) of the value.  For example there is only one significant
digit in the weight of most "eight-hundred pound gorillas" that you will
encounter, i.e., so NSD=1.  This is the most straightforward measure of
precision, and thus NSD is the default PPC algorithm.

   The other PPC algorithm preserves the number of "Decimal Significant
Digits" (DSD), i.e., the number of significant digits following
(positive, by convention) or preceding (negative) the decimal point.
For example, '0.008' and '800' have, respectively, three and negative
two digits digits following the decimal point, corresponding to DSD=3
and DSD=-2.

   The only justifiable NSD for a given value depends on intrinsic
accuracy and error characteristics of the model or measurements, and not
on the units with which the value is stored.  The appropriate DSD for a
given value depends on these intrinsic characteristics and, in addition,
the units of storage.  This is the fundamental difference between the
NSD and DSD approaches.  The eight-hundred pound gorilla always has
NSD=1 regardless of whether the value is stored in pounds or in some
other unit.  DSD corresponding to this weight is DSD=-2 if the value is
stored in pounds, DSD=4 if stored in megapounds.

   Users may wish to express the precision to be preserved as either NSD
or DSD.  Invoke PPC with the long option '--ppc var=prc', or give the
same arguments to the synonyms '--precision_preserving_compression', or
to '--quantize'.  Here VAR is the variable to quantize, and PRC is its
precision.  The option '--ppc' (and its long option equivalents such as
'--quantize') indicates the argument syntax will be KEY=VAL.  As such,
'--ppc' and its synonyms are indicator options that accept arguments
supplied one-by-one like '--ppc KEY1=VAL1 --ppc KEY2=VAL2', or
aggregated together in multi-argument format like '--ppc
KEY1=VAL1#KEY2=VAL2' (*note Multi-arguments::).  The default algorithm
assumes PRC specifies NSD precision, e.g., 'T=2' means NSD=2.  Prepend
PRC with a decimal point to specify DSD precision, e.g., 'T=.2' means
DSD=2.  NSD precision must be specified as a positive integer.  DSD
precision may be a positive or negative integer; and is specified as the
negative base 10 logarithm of the desired precision, in accord with
common usage.  For example, specifying 'T=.3' or 'T=.-2' tells the DSD
algorithm to store only enough bits to preserve the value of T rounded
to the nearest thousandth or hundred, respectively.

   Setting VAR to 'default' has the special meaning of applying the
associated NSD or DSD algorithm to all floating point variables except
coordinate variables.  Variables _not affected_ by 'default' include
integer and non-numeric atomic types, coordinates, and variables
mentioned in the 'bounds', 'climatology', or 'coordinates' attribute of
any variable.  NCO applies PPC to coordinate variables only if those
variables are explicitly specified (i.e., not with the 'default=PRC'
mechanism.  NCO applies PPC to integer-type variables only if those
variables are explicitly specified (i.e., not with the 'default=PRC',
and only if the DSD algorithm is invoked with a negative PRC.  To
prevent PPC from applying to certain non-coordinate variables (e.g.,
'gridcell_area' or 'gaussian_weight'), explicitly specify a precision
exceeding 7 (for 'NC_FLOAT') or 15 (for 'NC_DOUBLE') for those
variables.  Since these are the maximum representable precisions in
decimal digits, NCO _turns-off_ PPC (i.e., does nothing) when more
precision is requested.

   The time-penalty for compressing and uncompressing data varies
according to the algorithm.  The Number of Significant Digit (NSD)
algorithm quantizes by bitmasking, and employs no floating-point math.
The Decimal Significant Digit (DSD) algorithm quantizes by rounding,
which does require floating-point math.  Hence NSD is likely faster than
DSD, though the difference has not been measured.  NSD creates a bitmask
to alter the "significand" of IEEE 754 floating-point data.  The bitmask
is one for all bits to be retained and zero or one for all bits to be
ignored.  The algorithm assumes that the number of binary digits (i.e.,
bits) necessary to represent a single base-10 digit is ln(10)/ln(2) =
3.32.  The exact numbers of bits NBIT retained for single and double
precision values are ceil(3.32*NSD)+1 and ceil(3.32*NSD)+2,
respectively.  Once these reach 23 and 53, respectively, bitmasking is
completely ineffective.  This occurs at NSD=6.3 and 15.4, respectively.

   The DSD algorithm, by contrast, uses rounding to remove undesired
precision.  The rounding (1) zeroes the greatest number of significand
bits consistent with the desired precision.

   To demonstrate the change in IEEE representation caused by PPC
rounding algorithms, consider again the case of PI, represented as an
'NC_FLOAT'.  The IEEE 754 single precision representations of the exact
value (3.141592...), the value with only three significant digits
treated as exact (3.140000...), and the value as stored (3.140625) after
PPC-rounding with either the NSD (PRC=3) or DSD (PRC=2) algorithm are,
respectively,
     S Exponent  Fraction (Significand)   Decimal    Notes
     0 100000001 0010010000111111011011 # 3.14159265 Exact
     0 100000001 0010001111010111000011 # 3.14000000
     0 100000001 0010010000000000000000 # 3.14062500 NSD = 3
     0 100000001 0010010000000000000000 # 3.14062500 DSD = 2
   The string of trailing zero-bits in the rounded values facilitates
byte-stream compression.  Note that the NSD and DSD algorithms do not
always produce results that are bit-for-bit identical, although they do
in this particular case.

   Reducing the preserved precision of NSD-rounding produces
increasingly long strings of identical-bits amenable to compression:
     S Exponent  Fraction (Significand)   Decimal    Notes
     0 100000001 0010010000111111011011 # 3.14159265 Exact
     0 100000001 0010010000111111011011 # 3.14159265 NSD = 8
     0 100000001 0010010000111111011010 # 3.14159262 NSD = 7
     0 100000001 0010010000111111011000 # 3.14159203 NSD = 6
     0 100000001 0010010000111111000000 # 3.14158630 NSD = 5
     0 100000001 0010010000111100000000 # 3.14154053 NSD = 4
     0 100000001 0010010000000000000000 # 3.14062500 NSD = 3
     0 100000001 0010010000000000000000 # 3.14062500 NSD = 2
     0 100000001 0010000000000000000000 # 3.12500000 NSD = 1
   The consumption of about 3 bits per digit of base-10 precision is
evident, as is the coincidence of a quantized value that greatly exceeds
the mandated precision for NSD = 2.  Although the NSD algorithm
generally masks some bits for all NSD <= 7 (for 'NC_FLOAT'), compression
algorithms like DEFLATE may need byte-size-or-greater (i.e., at least
eight-bit) bit patterns before their algorithms can take advantage of of
encoding such patterns for compression.  Do not expect significantly
enhanced compression from NSD > 5 (for 'NC_FLOAT') or NSD > 14 (for
'NC_DOUBLE').  Clearly values stored as 'NC_DOUBLE' (i.e., eight-bytes)
are susceptible to much greater compression than 'NC_FLOAT' for a given
precision because their significands explicitly contain 53 bits rather
than 23 bits.

   Maintaining non-biased statistical properties during lossy
compression requires special attention.  The DSD algorithm uses
'rint()', which rounds toward the nearest even integer.  Thus DSD has no
systematic bias.  However, the NSD algorithm uses a bitmask technique
susceptible to statistical bias.  Zeroing all non-significant bits is
guaranteed to produce numbers quantized to the specified tolerance,
i.e., half of the decimal value of the position occupied by the LSD.
However, always zeroing the non-significant bits results in quantized
numbers that never exceed the exact number.  This would produce a
negative bias in statistical quantities (e.g., the average) subsequently
derived from the quantized numbers.  To avoid this bias, our NSD
implementation rounds non-significant bits down (to zero) or up (to one)
in an alternating fashion when processing array data.  In general, the
first element is rounded down, the second up, and so on.  This results
in a mean bias quite close to zero.  The only exception is that the
floating-point value of zero is never quantized upwards.  For
simplicity, NSD always rounds scalars downwards.

   Although NSD or DSD are different algorithms under the hood, they
both replace the (unwanted) least siginificant bits of the IEEE
significand with a string of consecutive zeroes.  Byte-stream
compression techniques, such as the 'gzip' DEFLATE algorithm compression
available in HDF5, always compress zero-strings more efficiently than
random digits.  The net result is netCDF files that utilize compression
can be significantly reduced in size.  This feature only works when the
data are compressed, either internally (by netCDF) or externally (by
another user-supplied mechanism).  It is most straightfoward to compress
data internally using the built-in compression and decompression
supported by netCDF4.  For convenience, NCO automatically activates
file-wide Lempel-Ziv deflation (*note Deflation::) level one (i.e., '-L
1') when PPC is invoked on any variable in a netCDF4 output file.  This
makes PPC easier to use effectively, since the user need not explicitly
specify deflation.  Any explicitly specified deflation (including no
deflation, '-L 0') will override the PPC deflation default.  If the
output file is a netCDF3 format, NCO will emit a message suggesting
internal netCDF4 or external netCDF3 compression.  netCDF3 files
compressed by an external utility such as 'gzip' accrue approximately
the same benefits (shrinkage) as netCDF4, although with netCDF3 the user
or provider must uncompress (e.g., 'gunzip') the file before accessing
the data.  There is no benefit to rounding numbers and storing them in
netCDF3 files unless such custom compression/decompression is employed.
Without that, one may as well maintain the undesired precision.

   The user accesses PPC through a single switch, '--ppc', repeated as
many times as necessary.  To apply the NSD algorithm to variable U use,
e.g.,
     ncks -7 --ppc u=2 in.nc out.nc
   The output file will preserve only two significant digits of U.  The
options '-4' or '-7' ensure a netCDF4-format output (regardless of the
input file format) to support internal compression.  It is recommended
though not required to write netCDF4 files after PPC.  For clarity the
'-4/-7' switches are omitted in subsequent examples.  NCO attaches
attributes that indicate the algorithm used and degree of precision
retained for each variable affected by PPC.  The NSD and DSD algorithms
store the attributes 'number_of_significant_digits' and
'least_significant_digit' (2), respectively.

   It is safe to attempt PPC on input that has already been rounded.
Variables can be made rounder, not sharper, i.e., variables cannot be
"un-rounded".  Thus PPC attempted on an input variable with an existing
PPC attribute proceeds only if the new rounding level exceeds the old,
otherwise no new rounding occurs (i.e., a "no-op"), and the original PPC
attribute is retained rather than replaced with the newer value of PRC.

   To request, say, five significant digits (NSD=5) for all fields,
except, say, wind speeds which are only known to integer values (DSD=0)
in the supplied units, requires '--ppc' twice:
     ncks -4 --ppc default=5 --ppc u,v=.0 in.nc out.nc
   To preserve five digits in all variables except coordinate variables
and U and V, use the 'default' option and separately specify the
exceptions:
     ncks --ppc default=5 --ppc u,v=20 in.nc out.nc
   The '--ppc' option may be specified any number of times to support
varying precision types and levels, and each option may aggregate all
the variables with the same precision
     ncks --ppc p,w,z=5 --ppc q,RH=4 --ppc T,u,v=3 in.nc out.nc
     ncks --ppc p,w,z=5#q,RH=4#T,u,v=3 in.nc out.nc # Multi-argument format
   Any VAR argument may be a regular expression.  This simplifies
generating lists of related variables:
     ncks --ppc Q.?=5 --ppc FS.?,FL.?=4 --ppc RH=.3 in.nc out.nc
     ncks --ppc Q.?=5#FS.?,FL.?=4#RH=.3 in.nc out.nc # Multi-argument format
   Although PPC-rounding instantly reduces data precision, on-disk
storage reduction only occurs once the data are compressed.

   How can one be sure the lossy data are sufficiently precise?  PPC
preserves all significant digits of every value.  The DSD algorithm uses
floating-point math to round each value optimally so that it has the
maximum number of zeroed bits that preserve the specified precision.
The NSD algorithm uses a theoretical approach (3.2 bits per base-10
digit), tuned and tested to ensure the _worst_ case quantization error
is less than half the value of the minimum increment in the least
significant digit.

   _Note for Info users_: The definition of error metrics relies heavily
on mathematical expressions which cannot be easily represented in Info.
_See the printed manual (./nco.pdf) for much more detailed and complete
documentation of this subject._

   All three metrics are expressed in terms of the fraction of the ten's
place occupied by the LSD.  If the LSD is the hundreds digit or the
thousandths digit, then the metrics are fractions of 100, or of 1/100,
respectively.  PPC algorithms should produce maximum absolute errors no
greater than 0.5 in these units.  If the LSD is the hundreds digit, then
quantized versions of true values will be within fifty of the true
value.  It is much easier to satisfy this tolerance for a true value
of 100 (only 50% accuracy required) than for 999 (5% accuracy required).
Thus the minimum accuracy guaranteed for NSD=1 ranges from 5-50%.  For
this reason, the best and worst cast performance usually occurs for true
values whose LSD value is close to one and nine, respectively.  Of
course most users prefer PRC > 1 because accuracies increase
exponentially with PRC.  Continuing the previous example to PRC=2,
quantized versions of true values from 1000-9999 will also be within 50
of the true value, i.e., have accuracies from 0.5-5%.  In other words,
only two significant digits are necessary to guarantee better than 5%
accuracy in quantization.  We recommend that dataset producers and users
consider quantizing datasets with NSD=3.  This guarantees accuracy of
0.05-0.5% for individual values.  Statistics computed from ensembles of
quantized values will, assuming the mean error EMEAN is small, have much
better accuracy than 0.5%.  This accuracy is the most that can be
justified for many applications.

   To demonstrate these principles we conduct error analyses on an
artificial, reproducible dataset, and on an actual dataset of
observational analysis values.  (3) The table summarizes quantization
accuracy based on the three metrics.
'NSD'
     Number of Significant Digits.
'Emabs'
     Maximum absolute error.
'Emebs'
     Mean absolute error.
'Emean'
     Mean error.

     Artificial Data: N=1000000 values in [1.0,2.0) in steps of 1.0e-6
     Single-Precision        Double-Precision   Single-Precision
     NSD Emabs Emebs Emean   Emabs Emebs Emean  DSD Emabs Emebs Emean
      1  0.31  0.11  4.1e-4  0.31  0.11  4.0e-4  1  0.30  0.11 -8.1e-4
      2  0.39  0.14  6.8e-5  0.39  0.14  5.5e-5  2  0.39  0.14 -1.3e-4
      3  0.49  0.17  1.0e-6  0.49  0.17 -5.5e-7  3  0.49  0.17 -2.0e-5
      4  0.30  0.11  3.2e-7  0.30  0.11 -6.1e-6  4  0.30  0.11  5.1e-8
      5  0.37  0.13  3.1e-7  0.38  0.13 -5.6e-6  5  0.38  0.13  2.6e-6
      6  0.36  0.12 -4.4e-7  0.48  0.17 -4.1e-7  6  0.48  0.17  7.2e-6
      7  0.00  0.00  0.0     0.30  0.10  1.5e-7  7  0.00  0.00  0.0

     Observational Analysis: N=13934592 values MERRA Temperature 20130601
     Single-Precision
     NSD Emabs Emebs Emean
      1  0.31  0.11  2.4e-3
      2  0.39  0.14  3.8e-4
      3  0.49  0.17 -9.6e-5
      4  0.30  0.11  2.3e-3
      5  0.37  0.13  2.2e-3
      6  0.36  0.13  1.7e-2
      7  0.00  0.00  0.0
   All results show that PPC quantization performs as expected.
Absolute maximum errors EMABS < 0.5 for all PRC.  For 1 <= PRC <= 6,
quantization results in comparable maximum absolute and mean absolute
errors EMABS and EMEBS, respectively.  Mean errors EMEAN are orders of
magnitude smaller because quantization produces over- and
under-estimated values in balance.  When PRC=7, quantization of
single-precision values is ineffective, because all available bits are
used to represent the maximum precision of seven digits.  The maximum
and mean absolute errors EMABS and EMEBS are nearly identical across
algorithms, precisions, and dataset types.  This is consistent with both
the artificial data and empirical data being random, and thus exercising
equally strengths and weaknesses of the algorithms over the course of
millions of input values.  We generated artificial arrays with many
different starting values and interval spacing and all gave
qualitatively similar results.  The results presented are the worst
obtained.

   The artificial data has much smaller mean error EMEAN than the
observational analysis.  The reason why is unclear.  It may be because
the temperature field is concentrated in particular ranges of values
(and associated quantization errors) prevalent on Earth, e.g., 200 < T <
320.  It is worth noting that the mean error EMEAN < 0.01 for 1 <= PRC <
6, and that EMEAN is typically at least two or more orders of magnitude
less than EMABS.  Thus quantized values with precisions as low as PRC=1
still yield highly significant statistics by contemporary scientific
standards.

   Testing shows that PPC quantization enhances compression of typical
climate datasets.  The degree of enhancement depends, of course, on the
required precision.  Model results are often computed as 'NC_DOUBLE'
then archived as 'NC_FLOAT' to save space.  This table summarizes the
performance of lossless and lossy compression on two typical, or at
least random, netCDF data files.  The files were taken from
representative model-simulated and satellite-retrieved datasets.  Only
floating-point data were compressed.  No attempt was made to compress
integer-type variables as they occupy an insignificant fraction of every
dataset.  The columns are
'Type'
     File-type: 'N3' for netCDF 'CLASSIC', 'N4' for 'NETCDF4', 'N7' for
     'NETCDF4_CLASSIC' (which comprises netCDF3 data types and
     structures with netCDF4 storage features like compression), 'H4'
     for HDF4, and 'H5' for HDF5.  'N4/7' means results apply to both
     'N4' and 'N7' filetypes.
'LLC'
     Type of lossless compression employed, if any.  Bare numbers refer
     to the strength of the DEFLATE algorithm employed internally by
     netCDF4/HDF5, while numbers prefixed with 'B' refer to the block
     size employed by the Burrows-Wheeler algorithm in 'bzip2'.
'PPC'
     Number of significant digits retained by the precision-preserving
     compression NSD algorithm.
'Pck'
     'Y' if the default 'ncpdq' packing algorithm (convert
     floating-point types to 'NC_SHORT') was employed.
'Size'
     Resulting filesize in MB.
'%'
     Compression ratio, i.e., resulting filesize relative to original
     size, in percent.  In some cases the original files is already
     losslessly compressed.  The compression ratios reported are
     relative to the size of the original file as distributed, not as
     optimally losslessly compressed.
   A dash ('-') indicates the associated compression feature was not
employed.
     # dstmch90_clm.nc
     Type LLC PPC Pck  Size   %    Flags and Notes
       N3   -   -  -   34.7 100.0  Original is not compressed
       N3  B1   -  -   28.9  83.2  bzip2 -1
       N3  B9   -  -   29.3  84.4  bzip2 -9
       N7   -   -  -   35.0 101.0
       N7   1   -  -   28.2  81.3  -L 1
       N7   9   -  -   28.0  80.8  -L 9
       N7   -   -  Y   17.6  50.9  ncpdq -L 0
       N7   1   -  Y    7.9  22.8  ncpdq -L 1
       N7   1   7  -   28.2  81.3  --ppc default=7
       N7   1   6  -   27.9  80.6  --ppc default=6
       N7   1   5  -   25.9  74.6  --ppc default=5
       N7   1   4  -   22.3  64.3  --ppc default=4
       N7   1   3  -   18.9  54.6  --ppc default=3
       N7   1   2  -   14.5  43.2  --ppc default=2
       N7   1   1  -   10.0  29.0  --ppc default=1

     # b1850c5cn_doe_polar_merged_0_cesm1_2_0_HD+MAM4+tun2b.hp.e003.cam.h0.0001-01.nc
     Type LLC PPC Pck  Size   %    Flags and Notes
       N3   -   -  -  119.8 100.0  Original is not compressed
       N3  B1   -  -   84.2  70.3  bzip2 -1
       N3  B9   -  -   84.8  70.9  bzip2 -9
       N7   -   -  -  120.5 100.7
       N7   1   -  -   82.6  69.0  -L 1
       N7   9   -  -   82.1  68.6  -L 9
       N7   -   -  Y   60.7  50.7  ncpdq -L 0
       N7   1   -  Y   26.0  21.8  ncpdq -L 1
       N7   1   7  -   82.6  69.0  --ppc default=7
       N7   1   6  -   81.9  68.4  --ppc default=6
       N7   1   5  -   77.2  64.5  --ppc default=5
       N7   1   4  -   69.0  57.6  --ppc default=4
       N7   1   3  -   59.3  49.5  --ppc default=3
       N7   1   2  -   49.5  41.3  --ppc default=2
       N7   1   1  -   38.2  31.9  --ppc default=1

     # MERRA300.prod.assim.inst3_3d_asm_Cp.20130601.hdf
     Type LLC PPC Pck  Size   %    Flags and Notes
       H4   5   -  -  244.3 100.0  Original is compressed
       H4  B1   -  -  244.7 100.1  bzip2 -1
       N4   5   -  -  214.5  87.8
       N7   5   -  -  210.6  86.2
       N4  B1   -  -  215.4  88.2  bzip2 -1
       N4  B9   -  -  214.8  87.9  bzip2 -9
       N3   -   -  -  617.1 252.6
     N4/7   -   -  -  694.0 284.0  -L 0
     N4/7   1   -  -  223.2  91.3  -L 1
     N4/7   9   -  -  207.3  84.9  -L 9
     N4/7   -   -  Y  347.1 142.1  ncpdq -L 0
     N4/7   1   -  Y  133.6  54.7  ncpdq -L 1
     N4/7   1   7  -  223.1  91.3  --ppc default=7
     N4/7   1   6  -  225.1  92.1  --ppc default=6
     N4/7   1   5  -  221.4  90.6  --ppc default=5
     N4/7   1   4  -  201.4  82.4  --ppc default=4
     N4/7   1   3  -  185.3  75.9  --ppc default=3
     N4/7   1   2  -  150.0  61.4  --ppc default=2
     N4/7   1   1  -  100.8  41.3  --ppc default=1

     # OMI-Aura_L2-OMIAuraSO2_2012m1222-o44888_v01-00-2014m0107t114720.h5
     Type LLC PPC Pck  Size   %    Flags and Notes
       H5   5   -  -   29.5 100.0  Original is compressed
       H5  B1   -  -   29.3  99.6  bzip2 -1
       N4   5   -  -   29.5 100.0
       N4  B1   -  -   29.3  99.6  bzip2 -1
       N4  B9   -  -   29.3  99.4  bzip2 -9
       N4   -   -  -   50.7 172.3  -L 0
       N4   1   -  -   29.8 101.3  -L 1
       N4   9   -  -   29.4  99.8  -L 9
       N4   -   -  Y   27.7  94.0  ncpdq -L 0
       N4   1   -  Y   12.9  43.9  ncpdq -L 1
       N4   1   7  -   29.7 100.7  --ppc default=7
       N4   1   6  -   29.7 100.8  --ppc default=6
       N4   1   5  -   27.3  92.8  --ppc default=5
       N4   1   4  -   23.8  80.7  --ppc default=4
       N4   1   3  -   20.3  69.0  --ppc default=3
       N4   1   2  -   15.1  51.2  --ppc default=2
       N4   1   1  -    9.9  33.6  --ppc default=1

   A selective, per-variable approach to PPC yields the best balance of
precision and compression yet requires the dataset producer to
understand the intrinsic precision of each variable.  Such a
specification for a GCM dataset might look like this (using names for
the NCAR CAM model):
     # Be conservative on non-explicit quantities, so default=5
     # Some quantities deserve four significant digits
     # Many quantities, such as aerosol optical depths and burdens, are
     # highly uncertain and only useful to three significant digits.
     ncks -7 -O \
     --ppc default=5 \
     --ppc AN.?,AQ.?=4 \
     --ppc AER.?,AOD.?,ARE.?,AW.?,BURDEN.?=3 \
     ncar_cam.nc ~/foo.nc

   ---------- Footnotes ----------

   (1) Rounding is performed by the internal math library 'rint()'
family of functions that were standardized in C99.  The exact alorithm
employed is VAL := rint(SCALE*VAL)/SCALE where SCALE is the nearest
power of 2 that exceeds 10**PRC, and the inverse of SCALE is used when
PRC < 0.  For PPC = 3 or PPC = -2, for example, we have SCALE = 1024 and
SCALE = 1/128.

   (2) A suggestion by Rich Signell and the 'nc3tonc4' tool by Jeff
Whitaker inspired NCO to implement PPC.  Note that NCO implements a
different DSD algorithm than 'nc3tonc4', and produces slightly different
(not bit-for-bit) though self-consistent and equivalent results.
'nc3tonc4' records the precision of its DSD algorithm in the attribute
'least_significant_digit' and NCO does the same for consistency.  The
Unidata blog here
(http://www.unidata.ucar.edu/blogs/developer/en/entry/compression_by_bit_shaving)
also shows how to compress IEEE floating-point data by zeroing
insignificant bits.  The author, John Caron, writes that the technique
has been called "bit-shaving".  We call the algorithm of always
rounding-up "bit-setting".  And we named the algorithm produced by
alternately rounding up and down (with a few other bells and whistles)
"bit-grooming".  Imagine orthogonally raking an already-groomed Japanese
rock garden.  The criss-crossing tracks increase the pattern's entropy,
and this entropy produces self-compensating instead of accumulating
errors during statistical operations.

   (3) The artificial dataset employed is one million evenly spaced
values from 1.0-2.0.  The analysis data are N=13934592 values of the
temperature field from the NASA MERRA analysis of 20130601.


File: nco.info,  Node: Deflation,  Next: MD5 digests,  Prev: Compression,  Up: Shared features

3.31 Deflation
==============

Availability: 'ncap2', 'ncbo', 'ncclimo', 'nces', 'ncecat', 'ncflint',
'ncks', 'ncpdq', 'ncra', 'ncrcat', 'ncremap', 'ncwa'
Short options: '-L'
Long options: '--dfl_lvl', '--deflate'

   All NCO operators that define variables support the netCDF4 feature
of storing variables compressed with the lossless DEFLATE compression
algorithm.  DEFLATE combines the Lempel-Ziv encoding with Huffman
coding.  The specific version used by netCDF4/HDF5 is that implemented
in the 'zlib' library used by 'gzip'.  Activate deflation with the '-L
DFL_LVL' short option (or with the same argument to the '--dfl_lvl' or
'--deflate' long options).  Specify the deflation level DFL_LVL on a
scale from no deflation (DFL_LVL = 0) to maximum deflation (DFL_LVL =
9).  Under the hood, this selects the compression blocksize.  Minimal
deflation (DFL_LVL = 1) achieves considerable storage compression with
little time penalty.  Higher deflation levels require more time for
compression.  File sizes resulting from minimal (DFL_LVL = 1) and
maximal (DFL_LVL = 9) deflation levels typically differ by less than 10%
in size.

   To compress an entire file using deflation, use
     ncks -4 -L 0 in.nc out.nc # No deflation (fast, no time penalty)
     ncks -4 -L 1 in.nc out.nc # Minimal deflation (little time penalty)
     ncks -4 -L 9 in.nc out.nc # Maximal deflation (much slower)

   Unscientific testing shows that deflation compresses typical climate
datasets by 30-60%.  Packing, a lossy compression technique available
for all netCDF files (see *note Packed data::), can easily compress
files by 50%.  Packed data may be deflated to squeeze datasets by about
80%:
     ncks  -4 -L 1 in.nc out.nc # Minimal deflation (~30-60% compression)
     ncks  -4 -L 9 in.nc out.nc # Maximal deflation (~31-63% compression)
     ncpdq         in.nc out.nc # Standard packing  (~50% compression)
     ncpdq -4 -L 9 in.nc out.nc # Deflated packing  (~80% compression)
   'ncks' prints deflation parameters, if any, to screen (*note ncks
netCDF Kitchen Sink::).


File: nco.info,  Node: MD5 digests,  Next: Buffer sizes,  Prev: Deflation,  Up: Shared features

3.32 MD5 digests
================

Availability: 'ncecat', 'ncks', 'ncrcat'
Short options:
Long options: '--md5_dgs', '--md5_digest', '--md5_wrt_att',
'--md5_write_attribute'

   As of NCO version 4.1.0 (April, 2012), NCO supports data integrity
verification using the MD5 digest algorithm.  This support is currently
implemented in 'ncks' and in the multi-file concatenators 'ncecat' and
'ncrcat'.  Activate it with the '--md5_dgs' or '--md5_digest' long
options.  As of NCO version 4.3.3 (July, 2013), NCO will write the MD5
digest of each variable as an 'NC_CHAR' attribute named 'MD5'.  This
support is currently implemented in 'ncks' and in the multi-file
concatenators 'ncecat' and 'ncrcat'.  Activate it with the
'--md5_wrt_att' or '--md5_write_attribute' long options.

   The behavior and verbosity of the MD5 digest is operator-dependent.
MD5 digests may be activated in both 'ncks' invocation types, the
one-filename argument mode for printing sub-setted and hyperslabbed
data, and the two-filename argument mode for copying that data to a new
file.  Both modes will incur minor overhead from performing the hash
algorithm for each variable read, and each variable written will have an
additional attribute named 'MD5'.  When activating MD5 digests with
'ncks' it is assumed that the user wishes to print the digest of every
variable when the debugging level exceeds one.

   'ncks' displays an MD5 digest as a 32-character hexadecimal string in
which each two characters represent one byte of the 16-byte digest:
     > ncks --trd -D 2 -C --md5 -v md5_a,md5_abc ~/nco/data/in.nc
     ...
     ncks: INFO MD5(md5_a) = 0cc175b9c0f1b6a831c399e269772661
     md5_a = 'a'
     ncks: INFO MD5(md5_abc) = 900150983cd24fb0d6963f7d28e17f72
     lev[0]=100 md5_abc[0--2]='abc'
     > ncks --trd -D 2 -C -d lev,0 --md5 -v md5_a,md5_abc ~/nco/data/in.nc
     ...
     ncks: INFO MD5(md5_a) = 0cc175b9c0f1b6a831c399e269772661
     md5_a = 'a'
     ncks: INFO MD5(md5_abc) = 0cc175b9c0f1b6a831c399e269772661
     lev[0]=100 md5_abc[0--0]='a'
   In fact these examples demonstrate the validity of the hash algorithm
since the MD5 hashes of the strings "a" and "abc" are widely known.  The
second example shows that the hyperslab of variable 'md5_abc' (= "abc")
consisting of only its first letter (= "a") has the same hash as the
variable 'md5_a' ("a").  This illustrates that MD5 digests act only on
variable data, not on metadata.

   When activating MD5 digests with 'ncecat' or 'ncrcat' it is assumed
that the user wishes to verify that every variable written to disk has
the same MD5 digest as when it is subsequently read from disk.  This
incurs the major additional overhead of reading in each variable after
it is written and performing the hash algorithm again on that to compare
to the original hash.  Moreover, it is assumed that such operations are
generally done in "production mode" where the user is not interested in
actually examining the digests herself.  The digests proceed silently
unless the debugging level exceeds three:
     > ncecat -O -D 4 --md5 -p ~/nco/data in.nc in.nc ~/foo.nc | grep MD5
     ...
     ncecat: INFO MD5(wnd_spd) = bec190dd944f2ce2794a7a4abf224b28
     ncecat: INFO MD5 digests of RAM and disk contents for wnd_spd agree
     > ncrcat -O -D 4 --md5 -p ~/nco/data in.nc in.nc ~/foo.nc | grep MD5
     ...
     ncrcat: INFO MD5(wnd_spd) = 74699bb0a72b7f16456badb2c995f1a1
     ncrcat: INFO MD5 digests of RAM and disk contents for wnd_spd agree
   Regardless of the debugging level, an error is returned when the
digests of the variable read from the source file and from the output
file disagree.

   These rules may further evolve as NCO pays more attention to data
integrity.  We welcome feedback and suggestions from users.


File: nco.info,  Node: Buffer sizes,  Next: RAM disks,  Prev: MD5 digests,  Up: Shared features

3.33 Buffer sizes
=================

Availability: All operators
Short options:
Long options: '--bfr_sz_hnt', '--buffer_size_hint'

   As of NCO version 4.2.0 (May, 2012), NCO allows the user to request
specific buffer sizes to allocate for reading and writing files.  This
buffer size determines how many system calls the netCDF layer must
invoke to read and write files.  By default, netCDF uses the preferred
I/O block size returned as the 'st_blksize' member of the 'stat'
structure returned by the 'stat()' system call (1).  Otherwise, netCDF
uses twice the system pagesize.  Larger sizes can increase access speed
by reducing the number of system calls netCDF makes to read/write data
from/to disk.  Because netCDF cannot guarantee the buffer size request
will be met, the actual buffer size granted by the system is printed as
an INFO statement.
     # Request 2 MB file buffer instead of default 8 kB buffer
     > ncks -O -D 3 --bfr_sz=2097152 ~/nco/data/in.nc ~/foo.nc
     ...
     ncks: INFO nc__open() will request file buffer size = 2097152 bytes
     ncks: INFO nc__open() opened file with buffer size = 2097152 bytes
     ...

   ---------- Footnotes ----------

   (1) On modern Linux systems the block size defaults to 8192 B. The
GLADE filesystem at NCAR has a block size of 512 kB.


File: nco.info,  Node: RAM disks,  Next: Packed data,  Prev: Buffer sizes,  Up: Shared features

3.34 RAM disks
==============

Availability: All operators
Short options:
Long options: '--ram_all', '--create_ram', '--open_ram',
'--diskless_all'

   As of NCO version 4.2.1 (August, 2012), NCO supports the use of
diskless files, aka RAM disks, for file access and creation.  Two
independent switches, '--open_ram' and '--create_ram', control this
feature.  Before describing the specifics of these switches, we describe
why many NCO operations will not benefit from them.  Essentially,
reading/writing from/to RAM rather than disk only hastens the task when
reads/writes to disk are avoided.  Most NCO operations are simple enough
that they require a single read-from/write-to disk for every block of
input/output.  Diskless access does not change this, but it does add an
extra read-from/write-to RAM.  However this extra RAM write/read does
avoid contention for limited system resources like disk-head access.
Operators which may benefit from RAM disks include 'ncwa', which may
need to read weighting variables multiple times, the multi-file
operators 'ncra', 'ncrcat', and 'ncecat', which may try to write output
at least once per input file, and 'ncap2' scripts which may be
arbitrarily long and convoluted.

   The '--open_ram' switch causes input files to copied to RAM when
opened.  All further metadata and data access occurs in RAM and thus
avoids access time delays caused by disk-head movement.  Usually input
data is read at most once so it is unlikely that requesting input files
be stored in RAM will save much time.  The likeliest exceptions are
files that are accessed numerous times, such as those repeatedly
analyzed by 'ncap2'.

   Invoking '--open_ram', '--ram_all', or '--diskless_all' uses much
more system memory.  To copy the input file to RAM increases the
sustained memory use by exactly the on-disk filesize of the input file,
i.e., MS += FT. For large input files this can be a huge memory burden
that starves the rest of the NCO analysis of sufficient RAM.  To be
safe, use '--open_ram', '--ram_all', or '--diskless_all' only on files
that are much (say at least a factor of four) smaller than your
available system RAM.  See *note Memory Requirements:: for further
details.

   The '--create_ram' switch causes output files to be created in RAM,
rather than on disk.  These files are copied to disk only when closed,
i.e., when the operator completes.  Creating files in RAM may save time,
especially with 'ncap2' computations that are iterative, e.g., loops,
and for multi-file operators that write output every record (timestep)
or file.  RAM files provide many of the same benefits as RAM variables
in such cases (*note RAM variables::).

   Two switches, '--ram_all' and '--diskless_all', are convenient
shortcuts for specifying both '--create_ram' and '--diskless_ram'.  Thus
     ncks in.nc out.nc # Default: Open in.nc on disk, write out.nc to disk
     ncks --open_ram in.nc out.nc # Open in.nc in RAM, write out.nc to disk
     ncks --create_ram in.nc out.nc # Create out.nc in RAM, write to disk
     # Open in.nc in RAM, create out.nc in RAM, then write out.nc to disk
     ncks --open_ram --create_ram in.nc out.nc
     ncks --ram_all in.nc out.nc # Same as above
     ncks --diskless_all in.nc out.nc # Same as above

   It is straightforward to demonstrate the efficacy of RAM disks.  For
NASA we constructed a test that employs 'ncecat' an arbitrary number
(set to one hundred thousand) of files that are all symbolically linked
to the same file.  Everything is on the local filesystem (not DAP).
     # Create symbolic links for benchmark
     cd ${DATA}/nco # Do all work here
     for idx in {1..99999}; do
       idx_fmt=`printf "%05d" ${idx}`
       /bin/ln -s ${DATA}/nco/LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T111931Z_20020619.nc \
                  ${DATA}/nco/${idx_fmt}.nc
     done
     # Benchmark time to ncecat one hundred thousand files
     time ncecat --create_ram -O -u time -v ts -d Latitude,40.0 \ 
      -d Longitude,-105.0 -p ${DATA}/nco -n 99999,5,1 00001.nc ~/foo.nc
   Run normally on a laptop in 201303, this completes in 21 seconds.
The '--create_ram' reduces the elapsed time to 9 seconds.  Some of this
speed may be due to using symlinks and caching.  However, the efficacy
of '--create_ram' is clear.  Placing the output file in RAM avoids
thousands of disk writes.  It is not unreasonable to for NCO to process
a million files like this in a few minutes.  However, there is no
substitute for benchmarking with real files.

   A completely independent way to reduce time spent writing files is to
refrain from writing temporary output files.  This is accomplished with
the '--no_tmp_fl' switch (*note Temporary Output Files::).


File: nco.info,  Node: Packed data,  Next: Operation Types,  Prev: RAM disks,  Up: Shared features

3.35 Packed data
================

Availability: 'ncap2', 'ncbo', 'nces', 'ncflint', 'ncpdq', 'ncra',
'ncwa'
Short options: None
Long options: '--hdf_upk', '--hdf_unpack'

   The phrase "packed data" refers to data which are stored in the
standard netCDF3 lossy linear packing format.  See *note ncks netCDF
Kitchen Sink:: for a description of deflation, a lossless compression
technique available with netCDF4 only.  Packed data may be deflated to
save additional space.

Standard Packing Algorithm
--------------------------

"Packing" The standard netCDF linear packing algorithm (described here
(http://www.unidata.ucar.edu/software/netcdf/docs/netcdf/Attribute-Conventions.html))
produces packed data with the same dynamic range as the original but
which requires no more than half the space to store.  NCO will always
use this algorithm for packing.  Like all packing algorithms, linear
packing is _lossy_.  Just how lossy depends on the values themselves,
especially their range.  The packed variable is stored (usually) as type
'NC_SHORT' with the two attributes required to unpack the variable,
'scale_factor' and 'add_offset', stored at the original (unpacked)
precision of the variable (1).  Let MIN and MAX be the minimum and
maximum values of X.

   SCALE_FACTOR = (MAX-MIN)/NDRV
ADD_OFFSET = 0.5*(MIN+MAX)
PCK = (UPK-ADD_OFFSET)/SCALE_FACTOR = (UPK-0.5*(MIN+MAX))*NDRV/(MAX-MIN)

   where NDRV is the number of discrete representable values for given
type of packed variable.  The theoretical maximum value for NDRV is two
raised to the number of bits used to store the packed variable.  Thus if
the variable is packed into type 'NC_SHORT', a two-byte datatype, then
there are at most 2^{16} = 65536 distinct values representable.  In
practice, the number of discretely representible values is taken to be
two less than the theoretical maximum.  This leaves space for a missing
value and solves potential problems with rounding that may occur during
the unpacking of the variable.  Thus for 'NC_SHORT', ndrv = 65536 - 2 =
65534.  Less often, the variable may be packed into type 'NC_CHAR',
where ndrv = 2^{8} - 2 = 256 - 2 = 254, or type 'NC_INT' where where
ndrv = 2^{32} - 2 = 4294967295 - 2 = 4294967293.  One useful feature of
the (lossy) netCDF packing algorithm is that lossless packing algorithms
perform well on top of it.

Standard (Default) Unpacking Algorithm
--------------------------------------

"Unpacking" The unpacking algorithm depends on the presence of two
attributes, 'scale_factor' and 'add_offset'.  If 'scale_factor' is
present for a variable, the data are multiplied by the value
SCALE_FACTOR after the data are read.  If 'add_offset' is present for a
variable, then the ADD_OFFSET value is added to the data after the data
are read.  If both 'scale_factor' and 'add_offset' attributes are
present, the data are first scaled by SCALE_FACTOR before the offset
ADD_OFFSET is added.

   UPK = SCALE_FACTOR*PCK + ADD_OFFSET = (MAX-MIN)*PCK/NDRV +
0.5*(MIN+MAX)

   NCO will use this algorithm for unpacking unless told otherwise as
described below.  When 'scale_factor' and 'add_offset' are used for
packing, the associated variable (containing the packed data) is
typically of type 'byte' or 'short', whereas the unpacked values are
intended to be of type 'int', 'float', or 'double'.  An attribute's
'scale_factor' and 'add_offset' and '_FillValue', if any, should all be
of the type intended for the unpacked data, i.e., 'int', 'float' or
'double'.

Non-Standard Packing and Unpacking Algorithms
---------------------------------------------

Many (most?)  files originally written in HDF4 format use poorly
documented packing/unpacking algorithms that are incompatible and easily
confused with the netCDF packing algorithm described above.  The
unpacking component of the "conventional" HDF algorithm (described here
(http://www.hdfgroup.org/HDF5/doc/UG/UG_frame10Datasets.html) and in
Section 3.10.6 of the HDF4 Users Guide here
(http://www.hdfgroup.org/release4/doc/UsrGuide_html/UG_PDF.pdf), and in
the FAQ for MODIS MOD08 data here
(http://modis-atmos.gsfc.nasa.gov/MOD08_D3/faq.html)) is

   UPK = SCALE_FACTOR*(PCK - ADD_OFFSET)

   The unpacking component of the HDF algorithm employed for MODIS MOD13
data is

   UPK = (PCK - ADD_OFFSET)/SCALE_FACTOR

   The unpacking component of the HDF algorithm employed for MODIS MOD04
data is the same as the netCDF algorithm.

   Confusingly, the (incompatible) netCDF and HDF algorithms both store
their parameters in attributes with the same names ('scale_factor' and
'add_offset').  Data packed with one algorithm should never be unpacked
with the other; doing so will result in incorrect answers.
Unfortunately, few users are aware that their datasets may be packed,
and fewer know the details of the packing algorithm employed.  This is
what we in the "bizness" call an "interoperability" issue because it
hampers data analysis performed on heterogeneous systems.

   As described below, NCO automatically unpacks data before performing
arithmetic.  This automatic unpacking occurs silently since there is
usually no reason to bother users with these details.  There is as yet
no generic way for NCO to know which packing convention was used, so NCO
_assumes_ the netCDF convention was used.  NCO uses the same convention
for unpacking unless explicitly told otherwise with the '--hdf_upk'
(also '--hdf_unpack') switch.  Until and unless a method of
automatically detecting the packing method is devised, it must remain
the user's responsibility to tell NCO when to use the HDF convention
instead of the netCDF convention to unpack.

   If your data originally came from an HDF file (e.g., NASA EOS) then
it was likely packed with the HDF convention and must be unpacked with
the same convention.  Our recommendation is to only request HDF
unpacking when you are certain.  Most packed datasets encountered by NCO
will have used the netCDF convention.  Those that were not will
hopefully produce noticeably weird values when unpacked by the wrong
algorithm.  Before or after panicking, treat this as a clue to re-try
your commands with the '--hdf_upk' switch.  See *note ncpdq netCDF
Permute Dimensions Quickly:: for an easy technique to unpack data packed
with the HDF convention, and then re-pack it with the netCDF convention.

Handling of Packed Data by Other Operators
------------------------------------------

All NCO arithmetic operators understand packed data.  The operators
automatically unpack any packed variable in the input file which will be
arithmetically processed.  For example, 'ncra' unpacks all record
variables, and 'ncwa' unpacks all variable which contain a dimension to
be averaged.  These variables are stored unpacked in the output file.

   On the other hand, arithmetic operators do not unpack non-processed
variables.  For example, 'ncra' leaves all non-record variables packed,
and 'ncwa' leaves packed all variables lacking an averaged dimension.
These variables (called fixed variables) are passed unaltered from the
input to the output file.  Hence fixed variables which are packed in
input files remain packed in output files.  Completely packing and
unpacking files is easily accomplished with 'ncpdq' (*note ncpdq netCDF
Permute Dimensions Quickly::).  Pack and unpack individual variables
with 'ncpdq' and the 'ncap2' 'pack()' and 'unpack()' functions (*note
Methods and functions::).

   ---------- Footnotes ----------

   (1) Although not a part of the standard, NCO enforces the policy that
the '_FillValue' attribute, if any, of a packed variable is also stored
at the original precision.


File: nco.info,  Node: Operation Types,  Next: Type Conversion,  Prev: Packed data,  Up: Shared features

3.36 Operation Types
====================

Availability: 'ncap2', 'ncra', 'nces', 'ncwa'
Short options: '-y'
Long options: '--operation', '--op_typ'
The '-y OP_TYP' switch allows specification of many different types of
operations Set OP_TYP to the abbreviated key for the corresponding
operation:
'avg'
     Mean value
'sqravg'
     Square of the mean
'avgsqr'
     Mean of sum of squares
'max'
     Maximum value
'min'
     Minimum value
'mabs'
     Maximum absolute value
'mebs'
     Mean absolute value
'mibs'
     Minimum absolute value
'rms'
     Root-mean-square (normalized by N)
'rmssdn'
     Root-mean square (normalized by N-1)
'sqrt'
     Square root of the mean
'tabs'
     Sum of absolute values
'ttl'
     Sum of values
NCO assumes coordinate variables represent grid axes, e.g., longitude.
The only rank-reduction which makes sense for coordinate variables is
averaging.  Hence NCO implements the operation type requested with '-y'
on all non-coordinate variables, not on coordinate variables.  When an
operation requires a coordinate variable to be reduced in rank, i.e.,
from one dimension to a scalar or from one dimension to a degenerate
(single value) array, then NCO _always averages_ the coordinate variable
regardless of the arithmetic operation type performed on the
non-coordinate variables.

   The mathematical definition of each arithmetic operation is given
below.  *Note ncwa netCDF Weighted Averager::, for additional
information on masks and normalization.  If an operation type is not
specified with '-y' then the operator performs an arithmetic average by
default.  Averaging is described first so the terminology for the other
operations is familiar.

   _Note for Info users_: The definition of mathematical operations
involving rank reduction (e.g., averaging) relies heavily on
mathematical expressions which cannot be easily represented in Info.
_See the printed manual (./nco.pdf) for much more detailed and complete
documentation of this subject._

   The definitions of some of these operations are not universally
useful.  Mostly they were chosen to facilitate standard statistical
computations within the NCO framework.  We are open to redefining and or
adding to the above.  If you are interested in having other statistical
quantities defined in NCO please contact the NCO project (*note Help
Requests and Bug Reports::).

EXAMPLES

Suppose you wish to examine the variable 'prs_sfc(time,lat,lon)' which
contains a time series of the surface pressure as a function of latitude
and longitude.  Find the minimum value of 'prs_sfc' over all dimensions:
     ncwa -y min -v prs_sfc in.nc foo.nc
Find the maximum value of 'prs_sfc' at each time interval for each
latitude:
     ncwa -y max -v prs_sfc -a lon in.nc foo.nc
Find the root-mean-square value of the time-series of 'prs_sfc' at every
gridpoint:
     ncra -y rms -v prs_sfc in.nc foo.nc
     ncwa -y rms -v prs_sfc -a time in.nc foo.nc
The previous two commands give the same answer but 'ncra' is preferred
because it has a smaller memory footprint.  A dimension of size one is
said to be "degenerate".  By default, 'ncra' leaves the (degenerate)
'time' dimension in the output file (which is usually useful) whereas
'ncwa' removes the 'time' dimension (unless '-b' is given).

These operations work as expected in multi-file operators.  Suppose that
'prs_sfc' is stored in multiple timesteps per file across multiple
files, say 'jan.nc', 'feb.nc', 'march.nc'.  We can now find the three
month maximum surface pressure at every point.
     nces -y max -v prs_sfc jan.nc feb.nc march.nc out.nc

It is possible to use a combination of these operations to compute the
variance and standard deviation of a field stored in a single file or
across multiple files.  The procedure to compute the temporal standard
deviation of the surface pressure at all points in a single file 'in.nc'
involves three steps.
     ncwa -O -v prs_sfc -a time in.nc out.nc
     ncbo -O -v prs_sfc in.nc out.nc out.nc
     ncra -O -y rmssdn out.nc out.nc
   First construct the temporal mean of 'prs_sfc' in the file 'out.nc'.
Next overwrite 'out.nc' with the anomaly (deviation from the mean).
Finally overwrite 'out.nc' with the root-mean-square of itself.  Note
the use of '-y rmssdn' (rather than '-y rms') in the final step.  This
ensures the standard deviation is correctly normalized by one fewer than
the number of time samples.  The procedure to compute the variance is
identical except for the use of '-y avgsqr' instead of '-y rmssdn' in
the final step.

   'ncap2' can also compute statistics like standard deviations.
Brute-force implementation of formulae is one option, e.g.,
     ncap2 -s 'prs_sfc_sdn=sqrt((prs_sfc-prs_sfc.avg($time)^2). \
           total($time)/($time.size-1))' in.nc out.nc
   The operation may, of course, be broken into multiple steps in order
to archive intermediate quantities, such as the time-anomalies
     ncap2 -s 'prs_sfc_anm=prs_sfc-prs_sfc.avg($time)' \
           -s 'prs_sfc_sdn=sqrt((prs_sfc_anm^2).total($time)/($time.size-1))' \
           in.nc out.nc

   'ncap2' supports intrinsic standard deviation functions (*note
Operation Types::) which simplify the above expression to
     ncap2 -s 'prs_sfc_sdn=(prs_sfc-prs_sfc.avg($time)).rmssdn($time)' in.nc out.nc
   These instrinsic functions compute the answer quickly and concisely.

   The procedure to compute the spatial standard deviation of a field in
a single file 'in.nc' involves three steps.
     ncwa -O -v prs_sfc,gw -a lat,lon -w gw in.nc out.nc
     ncbo -O -v prs_sfc,gw in.nc out.nc out.nc
     ncwa -O -y rmssdn -v prs_sfc -a lat,lon -w gw out.nc out.nc
   First the spatially weighted (by '-w gw') mean values are written to
the output file, as are the mean weights.  The initial output file is
then overwritten with the gridpoint deviations from the spatial mean.
It is important that the output file after the second line contain the
original, non-averaged weights.  This will be the case if the weights
are named so that NCO treats them like a coordinate (*note CF
Conventions::).  One such name is 'gw', and any variable whose name
begins with 'msk_' (for "mask") or 'wgt_' (for "weight") will likewise
be treated as a coordinate, and will be copied (not differenced)
straight from 'in.nc' to 'out.nc' in the second step.  When using
weights to compute standard deviations one must remember to include the
weights in the initial output files so that they may be used again in
the final step.  Finally the root-mean-square of the appropriately
weighted spatial deviations is taken.

   No elegant 'ncap2' solution exists to compute weighted standard
deviations.  Those brave of heart may try to formulate one.  A general
formula should allow weights to have fewer than and variables to have
more than the minimal spatial dimensions (latitude and longitude).

   The procedure to compute the standard deviation of a time-series
across multiple files involves one extra step since all the input must
first be collected into one file.
     ncrcat -O -v tpt in.nc in.nc foo1.nc
     ncwa -O -a time foo1.nc foo2.nc
     ncbo -O -v tpt foo1.nc foo2.nc foo3.nc
     ncra -O -y rmssdn foo3.nc out.nc
   The first step assembles all the data into a single file.  Though
this may consume a lot of temporary disk space, it is more or less
required by the 'ncbo' operation in the third step.


File: nco.info,  Node: Type Conversion,  Next: Batch Mode,  Prev: Operation Types,  Up: Shared features

3.37 Type Conversion
====================

Availability (automatic type conversion): 'ncap2', 'ncbo', 'nces',
'ncflint', 'ncra', 'ncwa'
Short options: None (it's _automatic_)
Availability (manual type conversion): 'nces', 'ncra', 'ncwa'
Short options: None
Long options: '--dbl', '--flt', '--rth_dbl', '--rth_flt'
   Type conversion refers to the casting or coercion of one fundamental
or atomic data type to another, e.g., converting 'NC_SHORT' (two bytes)
to 'NC_DOUBLE' (eight bytes).  Type conversion always "promotes" or
"demotes" the range and/or precision of the values a variable can hold.
Type conversion is automatic when the language carries out this
promotion according to an internal set of rules without explicit user
intervention.  In contrast, manual type conversion refers to explicit
user commands to change the type of a variable or attribute.  Most type
conversion happens automatically, yet there are situations in which
manual type conversion is advantageous.

* Menu:

* Automatic type conversion::
* Promoting Single-precision to Double::
* Manual type conversion::


File: nco.info,  Node: Automatic type conversion,  Next: Promoting Single-precision to Double,  Prev: Type Conversion,  Up: Type Conversion

3.37.1 Automatic type conversion
--------------------------------

There are at least two reasons to avoid type conversions.  First, type
conversions are expensive since they require creating (temporary)
buffers and casting each element of a variable from its storage type to
some other type and then, often, converting it back.  Second, a
dataset's creator perhaps had a good reason for storing data as, say,
'NC_FLOAT' rather than 'NC_DOUBLE'.  In a scientific framework there is
no reason to store data with more precision than the observations merit.
Normally this is single-precision, which guarantees 6-9 digits of
precision.  Reasons to engage in type conversion include avoiding
rounding errors and out-of-range limitations of less-precise types.
This is the case with most integers.  Thus NCO defaults to automatically
promote integer types to floating-point when performing lengthy
arithmetic, yet NCO defaults to not promoting single to double-precision
floats.

   Before discussing the more subtle floating-point issues, we first
examine integer promotion.  We will show how following parsimonious
conversion rules dogmatically can cause problems, and what NCO does
about that.  That said, there are situations in which implicit
conversion of single- to double-precision is also warranted.
Understanding the narrowness of these situations takes time, and we hope
the reader appreciates the following detailed discussion.

   Consider the average of the two 'NC_SHORT's '17000s' and '17000s'.  A
straightforward average without promotion results in garbage since the
intermediate value which holds their sum is also of type 'NC_SHORT' and
thus overflows on (i.e., cannot represent) values greater than 32,767
(1).  There are valid reasons for expecting this operation to succeed
and the NCO philosophy is to make operators do what you want, not what
is purest.  Thus, unlike C and Fortran, but like many other higher level
interpreted languages, NCO arithmetic operators will perform automatic
type conversion on integers when all the following conditions are met
(2):
  1. The requested operation is arithmetic.  This is why type conversion
     is limited to the operators 'ncap2', 'ncbo', 'nces', 'ncflint',
     'ncra', and 'ncwa'.
  2. The arithmetic operation could benefit from type conversion.
     Operations that could benefit include averaging, summation, or any
     "hard" arithmetic that could overflow or underflow.  Larger
     representable sums help avoid overflow, and more precision helps to
     avoid underflow.  Type conversion does not benefit searching for
     minima and maxima ('-y min', or '-y max').
  3. The variable on disk is of type 'NC_BYTE', 'NC_CHAR', 'NC_SHORT',
     or 'NC_INT'.  Type 'NC_DOUBLE' is not promoted because there is no
     type of higher precision.  Conversion of type 'NC_FLOAT' is
     discussed in detail below.  When it occurs, it follows the same
     procedure (promotion then arithmetic then demotion) as conversion
     of integer types.

   When these criteria are all met, the operator promotes the variable
in question to type 'NC_DOUBLE', performs all the arithmetic operations,
casts the 'NC_DOUBLE' type back to the original type, and finally writes
the result to disk.  The result written to disk may not be what you
expect, because of incommensurate ranges represented by different types,
and because of (lack of) rounding.  First, continuing the above example,
the average (e.g., '-y avg') of '17000s' and '17000s' is written to disk
as '17000s'.  The type conversion feature of NCO makes this possible
since the arithmetic and intermediate values are stored as 'NC_DOUBLE's,
i.e., '34000.0d' and only the final result must be represented as an
'NC_SHORT'.  Without the type conversion feature of NCO, the average
would have been garbage (albeit predictable garbage near '-15768s').
Similarly, the total (e.g., '-y ttl') of '17000s' and '17000s' written
to disk is garbage (actually '-31536s') since the final result (the true
total) of 34000 is outside the range of type 'NC_SHORT'.

   After arithmetic is computed in double-precision for promoted
variables, the intermediate double-precision values must be demoted to
the variables' original storage type (e.g., from 'NC_DOUBLE' to
'NC_SHORT').  NCO has handled this demotion in three ways in its
history.  Prior to October, 2011 (version 4.0.8), NCO employed the
C library truncate function, 'trunc()' (3).  Truncation rounds X to the
nearest integer not larger in absolute value.  For example, truncation
rounds '1.0d', '1.5d', and '1.8d' to the same value, '1s'.  Clearly,
truncation does not round floating-point numbers to the nearest integer!
Yet truncation is how the C language performs implicit conversion of
real numbers to integers.

   NCO stopped using truncation for demotion when an alert user (Neil
Davis) informed us that this caused a small bias in the packing
algorithm employed by 'ncpdq'.  This led to NCO adopting rounding
functions for demotion.  Rounding functions eliminated the small bias in
the packing algorithm.

   From February, 2012 through March, 2013 (versions 4.0.9-4.2.6), NCO
employed the C library family of rounding functions, 'lround()'.  These
functions round X to the nearest integer, halfway cases away from zero.
The problem with 'lround()' is that it always rounds real values ending
in '.5' away from zero.  This rounds, for example, '1.5d' and '2.5d' to
'2s' and '3s', respectively.

   Since April, 2013 (version 4.3.0), NCO has employed the other
C library family of rounding functions, 'lrint()'.  This algorithm
rounds X to the nearest integer, using the current rounding direction.
Halfway cases are rounded to the nearest even integer.  This rounds, for
example, both '1.5d' and '2.5d' to the same value, '2s', as recommended
by the IEEE.  This rounding is symmetric: up half the time, down half
the time.  This is the current and hopefully final demotion algorithm
employed by NCO.

   Hence because of automatic conversion, NCO will compute the average
of '2s' and '3s' in double-precision arithmetic as ('2.0d' +
'3.0d')/'2.0d') = '2.5d'.  It then demotes this intermediate result back
to 'NC_SHORT' and stores it on disk as 'trunc(2.5d)' = '2s' (versions up
to 4.0.8), 'lround(2.5d)' = '3s' (versions 4.0.9-4.2.6), and
'lrint(2.5d)' = '2s' (versions 4.3.0 and later).

   ---------- Footnotes ----------

   (1) 32767 = 2^15-1

   (2) Operators began performing automatic type conversions before
arithmetic in NCO version 1.2, August, 2000.  Previous versions never
performed unnecessary type conversion for arithmetic.

   (3) The actual type conversions with trunction were handled by
intrinsic type conversion, so the 'trunc()' function was never
explicitly called, although the results would be the same if it were.


File: nco.info,  Node: Promoting Single-precision to Double,  Next: Manual type conversion,  Prev: Automatic type conversion,  Up: Type Conversion

3.37.2 Promoting Single-precision to Double
-------------------------------------------

Promotion of real numbers from single- to double-precision is
fundamental to scientific computing.  When it should occur depends on
the precision of the inputs and the number of operations.
Single-precision (four-byte) numbers contain about seven significant
figures, while double-precision contain about sixteen.  More, err,
precisely, the IEEE single-precision representation gives from 6 to 9
significant decimal digits precision (1).  And the IEEE double-precision
representation gives from 15 to 17 significant decimal digits precision
(2).  Hence double-precision numbers represent about nine digits more
precision than single-precision numbers.

   Given these properties, there are at least two possible arithmetic
conventions for the treatment of real numbers:
  1. Conservative, aka Fortran Convention Automatic type conversion
     during arithmetic in the Fortran language is, by default, performed
     only when necessary.  All operands in an operation are converted to
     the most precise type involved the operation before the arithmetic
     operation.  Expressions which involve only single-precision numbers
     are computed entirely in single-precision.  Expressions involving
     mixed precision types are computed in the type of higher precision.
     NCO by default employs the Fortan Convention for promotion.
  2. Aggressive, aka C Convention The C language is by default much more
     aggressive (and thus wasteful) than Fortran, and will always
     implicitly convert single- to double-precision numbers, even when
     there is no good reason.  All real-number standard C library
     functions are double-precision, and C programmers must take extra
     steps to only utilize single precision arithmetic.  The high-level
     interpreted data analysis languages IDL, Matlab, and NCL all adopt
     the C Convention.

   NCO does not automatically promote 'NC_FLOAT' because, in our
judgement, the performance penalty of always doing so would outweigh the
potential benefits.  The now-classic text "Numerical Recipes in C"
discusses this point under the section "Implicit Conversion of Float to
Double" (3).  That said, such promotion is warranted in some
circumstances.

   For example, rounding errors can accumulate to worrisome levels
during arithmetic performed on large arrays of single-precision floats.
This use-case occurs often in geoscientific studies of climate where
thousands-to-millions of gridpoints may contribute to a single average.
If the inputs are all single-precision, then so should be the output.
However the intermediate results where running sums are accumulated may
suffer from too much rounding or from underflow unless computed in
double-precision.

   The order of operations matters to floating-point math even when the
analytic expressions are equal.  Cautious users feel disquieted when
results from equally valid analyses differ in the final bits instead of
agreeing bit-for-bit.  For example, averaging arrays in multiple stages
produces different answers than averaging them in one step.  This is
easily seen in the computation of ensemble averages by two different
methods.  The NCO test file 'in.nc' contains single- and
double-precision representations of the same temperature timeseries as
'tpt_flt' and 'tpt_dbl'.  Pretend each datapoint in this timeseries
represents a monthly-mean temperature.  We will mimic the derivation of
a fifteen-year ensemble-mean January temperature by concatenating the
input file five times, and then averaging the datapoints representing
January two different ways.  In Method 1 we derive the 15-year ensemble
January average in two steps, as the average of three five-year
averages.  This method is naturally used when each input file contains
multiple years and multiple input files are needed (4).  In Method 2 we
obtain 15-year ensemble January average in a single step, by averaging
all 15 Januaries at one time:
     # tpt_flt and tpt_dbl are identical except for precision
     ncks -C -v tpt_flt,tpt_dbl ~/nco/data/in.nc
     # tpt_dbl = 273.1, 273.2, 273.3, 273.4, 273.5, 273.6, 273.7, 273.8, 273.9, 274
     # tpt_flt = 273.1, 273.2, 273.3, 273.4, 273.5, 273.6, 273.7, 273.8, 273.9, 274
     # Create file with five "ten-month years" (i.e., 50 timesteps) of temperature data
     ncrcat -O -v tpt_flt,tpt_dbl -p ~/nco/data in.nc in.nc in.nc in.nc in.nc ~/foo.nc
     # Average 1st five "Januaries" (elements 1, 11, 21, 31, 41)
     ncra --flt -O -F -d time,1,,10 ~/foo.nc ~/foo_avg1.nc
     # Average 2nd five "Januaries" (elements 2, 12, 22, 32, 42)
     ncra --flt -O -F -d time,2,,10 ~/foo.nc ~/foo_avg2.nc
     # Average 3rd five "Januaries" (elements 3, 13, 23, 33, 43)
     ncra --flt -O -F -d time,3,,10 ~/foo.nc ~/foo_avg3.nc
     # Method 1: Obtain ensemble January average by averaging the averages
     ncra --flt -O ~/foo_avg1.nc ~/foo_avg2.nc ~/foo_avg3.nc ~/foo_avg_mth1.nc
     # Method 2: Obtain ensemble January average by averaging the raw data
     # Employ ncra's "subcycle" feature (http://nco.sf.net/nco.html#ssc)
     ncra --flt -O -F -d time,1,,10,3 ~/foo.nc ~/foo_avg_mth2.nc
     # Difference the two methods
     ncbo -O ~/foo_avg_mth1.nc ~/foo_avg_mth2.nc ~/foo_avg_dff.nc
     ncks ~/foo_avg_dff.nc
     # tpt_dbl = 5.6843418860808e-14 ;
     # tpt_flt = -3.051758e-05 ;
   Although the two methods are arithmetically equivalent, they produce
slightly different answers due to the different order of operations.
Moreover, it appears at first glance that the single-precision answers
suffer from greater error than the double-precision answers.  In fact
both precisions suffer from non-zero rounding errors.  The answers
differ negligibly to machine precision, which is about seven significant
figures for single precision floats ('tpt_flt'), and sixteen significant
figures for double precision ('tpt_dbl').  The input precision
determines the answer precision.

   IEEE arithmetic guarantees that two methods will produce bit-for-bit
identical answers only if they compute the same operations in the same
order.  Bit-for-bit identical answers may also occur by happenstance
when rounding errors exactly compensate one another.  This is
demonstrated by repeating the example above with the '--dbl' (or
'--rth_dbl' for clarity) option which forces conversion of
single-precision numbers to double-precision prior to arithmetic.  Now
'ncra' will treat the first value of 'tpt_flt', '273.1000f', as
'273.1000000000000d'.  Arithmetic on 'tpt_flt' then proceeds in
double-precision until the final answer, which is converted back to
single-precision for final storage.
     # Average 1st five "Januaries" (elements 1, 11, 21, 31, 41)
     ncra --dbl -O -F -d time,1,,10 ~/foo.nc ~/foo_avg1.nc
     # Average 2nd five "Januaries" (elements 2, 12, 22, 32, 42)
     ncra --dbl -O -F -d time,2,,10 ~/foo.nc ~/foo_avg2.nc
     # Average 3rd five "Januaries" (elements 3, 13, 23, 33, 43)
     ncra --dbl -O -F -d time,3,,10 ~/foo.nc ~/foo_avg3.nc
     # Method 1: Obtain ensemble January average by averaging the averages
     ncra --dbl -O ~/foo_avg1.nc ~/foo_avg2.nc ~/foo_avg3.nc ~/foo_avg_mth1.nc
     # Method 2: Obtain ensemble January average by averaging the raw data
     # Employ ncra's "subcycle" feature (http://nco.sf.net/nco.html#ssc)
     ncra --dbl -O -F -d time,1,,10,3 ~/foo.nc ~/foo_avg_mth2.nc
     # Difference the two methods
     ncbo -O ~/foo_avg_mth1.nc ~/foo_avg_mth2.nc ~/foo_avg_dff.nc
     # Show differences
     ncks ~/foo_avg_dff.nc
     # tpt_dbl = 5.6843418860808e-14 ;
     # tpt_flt = 0 ;
   The '--dbl' switch has no effect on the results computed from
double-precision inputs.  But now the two methods produce bit-for-bit
identical results from the single-precision inputs!  This is due to the
happenstance of rounding along with the effects of the '--dbl' switch.
The '--flt' and '--rth_flt' switches are provided for symmetry.  They
enforce the traditional NCO and Fortran convention of keeping
single-precision arithmetic in single-precision unless a
double-precision number is explicitly involved.

   We have shown that forced promotion of single- to double-precision
prior to arithmetic has advantages and disadvantages.  The primary
disadvantages are speed and size.  Double-precision arithmetic is 10-60%
slower than, and requires twice the memory of single-precision
arithmetic.  The primary advantage is that rounding errors in
double-precision are much less likely to accumulate to values near the
precision of the underlying geophysical variable.

   For example, if we know temperature to five significant digits, then
a rounding error of 1-bit could affect the least precise digit of
temperature after 1,000-10,000 consecutive one-sided rounding errors
under the worst possible scenario.  Many geophysical grids have
tens-of-thousands to millions of points that must be summed prior to
normalization to compute an average.  It is possible for
single-precision rouding errors to accumulate and degrade the precision
in such situtations.  Double-precision arithmetic mititgates this
problem, so '--dbl' would be warranted.

   This can be seen with another example, averaging a global surface
temperature field with 'ncwa'.  The input contains a single-precision
global temperature field (stored in 'TREFHT') produced by the CAM3
general circulation model (GCM) run and stored at 1.9 by 2.5 degrees
resolution.  This requires 94 latitudes and 144 longitudes, or 13,824
total surface gridpoints, a typical GCM resolution in 2008-2013.  These
input characteristics are provided only to show the context to the
interested reader, equivalent results would be found in statistics of
any dataset of comparable size.  Models often represent Earth on a
spherical grid where global averages must be created by weighting each
gridcell by its latitude-dependent weight (e.g., a Gaussian weight
stored in 'gw'), or by the surface area of each contributing gridpoint
(stored in 'area').

   Like many geophysical models and most GCMs, CAM3 runs completely in
double-precision yet stores its archival output in single-precision to
save space.  In practice such models usually save multi-dimensional
prognostic and diagnostic fields (like 'TREFHT(lat,lon)') as
single-precision, while saving all one-dimensional coordinates and
weights (here 'lat', 'lon', and 'gw(lon)') as double-precision.  The
gridcell area 'area(lat,lon)' is an extensive grid property that should
be, but often is not, stored as double-precision.  To obtain pure
double-precision arithmetic _and_ storage of the globla mean
temperature, we first create and store double-precision versions of the
single-precision fields:
     ncap2 -O -s 'TREFHT_dbl=double(TREFHT);area_dbl=double(area)' in.nc in.nc
   The single- and double-precision temperatures may each be averaged
globally using four permutations for the precision of the weight and of
the intermediate arithmetic representation:
  1. Single-precision weight ('area'), single-precision arithmetic
  2. Double-precision weight ('gw'), single-precision arithmetic
  3. Single-precision weight ('area'), double-precision arithmetic
  4. Double-precision weight ('gw'), double-precision arithmetic
     # NB: Values below are printed with C-format %5.6f using
     # ncks -H -C -s '%5.6f' -v TREFHT,TREFHT_dbl out.nc
     # Single-precision weight (area), single-precision arithmetic
     ncwa --flt -O -a lat,lon -w area in.nc out.nc
     # TREFHT     = 289.246735
     # TREFHT_dbl = 289.239964
     # Double-precision weight (gw),   single-precision arithmetic
     ncwa --flt -O -a lat,lon -w gw   in.nc out.nc
     # TREFHT     = 289.226135
     # TREFHT_dbl = 289.239964
     # Single-precision weight (area), double-precision arithmetic
     ncwa --dbl -O -a lat,lon -w area in.nc out.nc
     # TREFHT     = 289.239960
     # TREFHT_dbl = 289.239964
     # Double-precision weight (gw),   double-precision arithmetic
     ncwa --dbl -O -a lat,lon -w gw   in.nc out.nc
     # TREFHT     = 289.239960
     # TREFHT_dbl = 289.239964
   First note that the 'TREFHT_dbl' average never changes because
'TREFHT_dbl(lat,lon)' is double-precision in the input file.  As
described above, NCO automatically converts all operands involving to
the highest precision involved in the operation.  So specifying '--dbl'
is redundant for double-precision inputs.

   Second, the single-precision arithmetic averages of the
single-precision input 'TREFHT' differ by 289.246735 - 289.226135 =
0.0206 from eachother, and, more importantly, by as much as 289.239964 -
289.226135 = 0.013829 from the correct (double-precision) answer.  These
averages differ in the fifth digit, i.e., they agree only to four
significant figures!  Given that climate scientists are concerned about
global temperature variations of a tenth of a degree or less, this
difference is large.  Global mean temperature changes significant to
climate scientists are comparable in size to the numerical artifacts
produced by the averaging procedure.

   Why are the single-precision numerical artifacts so large?  Each
global average is the result of multiplying almost 15,000 elements each
by its weight, summing those, and then dividing by the summed weights.
Thus about 50,000 single-precision floating-point operations caused the
loss of two to three significant digits of precision.  The net error of
a series of independent rounding errors is a random walk phenomena (5).
Successive rounding errors displace the answer further from the truth.
An ensemble of such averages will, on average, have no net bias.  In
other words, the expectation value of a series of IEEE rounding errors
is zero.  And the error of any given sequence of rounding errors obeys,
for large series, a Gaussian distribution centered on zero.

   Single-precision numbers use three of their four eight-bit bytes to
represent the mantissa so the smallest representable single-precision
mantissa is \epsilon \equiv 2^{-23} = 1.19209 \times 10^{-7}.  This
\epsilon is the smallest X such that 1.0 + x \ne 1.0.  This is the
rounding error for non-exact precision-numbers.  Applying random walk
theory to rounding, it can be shown that the expected rounding error
after N inexact operations is \sqrt{2n/\pi} for large N.  The expected
(i.e., mean absolute) rounding error in our example with 13,824
additions is about \sqrt{2 \times 13824 / \pi} = 91.96.  Hence, addition
alone of about fifteen thousand single-precision floats is expected to
consume about two significant digits of precision.  This neglects the
error due to the inner product (weights times values) and normalization
(division by tally) aspects of a weighted average.  The ratio of two
numbers each containing a numerical bias can magnify the size of the
bias.  In summary, a global mean number computed from about 15,000
gridpoints each with weights can be expected to lose up to three
significant digits.  Since single-precision starts with about seven
significant digits, we should not expect to retain more than four
significant digits after computing weighted averages in
single-precision.  The above example with 'TREFHT' shows the expected
four digits of agreement.

   The NCO results have been independently validated to the extent
possible in three other languages: C, Matlab, and NCL.  C and NCO are
the only languages that permit single-precision numbers to be treated
with single precision arithmetic:
     # Double-precision weight (gw),   single-precision arithmetic (C)
     ncwa_3528514.exe
     # TREFHT     = 289.240112
     # Double-precision weight (gw),   double-precision arithmetic (C)
     # TREFHT     = 289.239964
     # Single-precision weight (area), double-precision arithmetic (Matlab)
     # TREFHT     = 289.239964
     # Double-precision weight (gw),   double-precision arithmetic (Matlab)
     # TREFHT     = 289.239964
     # Single-precision weight (area), double-precision arithmetic (NCL)
     ncl < ncwa_3528514.ncl
     # TREFHT     = 289.239960
     # TREFHT_dbl = 289.239964
     # Double-precision weight (gw),   double-precision arithmetic (NCL)
     # TREFHT     = 289.239960
     # TREFHT_dbl = 289.239964
   All languages tested (C, Matlab, NCL, and NCO) agree to machine
precision with double-precision arithmetic.  Users are fortunate to have
a variety of high quality software that liberates them from the drudgery
of coding their own.  Many packages are free (as in beer)!  As shown
above NCO permits one to shift to their float-promotion preferences as
desired.  No other language allows this with a simple switch.

   To summarize, until version 4.3.6 (September, 2013), the default
arithmetic convention of NCO adhered to Fortran behavior, and
automatically promoted single-precision to double-precision in all
mixed-precision expressions, and left-alone pure single-precision
expressions.  This is faster and more memory efficient than other
conventions.  However, pure single-precision arithmetic can lose too
much precision when used to condense (e.g., average) large arrays.
Statistics involving about n = 10,000 single-precision inputs will lose
about 2-3 digits if not promoted to double-precision prior to
arithmetic.  The loss scales with the squareroot of N.  For larger N,
users should promote floats with the '--dbl' option if they want to
preserve more than four significant digits in their results.

   The '--dbl' and '--flt' switches are only available with the NCO
arithmetic operators that could potentially perform more than a few
single-precision floating-point operations per result.  These are
'nces', 'ncra', and 'ncwa'.  Each is capable of thousands to millions or
more operations per result.  By contrast, the arithmetic operators
'ncbo' and 'ncflint' perform at most one floating-point operation per
result.  Providing the '--dbl' option for such trivial operations makes
little sense, so the option is not currently made available.

   We are interested in users' opinions on these matters.  The default
behavior was changed from '--flt' to '--dbl' with the release of NCO
version 4.3.6 (October 2013).  We will change the default back to
'--flt' if users prefer.  Or we could set a threshold (e.g., n \ge
10000) after which single- to double-precision promotion is
automatically invoked.  Or we could make the default promotion
convention settable via an environment variable (GSL does this a lot).
Please let us know what you think of the selected defaults and options.

   ---------- Footnotes ----------

   (1) According to Wikipedia's summary of IEEE standard 754, "If a
decimal string with at most 6 significant digits is converted to
IEEE 754 single-precision and then converted back to the same number of
significant decimal, then the final string should match the original;
and if an IEEE 754 single-precision is converted to a decimal string
with at leastn 9 significant decimal and then converted back to single,
then the final number must match the original".

   (2) According to Wikipedia's summary of IEEE standard 754, "If a
decimal string with at most 15 significant digits is converted to
IEEE 754 double-precision representation and then converted back to a
string with the same number of significant digits, then the final string
should match the original; and if an IEEE 754 double precision is
converted to a decimal string with at least 17 significant digits and
then converted back to double, then the final number must match the
original".

   (3) See page 21 in Section 1.2 of the First edition for this gem:
     One does not need much experience in scientific computing to
     recognize that the implicit conversion rules are, in fact, sheer
     madness!  In effect, they make it impossible to write efficient
     numerical programs.

   (4) For example, the CMIP5 archive tends to distribute monthly
average timeseries in 50-year chunks.

   (5) Thanks to Michael J. Prather for explaining this to me.


File: nco.info,  Node: Manual type conversion,  Prev: Promoting Single-precision to Double,  Up: Type Conversion

3.37.3 Manual type conversion
-----------------------------

'ncap2' provides intrinsic functions for performing manual type
conversions.  This, for example, converts variable 'tpt' to external
type 'NC_SHORT' (a C-type 'short'), and variable 'prs' to external type
'NC_DOUBLE' (a C-type 'double').
     ncap2 -s 'tpt=short(tpt);prs=double(prs)' in.nc out.nc
   With ncap2 there also is the 'convert()' method that takes an integer
argument.  For example the above statements become:
     ncap2 -s 'tpt=tpt.convert(NC_SHORT);prs=prs.convert(NC_DOUBLE)' in.nc out.nc
   Can also use 'convert()' in combination with 'type()' so to make
variable 'ilev_new' the same type as 'ilev' just do:
     ncap2 -s 'ilev_new=ilev_new.convert(ilev.type())' in.nc out.nc

   *Note ncap2 netCDF Arithmetic Processor::, for more details.


File: nco.info,  Node: Batch Mode,  Next: Global Attribute Addition,  Prev: Type Conversion,  Up: Shared features

3.38 Batch Mode
===============

Availability: All operators
Short options: '-O', '-A'
Long options: '--ovr', '--overwrite', '--apn', '--append'
   If the OUTPUT-FILE specified for a command is a pre-existing file,
then the operator will prompt the user whether to overwrite (erase) the
existing OUTPUT-FILE, attempt to append to it, or abort the operation.
However, interactive questions reduce productivity when processing large
amounts of data.  Therefore NCO also implements two ways to override its
own safety features, the '-O' and '-A' switches.  Specifying '-O' tells
the operator to overwrite any existing OUTPUT-FILE without prompting the
user interactively.  Specifying '-A' tells the operator to attempt to
append to any existing OUTPUT-FILE without prompting the user
interactively.  These switches are useful in batch environments because
they suppress interactive keyboard input.  NB: As of 20120515, 'ncap2'
is unable to append to files that already contain the appended
dimensions.


File: nco.info,  Node: Global Attribute Addition,  Next: History Attribute,  Prev: Batch Mode,  Up: Shared features

3.39 Global Attribute Addition
==============================

Availability: All operators
Short options: None
Long options: '--glb', '--gaa', '--glb_att_add'
'--glb ATT_NM=ATT_VAL' (multiple invocations allowed)
   All operators can add user-specified global attributes to output
files.  As of NCO version 4.5.2 (July, 2015), NCO supports multiple uses
of the '--glb' (or equivalent '--gaa' or '--glb_att_add') switch.  The
option '--gaa' (and its long option equivalents such as '--glb_att_add')
indicates the argument syntax will be KEY=VAL.  As such, '--gaa' and its
synonyms are indicator options that accept arguments supplied one-by-one
like '--gaa KEY1=VAL1 --gaa KEY2=VAL2', or aggregated together in
multi-argument format like '--gaa KEY1=VAL1#KEY2=VAL2' (*note
Multi-arguments::).

   The switch takes mandatory arguments '--glb ATT_NM=ATT_VAL' where
ATT_NM is the desired name of the global attribute to add, and ATT_VAL
is its value.  Currently only text attributes are supported (recorded as
type 'NC_CHAR'), and regular expressions are not allowed (unlike *note
ncatted netCDF Attribute Editor::).  Attributes are added in "Append"
mode, meaning that values are appended to pre-existing values, if any.
Multiple invocations can simplify the annotation of output file at
creation (or modification) time:
     ncra --glb machine=${HOSTNAME} --glb created_by=${USER} in*.nc out.nc
   As of NCO version 4.6.2 (October, 2016), one may instead combine the
separate invocations into a single list of invocations separated by
colons:
     ncra --glb machine=${HOSTNAME}:created_by=${USER} in*.nc out.nc
   The list may contain any number of key-value pairs.  Special care
must be taken should a key or value contain a delimiter (i.e., a colon)
otherwise 'NCO' will interpret the colon as a delimiter and will attempt
to create a new attribute.  To protect a colon from being interpreted as
an argument delimiter, precede it with a backslash.

   The global attribution addition feature helps to avoid the
performance penalty incurred by using 'ncatted' separately to annotate
large files.  Should users emit a loud hue and cry, we will consider
ading the functionality of ncatted to the front-end of all operators,
i.e., accepting valid 'ncatted' arguments to modify attributes of any
type and to apply regular expressions.


File: nco.info,  Node: History Attribute,  Next: File List Attributes,  Prev: Global Attribute Addition,  Up: Shared features

3.40 History Attribute
======================

Availability: All operators
Short options: '-h'
Long options: '--hst', '--history'
   All operators automatically append a 'history' global attribute to
any file they create or modify.  The 'history' attribute consists of a
timestamp and the full string of the invocation command to the operator,
e.g., 'Mon May 26 20:10:24 1997: ncks in.nc out.nc'.  The full contents
of an existing 'history' attribute are copied from the first INPUT-FILE
to the OUTPUT-FILE.  The timestamps appear in reverse chronological
order, with the most recent timestamp appearing first in the 'history'
attribute.  Since NCO adheres to the 'history' convention, the entire
data processing path of a given netCDF file may often be deduced from
examination of its 'history' attribute.  As of May, 2002, NCO is
case-insensitive to the spelling of the 'history' attribute name.  Thus
attributes named 'History' or 'HISTORY' (which are non-standard and not
recommended) will be treated as valid history attributes.  When more
than one global attribute fits the case-insensitive search for
"history", the first one found is used.  To avoid information overkill,
all operators have an optional switch ('-h', '--hst', or '--history') to
override automatically appending the 'history' attribute (*note ncatted
netCDF Attribute Editor::).  Note that the '-h' switch also turns off
writing the 'nco_input_file_list'-attribute for multi-file operators
(*note File List Attributes::).

   As of NCO version 4.5.0 (June, 2015), NCO supports its own convention
to retain the 'history'-attribute contents of all files that were
appended to a file (1).  This convention stores those contents in the
'history_of_appended_files' attribute, which complements the
'history'-attribute to provide a more complete provenance.  These
attributes may appear something like this in output:
     // global attributes:
     :history = "Thu Jun  4 14:19:04 2015: ncks -A /home/zender/foo3.nc /home/zender/tmp.nc\n",
       "Thu Jun  4 14:19:04 2015: ncks -A /home/zender/foo2.nc /home/zender/tmp.nc\n",
       "Thu Jun  4 14:19:04 2015: ncatted -O -a att1,global,o,c,global metadata only in foo1 /home/zender/foo1.nc\n",
       "original history from the ur-file serving as the basis for subsequent appends." ;
     :history_of_appended_files = "Thu Jun  4 14:19:04 2015: Appended file \
       /home/zender/foo3.nc had following \"history\" attribute:\n",
       "Thu Jun  4 14:19:04 2015: ncatted -O -a att2,global,o,c,global metadata only in foo3 /home/zender/foo3.nc\n",
       "history from foo3 from which data was appended to foo1 after data from foo2 was appended\n",
       "Thu Jun  4 14:19:04 2015: Appended file /home/zender/foo2.nc had following \"history\" attribute:\n",
       "Thu Jun  4 14:19:04 2015: ncatted -O -a att2,global,o,c,global metadata only in foo2 /home/zender/foo2.nc\n",
       "history of some totally different file foo2 from which data was appended to foo1 before foo3 was appended\n",
     :att1 = "global metadata only in foo1" ;
   Note that the 'history_of_appended_files'-attribute is only created,
and will only exist, in a file that is, or descends from a file that
was, appended to.  The optional switch '-h' (or '--hst' or '--history')
also overrides automatically appending the 'history_of_appended_files'
attribute.

   ---------- Footnotes ----------

   (1) Note that before version 4.5.0, NCO could, in append ('-A') mode
only, inadvertently overwrite the global metadata (including 'history')
of the output file with that of the input file.  This is opposite the
behavior most would want.


File: nco.info,  Node: File List Attributes,  Next: CF Conventions,  Prev: History Attribute,  Up: Shared features

3.41 File List Attributes
=========================

Availability: 'nces', 'ncecat', 'ncra', 'ncrcat'
Short options: '-H'
Long options: '--fl_lst_in', '--file_list'
   Many methods of specifying large numbers of input file names pass
these names via pipes, encodings, or argument transfer programs (*note
Large Numbers of Files::).  When these methods are used, the input file
list is not explicitly passed on the command line.  This results in a
loss of information since the 'history' attribute no longer contains the
exact command by which the file was created.

   NCO solves this dilemma by archiving input file list attributes.
When the input file list to a multi-file operator is specified via
'stdin', the operator, by default, attaches two global attributes to any
file they create or modify.  The 'nco_input_file_number' global
attribute contains the number of input files, and 'nco_input_file_list'
contains the file names, specified as standard input to the multi-file
operator.  This information helps to verify that all input files the
user thinks were piped through 'stdin' actually arrived.  Without the
'nco_input_file_list' attribute, the information is lost forever and the
"chain of evidence" would be broken.

   The '-H' switch overrides (turns off) the default behavior of writing
the input file list global attributes when input is from 'stdin'.  The
'-h' switch does this too, and turns off the 'history' attribute as well
(*note History Attribute::).  Hence both switches allows space-conscious
users to avoid storing what may amount to many thousands of filenames in
a metadata attribute.


File: nco.info,  Node: CF Conventions,  Next: ARM Conventions,  Prev: File List Attributes,  Up: Shared features

3.42 CF Conventions
===================

Availability: 'ncbo', 'nces', 'ncecat', 'ncflint', 'ncpdq', 'ncra',
'ncwa'
Short options: None
   NCO recognizes some Climate and Forecast (CF) metadata conventions,
and applies special rules to such data.  NCO was contemporaneous with
COARDS and still contains some rules to handle older model datasets that
pre-date CF, such as NCAR CCM and early CCSM datasets.  Such datasets
may not contain an explicit 'Conventions' attribute (e.g., 'CF-1.0').
Nevertheless, we refer to all such metadata collectively as CF metadata.
Skip this section if you never work with CF metadata.

   The latest CF netCDF conventions are described here
(http://cfconventions.org/1.7.html).  Most CF netCDF conventions are
transparent to NCO.  There are no known pitfalls associated with using
any NCO operator on files adhering to these conventions.  NCO applies
some rules that are not in CF, or anywhere else, because experience
shows that they simplify data analysis, and stay true to the NCO mantra
to do what users want.

   Here is a general sense of NCO's CF-support:
   * Understand and implement NUG recommendations such as the history
     attribute, packing conventions, and attention to units.
   * Special handling of variables designated as coordinates, bounds, or
     ancillary variables, so that users subsetting a certain variable
     automatically obtain all related variables.
   * Special handling and prevention of meaningless operations (e.g.,
     the root-mean-square of latitude) so that coordinates and bounds
     preserve meaningful information even as normal (non-coordinate)
     fields are statistically transformed.
   * Understand units and certain calendars so that hyperslabs may be
     specified in physical units, and so that user needs not manually
     decode per-file time specifications.
   * Understand auxiliary coordinates so that irregular hyperslabs may
     be specified on complex geometric grids.
   * Check for CF-compliance on netCDF3 and netCDF4 and HDF files.
   * Convert netCDF4 and HDF files to netCDF3 for strict CF-compliance.
   Finally, a main use of NCO is to "produce CF", i.e., to improve
CF-compliance by annotating metadata, renaming objects (attributes,
variables, and dimensions), permuting and inverting dimensions,
recomputing values, and data compression.

   Currently, NCO determines whether a datafile is a CF output datafile
simply by checking (case-insensitively) whether the value of the global
attribute 'Conventions' (if any) equals 'CF-1.0' or 'NCAR-CSM' Should
'Conventions' equal either of these in the (first) INPUT-FILE, NCO will
apply special rules to certain variables because of their usual meaning
in CF files.  NCO will not average the following variables often found
in CF files: 'ntrm', 'ntrn', 'ntrk', 'ndbase', 'nsbase', 'nbdate',
'nbsec', 'mdt', 'mhisf'.  These variables contain scalar metadata such
as the resolution of the host geophysical model and it makes no sense to
change their values.

   Furthermore, the "size and rank-preserving arithmetic operators" try
not to operate on certain grid properties.  These operators are 'ncap2',
'ncbo', 'nces', 'ncflint', and 'ncpdq' (when used for packing, not for
permutation).  These operators do not operate, by default, on (i.e.,
add, subtract, pack, etc.)  the following variables: 'ORO', 'area',
'datesec', 'date', 'gw', 'hyai', 'hyam', 'hybi'.  'hybm', 'lat_bnds',
'lon_bnds', 'msk_*', and 'wgt_*'.  These variables represent Gaussian
weights, land/sea masks, time fields, hybrid pressure coefficients, and
latititude/longitude boundaries.  We call these fields non-coordinate
"grid properties".  Coordinate grid properties are easy to identify
because they are coordinate variables such as 'latitude' and
'longitude'.

   Users usually want _all_ grid properties to remain unaltered in the
output file.  To be treated as a grid property, the variable name must
_exactly_ match a name in the above list, or be a coordinate variable.
Handling of 'msk_*' and 'wgt_*' is exceptional in that _any_ variable
whose name starts with 'msk_' or 'wgt_' is considered to be a "mask" or
a "weight" and is thus preserved (not operated on when arithmetic can be
avoided).

   As of NCO version 4.7.7 (September, 2018), NCO began to explicitly
identify files adhering to the MPAS convention.  These files have a
global attribute 'Conventions' attribute that contains the string or
'MPAS'.  Size and rank-preserving arithmetic operators will not operate
on these MPAS non-coordinate grid properties: 'angleEdge', 'areaCell',
'areaTriangle', 'cellMask', 'cellsOnCell', 'cellsOnEdge',
'cellsOnVertex', 'dcEdge', 'dvEdge', 'edgesOnCell', 'edgesOnEdge',
'edgesOnVertex', 'indexToCellID', 'indexToEdgeID', 'indexToVertexID',
'kiteAreasOnVertex', 'latCell', 'latEdge', 'latVertex', 'lonCell',
'lonEdge', 'lonVertex', 'maxLevelCell', 'meshDensity', 'nEdgesOnCell',
'nEdgesOnEdge', 'vertexMask', 'verticesOnCell', 'verticesOnEdge',
'weightsOnEdge', 'xCell', 'xEdge', 'xVertex', 'yCell', 'yEdge',
'yVertex', 'zCell', 'zEdge', and 'zVertex'.

   As of NCO version 4.5.0 (June, 2015), NCO began to support behavior
required for the DOE E3SM/ACME program, and we refer to these rules
collectively as the E3SM/ACME convention.  The first E3SM/ACME rule
implemented is that the contents of INPUT-FILE variables named
'date_written' and 'time_written', if any, will be updated to the
current system-supplied (with 'gmtime()') GMT-time as the variables are
copied to the OUTPUT-FILE.

   You must spoof NCO if you would like any grid properties or other
special CF fields processed normally.  For example rename the variables
first with 'ncrename', or alter the 'Conventions' attribute.

   As of NCO version 4.0.8 (April, 2011), NCO supports the CF 'bounds'
convention for cell boundaries described here
(http://cfconventions.org/cf-conventions/cf-conventions.html#cell-boundaries).
This convention allows coordinate variables (including multidimensional
coordinates) to describe the boundaries of their cells.  This is done by
naming the variable which contains the bounds in in the 'bounds'
attribute.  Note that coordinates of rank N have bounds of rank N+1.
NCO-generated subsets of CF-compliant files with 'bounds' attributes
will include the coordinates specified by the 'bounds' attribute, if
any.  Hence the subsets will themselves be CF-compliant.  Bounds are
subject to the user-specified override switches (including '-c' and
'-C') described in *note Subsetting Coordinate Variables::.

   The CAM/EAM family of atmospheric models does not output a 'bounds'
variable or attribute corresponding to the 'lev' coordinate.  This
prevents NCO from activating its CF bounds machinery when 'lev' is
extracted.  As of version 4.7.7 (September, 2018), NCO works around this
by outputting the 'ilev' coordinate (and 'hyai', 'hybi') whenever the
'lev' coordinate is also output.

   As of NCO version 4.4.9 (May, 2015), NCO supports the CF
'climatology' convention for climatological statistics described here
(http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/build/cf-conventions.html#climatological-statistics).
This convention allows coordinate variables (including multidimensional
coordinates) to describe the (possibly nested) periods and statistical
methods of their associated statistics.  This is done by naming the
variable which contains the periods and methods in the 'climatology'
attribute.  Note that coordinates of rank N have climatology bounds of
rank N+1.  NCO-generated subsets of CF-compliant files with
'climatology' attributes will include the variables specified by the
'climatology' attribute, if any.  Hence the subsets will themselves be
CF-compliant.  Climatology variables are subject to the user-specified
override switches (including '-c' and '-C') described in *note
Subsetting Coordinate Variables::.

   As of NCO version 4.4.5 (July, 2014), NCO supports the CF
'ancillary_variables' convention for described here
(http://cfconventions.org/cf-conventions/cf-conventions.html#ancillary-data).
This convention allows ancillary variables to be associated with one or
more primary variables.  NCO attaches any such variables to the
extraction list along with the primary variable and its usual
(one-dimensional) coordinates, if any.  Ancillary variables are subject
to the user-specified override switches (including '-c' and '-C')
described in *note Subsetting Coordinate Variables::.

   As of NCO version 4.6.4 (January, 2017), NCO supports the CF
'cell_measures' convention described here
(http://cfconventions.org/cf-conventions/cf-conventions.html#cell-measures).
This convention allows variables to indicate which other variable or
variables contains area or volume information about a gridcell.  These
measures variables are pointed to by the 'cell_measures' attribute.  The
CDL specification of a measures variable for area looks like
     orog:cell_measures = "area: areacella"
   where 'areacella' is the name of the measures variable.  Unless the
default behavior is overridden, NCO attaches any measures variables to
the extraction list along with the primary variable and other associated
variables.  By definition, measures variables are a subset of the rank
of the variable they measure.  The most common case is that the measures
variable for area is the same size as 2D fields (like surface air
temperature) and much smaller than 3D fields (like full air
temperature).  In such cases the measures variable might occupy 50% of
the space of a dataset consisting of only one 2D field.  Extraction of
measures variables is subject to the user-specified override switches
(including '-c' and '-C') described in *note Subsetting Coordinate
Variables::.  To conserve space without sacrificing too much metadata,
NCO makes it possible to override the extraction of measures variables
independent of extracting other associated variables.  Override the
default with '--no_cell_measures' or '--no_cll_msr'.  These options are
available in all operators that perform subsetting (i.e., all operators
except 'ncatted' and 'ncrename').

   As of NCO version 4.6.4 (January, 2017), NCO supports the CF
'formula_terms' convention described here
(http://cfconventions.org/cf-conventions/cf-conventions.html#formula-terms).
This convention encodes formulas used to construct (usually vertical)
coordinate grids.  The CDL specification of a vertical coordinate
formula for looks like
     lev:standard_name = "atmosphere_hybrid_sigma_pressure_coordinate"
     lev:formula_terms = "a: hyam b: hybm p0: P0 ps: PS"
   where 'standard_name' contains the standardized name of the formula
variable and 'formula_terms' contains a list of the variables used,
called formula variables.  Above the formula variables are 'hyam',
'hybm', 'P0', and 'PS'.  Unless the default behavior is overridden, NCO
attaches any formula variables to the extraction list along with the
primary variable and other associated variables.  By definition, formula
variables are a subset of the rank of the variable they define.  One
common case is that the formula variables for constructing a 3D height
grid involves a 2D variable (like surface pressure, or elevation).  In
such cases the formula variables typically constitute only a small
fraction of a dataset consisting of one 3D field.  Extraction of formula
variables is subject to the user-specified override switches (including
'-c' and '-C') described in *note Subsetting Coordinate Variables::.  To
conserve space without sacrificing too much metadata, NCO makes it
possible to override the extraction of formula variables independent of
extracting other associated variables.  Override the default with
'--no_formula_terms' or '--no_frm_trm'.  These options are available in
all operators that perform subsetting (i.e., all operators except
'ncatted' and 'ncrename').

   As of NCO version 4.6.0 (May, 2016), NCO supports the CF
'grid_mapping' convention for described here
(http://cfconventions.org/cf-conventions/cf-conventions.html#grid-mappings-and-projections).
This convention allows descriptions of map-projections to be associated
with variables.  NCO attaches any such map-projection variables to the
extraction list along with the primary variable and its usual
(one-dimensional) coordinates, if any.  Map-projection variables are
subject to the user-specified override switches (including '-c' and
'-C') described in *note Subsetting Coordinate Variables::.

   As of NCO version 3.9.6 (January, 2009), NCO supports the CF
'coordinates' convention described here
(http://cfconventions.org/cf-conventions/cf-conventions.html#coordinate-system).
This convention allows variables to specify additional coordinates
(including mult-idimensional coordinates) in a space-separated string
attribute named 'coordinates'.  NCO attaches any such coordinates to the
extraction list along with the variable and its usual (one-dimensional)
coordinates, if any.  These auxiliary coordinates are subject to the
user-specified override switches (including '-c' and '-C') described in
*note Subsetting Coordinate Variables::.

   Elimination of reduced dimensions from the 'coordinates' attribute
helps ensure that rank-reduced variables become completely independent
from their former dimensions.  As of NCO version 4.4.9 (May, 2015), NCO
may modify the 'coordinates' attribute to assist this.  In particular,
'ncwa' eliminates from the 'coordinates' attribute any dimension that it
collapses, e.g., by averaging.  The former presence of this dimension
will usually be indicated by the CF 'cell_methods' convention described
here
(http://cfconventions.org/cf-conventions/cf-conventions.html#cell-methods).
Hence the CF 'cell_methods' and 'coordinates' conventions can be said to
work in tandem to characterize the state and history of a variable's
analysis.

   As of NCO version 4.4.2 (February, 2014), NCO supports some of the CF
'cell_methods' convention
(http://cfconventions.org/cf-conventions/cf-conventions.html#cell-methods)
to describe the analysis procedures that have been applied to data.  The
convention creates (or appends to an existing) 'cell_methods' attribute
a space-separated list of couplets of the form DMN: OP where DMN is a
comma-separated list of dimensions previously contained in the variable
that have been reduced by the arithmetic operation OP.  For example, the
'cell_methods' value 'time: mean' says that the variable in question was
averaged over the 'time' dimension.  In such cases 'time' will either be
a scalar variable or a degenerate dimension or coordinate.  This simply
means that it has been averaged-over.  The value 'time, lon: mean lat:
max' says that the variable in question is the maximum zonal mean of the
time averaged original variable.  Which is to say that the variable was
first averaged over time and longitude, and then the residual
latitudinal array was reduced by choosing the maximum value.  Since the
'cell methods' convention may alter metadata in an undesirable (or
possibly incorrect) fashion, we provide switches to ensure it is always
or never used.  Use long-options '--cll_mth' or '--cell_methods' to
invoke the algorithm (true by default), and options '--no_cll_mth' or
'--no_cell_methods' to turn it off.  These options are only available in
the operators 'ncwa' and 'ncra'.


File: nco.info,  Node: ARM Conventions,  Next: Operator Version,  Prev: CF Conventions,  Up: Shared features

3.43 ARM Conventions
====================

Availability: 'ncrcat'
Short options: None
   'ncrcat' has been programmed to correctly handle data files which
utilize the Atmospheric Radiation Measurement (ARM) Program convention
(http://www.arm.gov/data/time.stm) for time and time offsets.  If you do
not work with ARM data then you may skip this section.  ARM data files
store time information in two variables, a scalar, 'base_time', and a
record variable, 'time_offset'.  Subtle but serious problems can arise
when these type of files are blindly concatenated without CF or ARM
support.  'NCO' implements rebasing (*note Rebasing Time Coordinate::)
as necessary on both CF and ARM files.  Rebasing chains together
consecutive INPUT-FILES and produces an OUTPUT-FILE which contains the
correct time information.  For ARM files this is expecially complex
because the time coordinates are often stored as type 'NC_CHAR'.
Currently, 'ncrcat' determines whether a datafile is an ARM datafile
simply by testing for the existence of the variables 'base_time',
'time_offset', and the dimension 'time'.  If these are found in the
INPUT-FILE then 'ncrcat' will automatically perform two non-standard,
but hopefully useful, procedures.  First, 'ncrcat' will ensure that
values of 'time_offset' appearing in the OUTPUT-FILE are relative to the
'base_time' appearing in the first INPUT-FILE (and presumably, though
not necessarily, also appearing in the OUTPUT-FILE).  Second, if a
coordinate variable named 'time' is not found in the INPUT-FILES, then
'ncrcat' automatically creates the 'time' coordinate in the OUTPUT-FILE.
The values of 'time' are defined by the ARM conventions TIME = BASE_TIME
+ TIME_OFFSET.  Thus, if OUTPUT-FILE contains the 'time_offset'
variable, it will also contain the 'time' coordinate.  A short message
is added to the 'history' global attribute whenever these ARM-specific
procedures are executed.


File: nco.info,  Node: Operator Version,  Prev: ARM Conventions,  Up: Shared features

3.44 Operator Version
=====================

Availability: All operators
Short options: '-r'
Long options: '--revision', '--version', or '--vrs'
   All operators can be told to print their version information, library
version, copyright notice, and compile-time configuration with the '-r'
switch, or its long-option equivalent 'revision'.  The '--version' or
'--vrs' switches print the operator version information only.  The
internal version number varies between operators, and indicates the most
recent change to a particular operator's source code.  This is useful in
making sure you are working with the most recent operators.  The version
of NCO you are using might be, e.g., '3.9.5'.  Using '-r' on, say,
'ncks', produces something like 'NCO netCDF Operators version "3.9.5"
last modified 2008/05/11 built May 12 2008 on neige by zender Copyright
(C) 1995--2008 Charlie Zender ncks version 20090918'.  This tells you
that 'ncks' contains all patches up to version '3.9.5', which dates from
May 11, 2008.


File: nco.info,  Node: Reference Manual,  Next: Contributing,  Prev: Shared features,  Up: Top

4 Reference Manual
******************

This chapter presents reference pages for each of the operators
individually.  The operators are presented in alphabetical order.  All
valid command line switches are included in the syntax statement.
Recall that descriptions of many of these command line switches are
provided only in *note Shared features::, to avoid redundancy.  Only
options specific to, or most useful with, a particular operator are
described in any detail in the sections below.

* Menu:

* ncap2 netCDF Arithmetic Processor::
* ncatted netCDF Attribute Editor::
* ncbo netCDF Binary Operator::
* ncclimo netCDF Climatology Generator::
* ncecat netCDF Ensemble Concatenator::
* nces netCDF Ensemble Statistics::
* ncflint netCDF File Interpolator::
* ncks netCDF Kitchen Sink::
* ncpdq netCDF Permute Dimensions Quickly::
* ncra netCDF Record Averager::
* ncrcat netCDF Record Concatenator::
* ncremap netCDF Remapper::
* ncrename netCDF Renamer::
* ncwa netCDF Weighted Averager::


File: nco.info,  Node: ncap2 netCDF Arithmetic Processor,  Next: ncatted netCDF Attribute Editor,  Prev: Reference Manual,  Up: Reference Manual

4.1 'ncap2' netCDF Arithmetic Processor
=======================================

'ncap2' understands a relatively full-featured language of operations,
including loops, conditionals, arrays, and math functions.  'ncap2' is
the most rapidly changing NCO operator and its documentation is
incomplete.  The distribution file 'data/ncap2_tst.nco' contains an
up-to-date overview of its syntax and capabilities.  The 'data/*.nco'
distribution files (especially 'bin_cnt.nco', 'psd_wrf.nco', and
'rgr.nco') contain in-depth examples of 'ncap2' solutions to complex
problems.

SYNTAX
     ncap2 [-3] [-4] [-5] [-6] [-7] [-A] [-C] [-c]
     [-D DBG] [-F] [-f] [--glb ...] [-h] [--hdf] [--hdr_pad NBR] [--hpss]
     [-L DFL_LVL] [-l PATH] [--no_tmp_fl] [-O] [-o OUTPUT-FILE]
     [-p PATH] [-R] [-r] [--ram_all]
     [-s ALGEBRA] [-S FL.NCO] [-t THR_NBR] [-v]
     [INPUT-FILE] [OUTPUT-FILE]

DESCRIPTION

   'ncap2' arithmetically processes netCDF files.  'ncap2' is the
successor to 'ncap' which was put into maintenance mode in November,
2006, and completely removed from NCO in March, 2018.  This
documentation refers to 'ncap2' implements its own domain-specific
language to produc a powerful superset 'ncap'-functionality.  'ncap2'
may be renamed 'ncap' one day!  The processing instructions are
contained either in the NCO script file 'fl.nco' or in a sequence of
command line arguments.  The options '-s' (or long options '--spt' or
'--script') are used for in-line scripts and '-S' (or long options
'--fl_spt' or '--script-file') are used to provide the filename where
(usually multiple) scripting commands are pre-stored.  'ncap2' was
written to perform arbitrary algebraic transformations of data and
archive the results as easily as possible.  *Note Missing Values::, for
treatment of missing values.  The results of the algebraic manipulations
are called "derived fields".

   Unlike the other operators, 'ncap2' does not accept a list of
variables to be operated on as an argument to '-v' (*note Subsetting
Files::).  Rather, the '-v' switch takes no arguments and indicates that
'ncap2' should output _only_ user-defined variables.  'ncap2' neither
accepts nor understands the -X switch.  NB: As of 20120515, 'ncap2' is
unable to append to files that already contain the appended dimensions.

   Providing a name for OUTPUT-FILE is optional if INPUT-FILE is a
netCDF3 format, in which case 'ncap2' attempts to write modifications
directly to INPUT-FILE (similar to the behavior of 'ncrename' and
'ncatted').  Format-constraints prevent this type of appending from
working on a netCDF4 format INPUT-FILE.  In any case, reading and
writing the same file can be risky and lead to unexpected consequences
(since the file is being both read and written), so in normal usage we
recommend providing OUTPUT-FILE (which can be the same as INPUT-FILE
since the changes are first written to an intermediate file).

   As of NCO version 4.8.0 (released May, 2019), 'ncap2' does not
require that INPUT-FILE be specified when OUTPUT-FILE has no dependency
on it.  Prior to this, 'ncap2' required users to specify a dummy
INPUT-FILE even if it was not used to construct OUTPUT-FILE.  Input
files are always read by 'ncap2', and dummy input files were read though
not used for anything nor modified.  Now
     ncap2 -s 'quark=1' ~/foo.nc # Create new foo.nc
     ncap2 -s 'print(quark)' ~/foo.nc # Print existing foo.nc
     ncap2 -O -s 'quark=1' ~/foo.nc # Overwrite old with new foo.nc
     ncap2 -s 'quark=1' ~/foo.nc ~/foo.nc # Add to old foo.nc

* Menu:

* Syntax of ncap2 statements::
* Expressions::
* Dimensions::
* Left hand casting::
* Arrays and hyperslabs::
* Attributes::
* Value List::
* Number literals::
* if statement::
* Print & String methods::
* Missing values ncap2::
* Methods and functions::
* RAM variables::
* Where statement::
* Loops::
* Include files::
* Sort methods::
* UDUnits script::
* Vpointer::
* Irregular grids::
* Bilinear interpolation::
* GSL special functions::
* GSL interpolation::
* GSL least-squares fitting::
* GSL statistics::
* GSL random number generation::
* Examples ncap2::
* Intrinsic mathematical methods::
* Operator precedence and associativity ::
* ID Quoting::
* make_bounds() function::
* solar_zenith_angle function::

   Defining new variables in terms of existing variables is a powerful
feature of 'ncap2'.  Derived fields inherit the metadata (i.e.,
attributes) of their ancestors, if any, in the script or input file.
When the derived field is completely new (no identically-named ancestors
exist), then it inherits the metadata (if any) of the left-most variable
on the right hand side of the defining expression.  This metadata
inheritance is called "attribute propagation".  Attribute propagation is
intended to facilitate well-documented data analysis, and we welcome
suggestions to improve this feature.

   The only exception to this rule of attribute propagation is in cases
of left hand casting (*note Left hand casting::).  The user must
manually define the proper metadata for variables defined using left
hand casting.


File: nco.info,  Node: Syntax of ncap2 statements,  Next: Expressions,  Prev: ncap2 netCDF Arithmetic Processor,  Up: ncap2 netCDF Arithmetic Processor

4.1.1 Syntax of 'ncap2' statements
----------------------------------

Mastering 'ncap2' is relatively simple.  Each valid statement STATEMENT
consists of standard forward algebraic expression.  The 'fl.nco', if
present, is simply a list of such statements, whitespace, and comments.
The syntax of statements is most like the computer language C. The
following characteristics of C are preserved:
Array syntax
     Arrays elements are placed within '[]' characters;
Array indexing
     Arrays are 0-based;
Array storage
     Last dimension is most rapidly varying;
Assignment statements
     A semi-colon ';' indicates the end of an assignment statement.
Comments
     Multi-line comments are enclosed within '/* */' characters.  Single
     line comments are preceded by '//' characters.
Nesting
     Files may be nested in scripts using '#include SCRIPT'.  The
     '#include' command is not followed by a semi-colon because it is a
     pre-processor directive, not an assignment statement.  The filename
     'script' is interpreted relative to the run directory.
Attribute syntax
     The at-sign '@' is used to delineate an attribute name from a
     variable name.


File: nco.info,  Node: Expressions,  Next: Dimensions,  Prev: Syntax of ncap2 statements,  Up: ncap2 netCDF Arithmetic Processor

4.1.2 Expressions
-----------------

Expressions are the fundamental building block of 'ncap2'.  Expressions
are composed of variables, numbers, literals, and attributes.  The
following C operators are "overloaded" and work with scalars and
multi-dimensional arrays:
     Arithmetic Operators: * / % + - ^
     Binary Operators:     > >= < <= == != == || && >> <<
     Unary Operators:      + - ++ -- !
     Conditional Operator: exp1 ? exp2 : exp3
     Assign Operators:     = += -= /= *=

   In the following section a "variable" also refers to a number literal
which is read in as a scalar variable:

   *Arithmetic and Binary Operators *

   Consider _var1 'op' var2_

   *Precision*
   * When both operands are variables, the result has the precision of
     the higher precision operand.
   * When one operand is a variable and the other an attribute, the
     result has the precision of the variable.
   * When both operands are attributes, the result has the precision of
     the more precise attribute.
   * The exponentiation operator "^" is an exception to the above rules.
     When both operands have type less than 'NC_FLOAT', the result is
     'NC_FLOAT'.  When either type is 'NC_DOUBLE', the result is also
     'NC_DOUBLE'.

   *Rank*
   * The Rank of the result is generally equal to Rank of the operand
     that has the greatest number of dimensions.
   * If the dimensions in var2 are a subset of the dimensions in var1
     then its possible to make var2 conform to var1 through broadcasting
     and or dimension reordering.
   * Broadcasting a variable means creating data in non-existing
     dimensions by copying data in existing dimensions.
   * More specifically: If the numbers of dimensions in var1 is greater
     than or equal to the number of dimensions in var2 then an attempt
     is made to make var2 conform to var1 ,else var1 is made to conform
     to var2.  If conformance is not possible then an error message will
     be emitted and script execution will cease.

Even though the logical operators return True(1) or False(0) they are
treated in the same way as the arithmetic operators with regard to
precision and rank.
Examples:

     dimensions: time=10, lat=2, lon=4
     Suppose we have the two variables:
     
     double  P(time,lat,lon);
     float   PZ0(lon,lat);  // PZ0=1,2,3,4,5,6,7,8;
     
     Consider now the expression:
      PZ=P-PZ0
     
     PZ0 is made to conform to P and the result is
     PZ0 =
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
        1,3,5,7,2,4,6,8,
     
     Once the expression is evaluated then PZ will be of type double;
     
     Consider now 
      start=four-att_var@double_att;  // start =-69  and is of type intger;
      four_pow=four^3.0f               // four_pow=64 and is of type float  
      three_nw=three_dmn_var_sht*1.0f; // type is now float
      start@n1=att_var@short_att*att_var@int_att; 
                                       // start@n1=5329 and is type int 

*Binary Operators*
Unlike C the binary operators return an array of values.  There is no
such thing as short circuiting with the AND/OR operators.  Missing
values are carried into the result in the same way they are with the
arithmetic operators.  When an expression is evaluated in an if() the
missing values are treated as true.
The binary operators are, in order of precedence:

     !   Logical Not
     ----------------------------
     <<  Less Than Selection
     >>  Greater Than Selection
     ----------------------------
     >   Greater than
     >=  Greater than or equal to
     <   Less than
     <=  Less than or equal to
     ----------------------------
     ==  Equal to
     !=  Not equal to
     ----------------------------
     &&  Logical AND
     ----------------------------
     ||  Logical OR
     ----------------------------

   To see all operators: *note Operator precedence and associativity::
Examples:
     tm1=time>2 && time <7;  // tm1=0, 0, 1, 1, 1, 1, 0, 0, 0, 0 double
     tm2=time==3 || time>=6; // tm2=0, 0, 1, 0, 0, 1, 1, 1, 1, 1 double
     tm3=int(!tm1);          // tm3=1, 1, 0, 0, 0, 0, 1, 1, 1, 1 int
     tm4=tm1 && tm2;         // tm4=0, 0, 1, 0, 0, 1, 0, 0, 0, 0 double
     tm5=!tm4;               // tm5=1, 1, 0, 1, 1, 0, 1, 1, 1, 1 double

*Regular Assign Operator*
_var1 '=' exp1_
If var1 does not already exist in Input or Output then var1 is written
to Output with the values, type and dimensions from expr1.  If var1 is
in Input only it is copied to Output first.  Once the var is in Ouptut
then the only reqirement on expr1 is that the number of elements must
match the number already on disk.  The type of expr1 is converted as
necessary to the disk type.

   If you wish to change the type or shape of a variable in Input then
you must cast the variable.  See *note Left hand casting::
     time[time]=time.int();
     three_dmn_var_dbl[time,lon,lat]=666L;

* Other Assign Operators +=,-=,*=./= *
_var1 'ass_op' exp1 _
if exp1 is a variable and it doesn't conform to var1 then an attempt is
made to make it conform to var1.  If exp1 is an attribute it must have
unity size or else have the same number of elements as var1.  If expr1
has a different type to var1 the it is converted to the var1 type.
     z1=four+=one*=10 // z1=14 four=14 one=10;
     time-=2          // time= -1,0,1,2,3,4,5,6,7,8

*Increment/Decrement Operators
* These work in a similar fashion to their regular C counterparts.  If
say the variable 'four' is input only then the statement '++four'
effectively means read 'four' from input increment each element by one,
then write the new values to Output;

   Example:
     n2=++four;   n2=5, four=5
     n3=one--+20; n3=21  one=0;
     n4=--time;   n4=time=0.,1.,2.,3.,4.,5.,6.,7.,8.,9.;

*Conditional Operator ?:*
_exp1 ?  exp2 : exp3 _
The conditional operator (or ternary Operator) is a succinct way of
writing an if/then/else.  If exp1 evaluates to true then exp2 is
returned else exp3 is returned.

   Example:
     weight_avg=weight.avg();
     weight_avg@units= (weight_avg == 1 ? "kilo" : "kilos");  
     PS_nw=PS-(PS.min() > 100000 ? 100000 : 0);

*Clipping Operators*
<< Less-than Clipping
     For arrays, the less-than selection operator selects all values in
     the left operand that are less than the corresponding value in the
     right operand.  If the value of the left side is greater than or
     equal to the corresponding value of the right side, then the right
     side value is placed in the result
>> Greater-than Clipping
     For arrays, the greater-than selection operator selects all values
     in the left operand that are greater than the corresponding value
     in the right operand.  If the value of the left side is less than
     or equal to the corresponding value of the right side, then the
     right side value is placed in the result.

   Example:
     RDM2=RDM >> 100.0 // 100,100,100,100,126,126,100,100,100,100 double
     RDM2=RDM <<  90s  // 1, 9, 36, 84, 90, 90, 84, 36, 9, 1 int


File: nco.info,  Node: Dimensions,  Next: Left hand casting,  Prev: Expressions,  Up: ncap2 netCDF Arithmetic Processor

4.1.3 Dimensions
----------------

Dimensions are defined in Output using the 'defdim()' function.
     defdim("cnt",10); # Dimension size is fixed by default
     defdim("cnt",10,NC_UNLIMITED); # Dimension is unlimited (record dimension)
     defdim("cnt",10,0); # Dimension is unlimited (record dimension)
     defdim("cnt",10,1); # Dimension size is fixed
     defdim("cnt",10,737); # All non-zero values indicate dimension size is fixed

   This dimension name must then be prefixed with a dollar-sign '$' when
referred to in method arguments or left-hand-casting, e.g.,
     new_var[$cnt]=time;
     temperature[$time,$lat,$lon]=35.5;
     temp_avg=temperature.avg($time);

   The 'size' method allows dimension sizes to be used in arithmetic
expressions:
     time_avg=time.total()/$time.size;

   Increase the size of a new variable by one and set new member to
zero:
     defdim("cnt_new",$cnt.size+1);
     new_var[$cnt_new]=0.0;
     new_var(0:($cnt_new.size-2))=old_var;

   To define an unlimited dimension, simply set the size to zero
     defdim("time2",0)

*Dimension Abbreviations
* It is possible to use dimension abbreviations as method arguments:
'$0' is the first dimension of a variable
'$1' is the second dimension of a variable
'$n' is the n+1 dimension of a variable

     float four_dmn_rec_var(time,lat,lev,lon);
     double three_dmn_var_dbl(time,lat,lon);

     four_nw=four_dmn_rev_var.reverse($time,$lon)
     four_nw=four_dmn_rec_var.reverse($0,$3);

     four_avg=four_dmn_rec_var.avg($lat,$lev);
     four_avg=four_dmn_rec_var.avg($1,$2);

     three_mw=three_dmn_var_dbl.permute($time,$lon,$lat);
     three_mw=three_dmn_var_dbl.permute($0,$2,$1);

*ID Quoting
* If the dimension name contains non-regular characters use ID quoting:
See *note ID Quoting::
     defdim("a--list.A",10);
     A1['$a--list.A']=30.0;

*GOTCHA
* It is not possible to manually define in Output any dimensions that
exist in Input.  When a variable from Input appears in an expression or
statement its dimensions in Input are automagically copied to Output (if
they are not already present)


File: nco.info,  Node: Left hand casting,  Next: Arrays and hyperslabs,  Prev: Dimensions,  Up: ncap2 netCDF Arithmetic Processor

4.1.4 Left hand casting
-----------------------

The following examples demonstrate the utility of the "left hand
casting" ability of 'ncap2'.  Consider first this simple, artificial,
example.  If LAT and LON are one dimensional coordinates of dimensions
LAT and LON, respectively, then addition of these two one-dimensional
arrays is intrinsically ill-defined because whether LAT_LON should be
dimensioned LAT by LON or LON by LAT is ambiguous (assuming that
addition is to remain a "commutative" procedure, i.e., one that does not
depend on the order of its arguments).  Differing dimensions are said to
be "orthogonal" to one another, and sets of dimensions which are
mutually exclusive are orthogonal as a set and any arithmetic operation
between variables in orthogonal dimensional spaces is ambiguous without
further information.

   The ambiguity may be resolved by enumerating the desired dimension
ordering of the output expression inside square brackets on the left
hand side (LHS) of the equals sign.  This is called "left hand casting"
because the user resolves the dimensional ordering of the RHS of the
expression by specifying the desired ordering on the LHS.
     ncap2 -s 'lat_lon[lat,lon]=lat+lon' in.nc out.nc
     ncap2 -s 'lon_lat[lon,lat]=lat+lon' in.nc out.nc
   The explicit list of dimensions on the LHS, '[lat,lon]' resolves the
otherwise ambiguous ordering of dimensions in LAT_LON.  In effect, the
LHS "casts" its rank properties onto the RHS.  Without LHS casting, the
dimensional ordering of LAT_LON would be undefined and, hopefully,
'ncap2' would print an error message.

   Consider now a slightly more complex example.  In geophysical models,
a coordinate system based on a blend of terrain-following and
density-following surfaces is called a "hybrid coordinate system".  In
this coordinate system, four variables must be manipulated to obtain the
pressure of the vertical coordinate: PO is the domain-mean surface
pressure offset (a scalar), PS is the local (time-varying) surface
pressure (usually two horizontal spatial dimensions, i.e.  latitude by
longitude), HYAM is the weight given to surfaces of constant density
(one spatial dimension, pressure, which is orthogonal to the horizontal
dimensions), and HYBM is the weight given to surfaces of constant
elevation (also one spatial dimension).  This command constructs a
four-dimensional pressure 'prs_mdp' from the four input variables of
mixed rank and orthogonality:
     ncap2 -s 'prs_mdp[time,lat,lon,lev]=P0*hyam+PS*hybm' in.nc out.nc
   Manipulating the four fields which define the pressure in a hybrid
coordinate system is easy with left hand casting.

   Finally, we show how to use interface quantities to define midpoint
quantities.  In particular, we will define interface pressures using the
standard CESM output hybrid coordinate parameters, and then difference
those interface pressures to obtain the pressure difference between the
interfaces.  The pressure difference is necessary obtain gridcell mass
path and density (which are midpoint quantities).  Definitions are as in
the above example, with new variables HYAI and HYBI defined at grid cell
vertical interfaces (rather than midpoints like HYAM and HYBM).  The
approach naturally fits into two lines:
     cat > ~/pdel.nco << 'EOF'
     *prs_ntf[time,lat,lon,ilev]=P0*hyai+PS*hybi;
     // Requires NCO 4.5.4 and later:
     prs_dlt[time,lat,lon,lev]=prs_ntf(:,:,:,1:$ilev.size-1)-prs_ntf(:,:,:,0:$ilev.size-2);
     // Derived variable that require pressure thickness:
     // Divide by gravity to obtain total mass path in layer aka mpl [kg m-2] 
     mpl=prs_dlt/grv_sfc;
     // Multiply by mass mixing ratio to obtain mass path of constituent
     mpl_CO2=mpl*mmr_CO2;
     EOF
     ncap2 -O -v -S ~/pdel.nco ~/nco/data/in.nc ~/foo.nc
     ncks -O -C -v prs_dlt ~/foo.nc
   The first line defines the four-dimensional interface pressures
'prs_ntf' as a RAM variable because those are not desired in the output
file.  The second differences each pressure level from the pressure
above it to obtain the pressure difference.  This line employs both
left-hand casting and array hyperslabbing.  However, this syntax only
works with NCO version 4.5.4 (November, 2015) and later because earlier
versions require that LHS and RHS dimension names (not just sizes)
match.  From the pressure differences, one can obtain the mass path in
each layer as shown.

   Another reason to cast a variable is to modify the shape or type of a
variable already in Input
     gds_var[gds_crd]=gds_var.double();
     three_dmn_var_crd[lat,lon,lev]=10.0d;
     four[]=four.int();


File: nco.info,  Node: Arrays and hyperslabs,  Next: Attributes,  Prev: Left hand casting,  Up: ncap2 netCDF Arithmetic Processor

4.1.5 Arrays and hyperslabs
---------------------------

Generating a regularly spaced n-dimensional array with 'ncap2' is simple
with the 'array()' function.  The function comes in three (overloaded)
forms
     (A) var_out=array(val_srt, val_inc, $dmn_nm);           // One-dimensional output
     (B) var_out=array(val_srt, val_inc, var_tpl);           // Multi-dimensional output
     (C) var_out=array(val_srt, val_inc, /$dmn1,$dmn2...,$dmnN/); // Multi-dimensional output

"val_srt"
     Starting value of the array.  The TYPE of the array will be the
     TYPE of this starting value.
"val_inc"
     Spacing (or increment) between elements.
"var_tpl"
     Variable from which the array can derive its shape 1D or nD

*One-Dimensional Arrays
* Use form (A) or (B) for 1D arrays
     # var_out will be NC_DOUBLE:
     var_out=array(10.0,2,$time) // 10.5,12.5,14.5,16.5,18.5,20.5,22.5,24.5,26.5,28.5

     // var_out will be NC_UINT, and "shape" will duplicate "ilev"
     var_out=array(0ul,2,ilev) // 0,2,4,6

     // var_out will be NC_FLOAT
     var_out=array(99.0f,2.5,$lon) // 99,101.5,104,106.5

     // Create an array of zeros
     var_out=array(0,0,$time) // 0,0,0,0,0,0,0,0,0,0

     // Create array of ones
     var_out=array(1.0,0.0,$lon) // 1.0,1.0,1.0,1.0

*n-Dimensional Arrays
* Use form (B) or (C) for creating n-D arrays.
NB: In (C) the final argument is a list of dimensions

     // These are equivalent
     var_out=array(1.0, 2.0, three_dmn_var);
     var_out=array(1.0, 2.0,/$lat, $lev, $lon/);

     // var_out is NC_BYTE
     var_out=array(20b, -4, /$lat,$lon/); // 20,16,12,8,4,0,-4,-8

     srt=3.14159f;
     inc=srt/2.0f;
     var_out(srt,inc,var_2D_rrg);
     // 3.14159, 4.712385, 6.28318, 7.853975, 9.42477, 10.99557, 12.56636, 14.13716 ;

   Hyperslabs in 'ncap2' are more limited than hyperslabs with the other
NCO operators.  'ncap2' does not understand the shell command-line
syntax used to specify multi-slabs, wrapped co-ordinates, negative
stride or coordinate value limits.  However with a bit of syntactic
magic they are all are possible.  'ncap2' accepts (in fact, it requires)
N-hyperslab arguments for a variable of rank N:
     var1(arg1,arg2 ... argN);
   where each hyperslab argument is of the form
     start:end:stride
   and the arguments for different dimensions are separated by commas.
If START is omitted, it defaults to zero.  If END is omitted, it
defaults to dimension size minus one.  If STRIDE is omitted, it defaults
to one.

If a single value is present then it is assumed that that dimension
collapses to a single value (i.e., a cross-section).  The number of
hyperslab arguments MUST equal the variable's rank.

*Hyperslabs on the Right Hand Side of an assign
*

   A simple 1D example:
     ($time.size=10)
     od[$time]={20,22,24,26,28,30,32,34,36,38};
     
     od(7);     // 34
     od(7:);    // 34,36,38
     od(:7);    // 20,22,24,26,28,30,32,34 
     od(::4);   // 20,28,36
     od(1:6:2)  // 22,26,30
     od(:)      // 20,22,24,26,28,30,32,34,36,38 

   A more complex three dimensional example:
     ($lat.size=2,$lon.size=4)
     th[$time,$lat,$lon]=      
                               {1, 2, 3, 4, 5, 6, 7, 8,
                               9,10,11,12,13,14,15,16,
                               17,18,19,20,21,22,23,24,
                               -99,-99,-99,-99,-99,-99,-99,-99,
                               33,34,35,36,37,38,39,40,
                               41,42,43,44,45,46,47,48,
                               49,50,51,52,53,54,55,56,
                               -99,58,59,60,61,62,63,64,
                               65,66,67,68,69,70,71,72,
                               -99,74,75,76,77,78,79,-99 };
     
     th(1,1,3);        // 16
     th(2,0,:);        // 17, 18, 19, 20
     th(:,1,3);        // 8, 16, 24, -99, 40, 48, 56, 64, 72, -99 
     th(::5,:,0:3:2); // 1, 3, 5, 7, 41, 43, 45, 47

   If hyperslab arguments collapse to a single value (a cross-section
has been specified), then that dimension is removed from the returned
variable.  If all the values collapse then a scalar variable is
returned.  So, for example, the following is valid:
     th_nw=th(0,:,:)+th(9,:,:);
     // th_nw has dimensions $lon,$lat
     // NB: the time dimension has become degenerate

   The following is invalid:
     th_nw=th(0,:,0:1)+th(9,:,0:1);
   because the '$lon' dimension now only has two elements.  The above
can be calculated by using a LHS cast with '$lon_nw' as replacement dim
for '$lon':
     defdim("lon_nw",2);
     th_nw[$lat,$lon_nw]=th(0,:,0:1)+th(9,:,0:1);

*Hyperslabs on the Left Hand Side of an assign
* When hyperslabing on the LHS, the expression on the RHS must evaluate
to a scalar or a variable/attribute with the same number of elements as
the LHS hyperslab.  Set all elements of the last record to zero:
     th(9,:,:)=0.0;
   Set first element of each lon element to 1.0:
     th(:,:,0)=1.0;
   One may hyperslab on both sides of an assign.  For example, this sets
the last record to the first record:
     th(9,:,:)=th(0,:,:);
   Say TH0 represents pressure at height=0 and TH1 represents pressure
at height=1.  Then it is possible to insert these hyperslabs into the
records
     prs[$time,$height,$lat,$lon]=0.0;
     prs(:,0,:,:)=th0;
     prs(:,1,:,:)=th1;

*Reverse method*
Use the 'reverse()' method to reverse a dimension's elements in a
variable with at least one dimension.  This is equivalent to a negative
stride, e.g.,
     th_rv=th(1,:,:).reverse($lon); // {12,11,10,9 }, {16,15,14,13}
     od_rv=od.reverse($time);        // {38,36,34,32,30,28,26,24,22,20}

*Permute method*p
Use the 'permute()' method to swap the dimensions of a variable.  The
number and names of dimension arguments must match the dimensions in the
variable.  If the first dimension in the variable is of record type then
this must remain the first dimension.  If you want to change the record
dimension then consider using 'ncpdq'.

   Consider the variable:
     float three_dmn_var(lat,lev,lon);
     three_dmn_var_prm=three_dmn_var.permute($lon,$lat,$lev);
     // The permuted values are
     three_dmn_var_prm=
       0,4,8,
       12,16,20,
       1,5,9,
       13,17,21,
       2,6,10,
       14,18,22,
       3,7,11,
       15,19,23;


File: nco.info,  Node: Attributes,  Next: Value List,  Prev: Arrays and hyperslabs,  Up: ncap2 netCDF Arithmetic Processor

4.1.6 Attributes
----------------

Attributes are referred to by _var_nm@att_nm_
All the following are valid statements:
     global@text="Test Attributes"; /* Assign a global variable attribute */
     a1[$time]=time*20;
     a1@long_name="Kelvin";
     a1@min=a1.min();
     a1@max=a1.max();
     a1@min++;
     --a1@max; 
     a1(0)=a1@min;
     a1($time.size-1)=a1@max;

   The netCDF specification allows all attribute types to have a size
greater than one.  The maximum is defined by 'NC_MAX_ATTRS'.  The
following is an 'ncdump' of the metadata for variable A1
     double a1(time) ;
       a1:long_name = "Kelvin" ;
       a1:max = 199. ;
       a1:min = 21. ;
       a1:trip1 = 1, 2, 3 ;
       a1:triplet = 21., 110., 199. ;

   The following basic methods 'size(), type(), exists()' can be used
with attributes.  For example, to save an attribute text string in a
variable,
     defdim("sng_len",a1@long_name.size());
     sng_arr[$sng_len]=a1@long_name; // sng_arr now contains "Kelvin" 
   Attributes defined in a script are stored in memory and are written
to Output after script completion.  To stop the attribute being written
use the ram_delete() method or use a bogus variable name.

*Attribute Propagation and Inheritance*
   * Attribute propagation occurs in a regular assign statement.  The
     variable being defined on the LHS gets copies of the attributes
     from the leftermost variable on the RHS.
   * Attribute Inheritance: The LHS variable "inherits" attributes from
     an Input variable with the same name
   * It is possible to have a regular assign statement for which both
     propagation and inheritance occur.

     // prs_mdp inherits attributes from P0:
     prs_mdp[time,lat,lon,lev]=P0*hyam+hybm*PS;
     // th_min inherits attributes from three_dmn_var_dbl:
     th_min=1.0 + 2*three_dmn_var_dbl.min($time);

*Attribute Concatenation
*

   The push() function concatenates attributes, or appends an
"expression" to a pre-existing attribute.  It comes in two forms
     (A) att_new=push(att_exp, expr)
     (B) att_size=push(&att_nm,expr)

In form (A) The first argument should be an attribute identifier or an
expression that evaluates to an attribute.  The second argument can
evalute to an attribute or a variable.  The second argument is then
converted to the type of ATT_EXP; and appended to ATT_EXP ; and the
resulting attribute is returned.

In form (B) the first argument is a call-by-reference attribute
identifier (which may not yet exist).  The second argument is then
evaluated (and type-converted as needed) and appended to the
call-by-reference atttribute.  The final size of the attribute is then
returned.

     temp@range=-10.0;
     push(&temp@range,12.0); // temp@range=-10.0,12.0
     
     numbers@squares=push(1,4);
     numbers@squares=push(numbers@squares,9);
     push(&number@squares,16.0); 
     push(&number@squares,25ull); // numbers@squares=1,4,9,16,25  

Now some text examples.
Remember, an atttribute identifier that begins with @ implies a global
attribute.  For example, '@institution' is short for
'global@institution'.
     global@greetings=push("hello"," world !!");
     global@greek={"alpha"s,"beta"s,"gamma"s};
     // Append an NC_STRING
     push(&@greek,"delta"s);
     // Pushing an NC_CHAR to a NC_STRING attribute is allowed, it is converted to an an NC_CHAR
     @e="epsilon";
     push(&@greek,@e);
     push(&@greek,"zeta"); 
     
     // Pushing a single NC_STRING to an NC_CHAR is not allowed
     @h="hello";
     push(&@h," again"s); // BAD PUSH

   If the attribute name contains non-regular characters use ID quoting:
     'b..m1@c--lost'=23;
   See *note ID Quoting::.


File: nco.info,  Node: Value List,  Next: Number literals,  Prev: Attributes,  Up: ncap2 netCDF Arithmetic Processor

4.1.7 Value List
----------------

A _value list_ is a special type of attribute.  It can only be used on
the RHS of the assign family of statements.
That is _=, +=, -=, *=, /=_
A value list CANNOT be involved in any logical, binary, or arithmetical
operations (except those above).
A value list CANNOT be used as a function argument.
A value list CANNOT have nested value lists.
The type of a value list is the type of the member with the highest
type.

     a1@trip={1,2,3};
     a1@trip+={3,2,1}; // 4,4,4
     a1@triplet={a1@min,(a1@min+a1@max)/2,a1@max}; 
     lon[lon]={0.0,90.0,180.0,270.0};
     lon*={1.0,1.1,1.2,1.3} 
     dlon[lon]={1b,2s,3ull,4.0f}; // final type NC_FLOAT
     
     a1@ind={1,2,3}+{4,4,4}; // BAD
     a1@s=sin({1.0,16.0}); // BAD

One can also use a value_list to create an attribute of type NC_STRING.
Remember, a literal string of type NC_STRING has a postfix 's'.  A value
list of NC_CHAR has no semantic meaning and is plain wrong.
     array[lon]={1.0,2.,4.0,7.0};
     array@numbers={"one"s, "two"s, "four"s, "seven"s}; // GOOD
     
     ar[lat]={0,20} 
     ar@numbers={"zero","twenty"}; // BAD


File: nco.info,  Node: Number literals,  Next: if statement,  Prev: Value List,  Up: ncap2 netCDF Arithmetic Processor

4.1.8 Number literals
---------------------

The table below lists the postfix character(s) to add to a number
literal (aka, a naked constant) for explicit type specification.  The
same type-specification rules are used for variables and attributes.  A
floating-point number without a postfix defaults to 'NC_DOUBLE', while
an integer without a postfix defaults to type 'NC_INT':
     var[$rlev]=0.1;     // Variable will be type @code{NC_DOUBLE}
     var[$lon_grd]=2.0;  // Variable will be type @code{NC_DOUBLE}
     var[$gds_crd]=2e3;  // Variable will be type @code{NC_DOUBLE}
     var[$gds_crd]=2.0f; // Variable will be type @code{NC_FLOAT} (note "f")
     var[$gds_crd]=2e3f; // Variable will be type @code{NC_FLOAT} (note "f")
     var[$gds_crd]=2;    // Variable will be type @code{NC_INT}
     var[$gds_crd]=-3;   // Variable will be type @code{NC_INT}
     var[$gds_crd]=2s;   // Variable will be type @code{NC_SHORT}
     var[$gds_crd]=-3s;  // Variable will be type @code{NC_SHORT}
     var@att=41.;        // Attribute will be type @code{NC_DOUBLE}
     var@att=41.f;       // Attribute will be type @code{NC_FLOAT}
     var@att=41;         // Attribute will be type @code{NC_INT}
     var@att=-21s;       // Attribute will be type @code{NC_SHORT}  
     var@units="kelvin"; // Attribute will be type @code{NC_CHAR}
   There is no postfix for characters, use a quoted string instead for
'NC_CHAR'.  'ncap2' interprets a standard double-quoted string as a
value of type 'NC_CHAR'.  In this case, any receiving variable must be
dimensioned as an array of 'NC_CHAR' long enough to hold the value.

   To use the newer netCDF4 types NCO must be compiled/linked to the
netCDF4 library and the output file must be of type 'NETCDF4':
     var[$time]=1UL;    // Variable will be type @code{NC_UINT}
     var[$lon]=4b;      // Variable will be type @code{NC_BYTE}
     var[$lat]=5ull;    // Variable will be type @code{NC_UINT64}  
     var[$lat]=5ll;     // Variable will be type @code{NC_INT64}  
     var@att=6.0d;      // Attribute will be type @code{NC_DOUBLE}
     var@att=-666L;     // Attribute will be type @code{NC_INT}
     var@att="kelvin"s; // Attribute will be type @code{NC_STRING} (note the "s")
   Use a post-quote 's' for 'NC_STRING'.  Place the letter 's'
immediately following the double-quoted string to indicate that the
value is of type 'NC_STRING'.  In this case, the receiving variable need
not have any memory allocated to hold the string because netCDF4 handles
that memory allocation.

   Suppose one creates a file containing an ensemble of model results,
and wishes to label the record coordinate with the name of each model.
The 'NC_STRING' type is well-suited to this because it facilitates
storing arrays of strings of arbitrary length.  This is sophisticated,
though easy with 'ncap2':
     % ncecat -O -u model cesm.nc ecmwf.nc giss.nc out.nc
     % ncap2 -4 -O -s 'model[$model]={"cesm"s,"ecmwf"s,"giss"s}' out.nc out.nc
   The key here to place an 's' character after each double-quoted
string value to indicate an 'NC_STRING' type.  The '-4' ensures the
output filetype is netCDF4 in case the input filetype is not.

*netCDF3/4 Types*
b|B
     'NC_BYTE', a signed 1-byte integer
none
     'NC_CHAR', an ISO/ASCII character
s|S
     'NC_SHORT', a signed 2-byte integer
l|L
     'NC_INT', a signed 4-byte integer
f|F
     'NC_FLOAT', a single-precision (4-byte) floating-point number
d|D
     'NC_DOUBLE', a double-precision (8-byte) floating-point number
*netCDF4 Types*
ub|UB
     'NC_UBYTE', an unsigned 1-byte integer
us|US
     'NC_USHORT', an unsigned 2-byte integer
u|U|ul|UL
     'NC_UINT', an unsigned 4-byte integer
ll|LL
     'NC_INT64', a signed 8-byte integer
ull|ULL
     'NC_UINT64', an unsigned 8-byte integer
s
     'NC_STRING', a string of arbitrary length


File: nco.info,  Node: if statement,  Next: Print & String methods,  Prev: Number literals,  Up: ncap2 netCDF Arithmetic Processor

4.1.9 if statement
------------------

The syntax of the if statement is similar to its C counterpart.  The
_Conditional Operator (ternary operator)_ has also been implemented.
     if(exp1)
        stmt1;
     else if(exp2)     
        stmt2;
     else
        stmt3;
     
     # Can use code blocks as well:
     if(exp1){
        stmt1;
        stmt1a;
        stmt1b;
     }else if(exp2)     
        stmt2; 
     else{
        stmt3;
        stmt3a;
        stmt3b;
     }   

For a variable or attribute expression to be logically true all its
non-missing value elements must be logically true, i.e., non-zero.  The
expression can be of any type.  Unlike C there is no short-circuiting of
an expression with the OR ('||') and AND ('&&') operators.  The whole
expression is evaluated regardless if one of the AND/OR operands are
True/False.
     # Simple example
     if(time>0)
       print("All values of time are greater than zero\n");
     else if(time<0)
       print("All values of time are less than zero\n");   
     else {
       time_max=time.max();
       time_min=time.min();
       print("min value of time=");print(time_min,"%f");
       print("max value of time=");print(time_max,"%f");
     }
     
     # Example from ddra.nco
     if(fl_typ==fl_typ_gcm){
       var_nbr_apx=32;
       lmn_nbr=1.0*var_nbr_apx*varsz_gcm_4D; /* [nbr] Variable size */
       if(nco_op_typ==nco_op_typ_avg){
         lmn_nbr_avg=1.0*var_nbr_apx*varsz_gcm_4D; // Block size
         lmn_nbr_wgt=dmnsz_gcm_lat; /* [nbr] Weight size */
       } // !nco_op_typ_avg
     }else if(fl_typ==fl_typ_stl){
       var_nbr_apx=8;
       lmn_nbr=1.0*var_nbr_apx*varsz_stl_2D; /* [nbr] Variable size */
       if(nco_op_typ==nco_op_typ_avg){
         lmn_nbr_avg=1.0*var_nbr_apx*varsz_stl_2D; // Block size
         lmn_nbr_wgt=dmnsz_stl_lat; /* [nbr] Weight size */
       } // !nco_op_typ_avg
     } // !fl_typ

*Conditional Operator
*
     // netCDF4 needed for this example
     th_nw=(three_dmn_var_sht >= 0 ? three_dmn_var_sht.uint() : \
            three_dmn_var_sht.int());


File: nco.info,  Node: Print & String methods,  Next: Missing values ncap2,  Prev: if statement,  Up: ncap2 netCDF Arithmetic Processor

4.1.10 Print & String methods
-----------------------------

The print statement comes in a variety of forms

     (A)   print(variable_name, format string?);
     (A1)  print(expression/string, format string?);

     (B)   sprint(expression/string, format string?);
     (B1)  sprint4(expression/string, format string?);

*print()

* If the variable exists in I/O then it is printed in a similar fashion
to 'ncks -H'.
     print(lon);
     lon[0]=0 
     lon[1]=90 
     lon[2]=180 
     lon[3]=270 
     
     print(byt_2D)
     lat[0]=-90 lon[0]=0 byt_2D[0]=0 
     lat[0]=-90 lon[1]=90 byt_2D[1]=1 
     lat[0]=-90 lon[2]=180 byt_2D[2]=2 
     lat[0]=-90 lon[3]=270 byt_2D[3]=3 
     lat[1]=90 lon[0]=0 byt_2D[4]=4 
     lat[1]=90 lon[1]=90 byt_2D[5]=5 
     lat[1]=90 lon[2]=180 byt_2D[6]=6 
     lat[1]=90 lon[3]=270 byt_2D[7]=7 

If the first argument is NOT a variable the form (A1) is invoked.
     print(mss_val_fst@_FillValue);
     mss_val_fst@_FillValue, size = 1 NC_FLOAT, value = -999
     
     print("This function \t is monotonic\n");
     This function is 	  monotonic
     
     print(att_var@float_att)
     att_var@float_att, size = 7 NC_FLOAT, value = 73, 72, 71, 70.01, 69.001, 68.01, 67.01
     
     print(lon*10.0)
     lon, size = 4 NC_DOUBLE, value = 0, 900, 1800, 2700

If the format string is specified then the results from (A) and (A1)
forms are the same
     print(lon_2D_rrg,"%3.2f,");
     0.00,0.00,180.00,0.00,180.00,0.00,180.00,0.00,
     
     print(lon*10.0, "%g,")
     0,900,1800,2700,
     
     print(att_var@float_att,"%g," )
     73,72,71,70.01,69.001,68.01,67.01,

*sprint() & sprint4()

* These functions work in an identical fashion to (A1) except that
'sprint()' outputs a regular netCDF3 'NC_CHAR' attribute and 'sprint4()'
outputs a netCDF4 'NC_STRING' attribute
     time@units=sprint(nDays, "%d days since 1970-1-1") 
     bnd@num=sprint4(bnd_idx,"Band number=%d") 
     
     time@arr=sprint4(time,"%.2f,")  // "1.00,2.00,3.00,4.00,5.00,6.00,7.00,8.00,9.00,10.00,"

You can also use 'sprint4()' to convert a 'NC_CHAR' string to a
'NC_STRING' string and 'sprint()' to convert a 'NC_STRING' to a
'NC_CHAR'
     lat_1D_rct@long_name = "Latitude for 2D rectangular grid stored as 1D arrays"; // 
     
     // convert to NC_STRING
     lat_1D_rct@long_name = sprint4(lat_1D_rct@long_name) 

*hyperslab a netCDF string

* Its possible to index-into a NC_CHAR string.  Just like a C-String.
Remember an NC_CHAR string is has no terminating null.  You CANNOT index
into a NC_STRING. You have to convert to an NC_CHAR first.
     global@greeting="hello world!!!"
     @h=@greeting(0:4);  // "hello"
     @w=@greeting(6:11); // "world"
     
     // can use negative inidices
     @x=@greeting(-3:-1);  // "!!!"
     
     // can  use stride
     @n=@greeting(::2);  // "hlowrd!"
     
     // concatenation
     global@new_greeting=push(@h, " users !!!"); // "hello users!!!"
     
     @institution="hotel california"s; 
     @h=@institution(0:4); // BAD 
     
     // convert NC_STRING to NC_CHAR
     @is=sprint(@institution);
     @h=@is(0:4);  // "hotel"
     
     // convert NC_CHAR to NC_STRING
     @h=sprint4(@h);

*get_vars_in() & get_vars_out()*
     att_lst=get_vars_in(att_regexp?)
     att_lst=get_vars_out(att_regexp?)

   These functions are used to create a list of vars in Input or Output.
The optional arg 'att_regexp'.  Can be an NC_CHAR att or a NC_STRING
att.  If NC_CHAR then only a single reg-exp can be specified.  If
NC_STRING then multiple reg-exp can be specified.  The output is allways
an NC_STRING att.  The matching works in an identical fashion to the -v
switch in ncks.  if there is no arg then all vars are returned.

     @slist=get_vars_in("^time");  // "time", "time_bnds", "time_lon", "time_udunits"
     // Use NC_STRINGS
     @regExp={".*_bnd"s,".*_grd"s}
     @slist=get_vars_in(@regExp);  // "lat_bnd", "lat_grd", "lev_bnd", "lon_grd", "time_bnds", "cnv_CF_grd"


File: nco.info,  Node: Missing values ncap2,  Next: Methods and functions,  Prev: Print & String methods,  Up: ncap2 netCDF Arithmetic Processor

4.1.11 Missing values ncap2
---------------------------

Missing values operate slightly differently in 'ncap2' Consider the
expression where op is any of the following operators (excluding '=')
     Arithmetic operators ( * / % + - ^ )
     Binary Operators     ( >, >= <, <= ==, !=,==,||,&&, >>,<< )
     Assign Operators     ( +=,-=,/=, *= )

     var1 'op' var2

If var1 has a missing value then this is the value used in the
operation, otherwise the missing value for var2 is used.  If during the
element-by-element operation an element from either operand is equal to
the missing value then the missing value is carried through.  In this
way missing values 'percolate' or propagate through an expression.
Missing values associated with Output variables are stored in memory and
are written to disk after the script finishes.  During script execution
its possible (and legal) for the missing value of a variable to take on
several different values.
     # Consider the variable:
     int rec_var_int_mss_val_int(time); =-999,2,3,4,5,6,7,8,-999,-999;
     rec_var_int_mss_val_int:_FillValue = -999;

     n2=rec_var_int_mss_val_int + rec_var_int_mss_val_int.reverse($time);

     n2=-999,-999,11,11,11,11,11,11,999,-999;

   The following methods query or manipulate missing value information
associated with a variable.  The methods that "manipulate" will only
succeed on variables in Output

'set_miss(expr)'
     The numeric argument EXPR becomes the new missing value,
     overwriting the old missing value, if any.  The argument given is
     converted if necessary to the variable's type.  NB: This only
     changes the missing value attribute.  Missing values in the
     original variable remain unchanged, and thus are no long considered
     missing values.  They are effectively "orphaned".  Thus
     'set_miss()' is normally used only when creating new variables.
     The intrinsic function 'change_miss()' (see below) is typically
     used to edit values of existing variables.
'change_miss(expr)'
     Sets or changes (any pre-existing) missing value attribute and
     missing data values to EXPR.  NB: This is an expensive function
     since all values must be examined.  Use this function when changing
     missing values for pre-existing variables.
'get_miss()'
     Returns the missing value of a variable.  If the variable exists in
     Input and Output then the missing value of the variable in Output
     is returned.  If the variable has no missing value then an error is
     returned.
'delete_miss()'
     Delete the missing value associated with a variable.
'number_miss()'
     Count the number of missing values a variable contains.
'has_miss()'
     Returns 1 (True) if the variable has a missing value associated
     with it.  else returns 0 (False)
'missing()'
     This function creates a True/False mask array of where the missing
     value is set.  It is syntatically equivalent to ( var_in ==
     var_in.get_miss()).  except that cant do this unless you delete the
     missing value before-hand!!

     th=three_dmn_var_dbl;
     th.change_miss(-1e10d);
     /* Set values less than 0 or greater than 50 to missing value */
     where(th < 0.0 || th > 50.0) th=th.get_miss();
     
     # Another example:
     new[$time,$lat,$lon]=1.0;
     new.set_miss(-997.0);
     
     // Extract only elements divisible by 3
     where (three_dmn_var_dbl%3 == 0)
          new=three_dmn_var_dbl; 
     elsewhere
          new=new.get_miss();   
     
     // Print missing value and variable summary
     mss_val_nbr=three_dmn_var_dbl.number_miss();
     print(three_dmn_var_dbl@_FillValue);
     print("Number of missing values in three_dmn_var_dbl: ");
     print(mss_val_nbr,"%d");
     print(three_dmn_var_dbl);
     
     // find the total number of missing values along dims $lat and $lon
     mss_ttl=three_mn_var_dbl.missing().ttl($lat,$lon);
     print(mss_ttl); // 0, 0, 0, 8, 0, 0, 0, 1, 0, 2 ;

'simple_fill_miss(var)'
     This function takes a variable and attempts to fill missing values
     using an average of up to the 4 nearest neighbour grid points.  The
     method used is iterative (up to 1000 cycles).  For very large areas
     of missing values results can be unpredictable.  The given variable
     must be at least 2D; and the algorithm assumes that the last two
     dims are lat/lon or y/x
'weighted_fill_miss(var)'
     Weighted_fill_miss is more sophisticated.  Up to 8 nearest
     neighbours are used to calculate a weighted average.  The weighting
     used is the inverse square of distance.  Again the method is
     iterative (up to 1000 cycles).  The area filled is defined by the
     final two dims of the variable.  In addition this function assumes
     the existance of coordinate vars the same name as the last two
     dims.  if it doesn't find these dims it will gently exit with
     warning.


File: nco.info,  Node: Methods and functions,  Next: RAM variables,  Prev: Missing values ncap2,  Up: ncap2 netCDF Arithmetic Processor

4.1.12 Methods and functions
----------------------------

The convention within this document is that methods can be used as
functions.  However, functions are not and cannot be used as methods.
Methods can be daisy-chained d and their syntax is cleaner than
functions.  Method names are reserved words and CANNOT be used as
variable names.  The command 'ncap2 -f' shows the complete list of
methods available on your build.
     n2=sin(theta)
     n2=theta.sin()
     n2=sin(theta)^2 + cos(theta)^2
     n2=theta.sin().pow(2) + theta.cos()^2

   This statement chains together methods to convert three_dmn_var_sht
to type double, average it, then convert this back to type short:
     three_avg=three_dmn_var_sht.double().avg().short();


*Aggregate Methods
* These methods mirror the averaging types available in 'ncwa'.  The
arguments to the methods are the dimensions to average over.  Specifying
no dimensions is equivalent to specifying all dimensions i.e., averaging
over all dimensions.  A masking variable and a weighting variable can be
manually created and applied as needed.

'avg()'
     Mean value
'sqravg()'
     Square of the mean
'avgsqr()'
     Mean of sum of squares
'max()'
     Maximum value
'min()'
     Minimum value
'mabs()'
     Maximum absolute value
'mebs()'
     Mean absolute value
'mibs()'
     Minimum absolute value
'rms()'
     Root-mean-square (normalize by N)
'rmssdn()'
     Root-mean square (normalize by N-1)
'tabs() or ttlabs()'
     Sum of absolute values
'ttl() or total() or sum()'
     Sum of values

     // Average a variable over time
     four_time_avg=four_dmn_rec_var($time);


*Packing Methods
* For more information see *note Packed data:: and *note ncpdq netCDF
Permute Dimensions Quickly::
'pack() & pack_short()'
     The default packing algorithm is applied and variable is packed to
     'NC_SHORT'
'pack_byte()'
     Variable is packed to 'NC_BYTE'
'pack_short()'
     Variable is packed to 'NC_SHORT'
'pack_int()'
     Variable is packed to 'NC_INT'
'unpack()'
     The standard unpacking algorithm is applied.
   NCO automatically unpacks packed data before arithmetically modifying
it.  After modification NCO stores the unpacked data.  To store it as
packed data again, repack it with, e.g., the 'pack()' function.  To
ensure that 'temperature' is packed in the output file, regardless of
whether it is packed in the input file, one uses, e.g.,
     ncap2 -s 'temperature=pack(temperature-273.15)' in.nc out.nc

   All the above pack functions also take the additional two arguments
'scale_factor, add_offset'.  Both arguments must be included:
     ncap2 -v -O -s 'rec_pck=pack(three_dmn_rec_var,-0.001,40.0);' in.nc foo.nc

   *Basic Methods
* These methods work with variables and attributes.  They have no
arguments

'size()'
     Total number of elements
'ndims()'
     Number of dimensions in variable
'type()'
     Returns the netcdf type (see previous section)
'exists()'
     Return 1 (true) if var or att is present in I/O else return 0
     (false)
'getdims()'
     Returns an NC_STRING attribute of all the dim names of a variable


*Utility Methods
* These functions are used to manipulate missing values and RAM
variables.  *note Missing values ncap2::

'set_miss(expr)'
     Takes one argument the missing value.  Sets or overwrites the
     existing missing value.  The argument given is converted if
     necessary to the variable type
'change_miss(expr)'
     Changes the missing value elements of the variable to the new
     missing value (n.b.  an expensive function).
'get_miss()'
     Returns the missing value of a variable in Input or Output
'delete_miss()'
     Deletes the missing value associated with a variable.
'has_miss()'
     Returns 1 (True) if the variable has a missing else returns 0
     (False)
'number_miss'
     Returns the number of missing values a variable contains
'ram_write()'
     Writes a RAM variable to disk i.e., converts it to a regular disk
     type variable
'ram_delete()'
     Deletes a RAM variable or an attribute


*PDQ Methods
* See *note ncpdq netCDF Permute Dimensions Quickly::
'reverse(dim args)'
     Reverse the dimension ordering of elements in a variable.
'permute(dim args)'
     Re-shape variables by re-ordering the dimensions.  All the
     dimensions of the variable must be specified in the arguments.  A
     limitation of this permute (unlike 'ncpdq') is that the record
     dimension cannot be re-assigned.
   // Swap dimensions about and reorder along lon
     lat_2D_rrg_new=lat_2D_rrg.permute($lon,$lat).reverse($lon);
     lat_2D_rrg_new=0,90,-30,30,-30,30,-90,0


*Type Conversion Methods and Functions
* These methods allow 'ncap2' to convert variables and attributes to the
different netCDF types.  For more details on automatic and manual type
conversion see (*note Type Conversion::).  netCDF4 types are only
available if you have compiled/links NCO with the netCDF4 library and
the Output file is HDF5.
'*netCDF3/4 Types*'
'byte()'
     convert to 'NC_BYTE', a signed 1-byte integer
'char()'
     convert to 'NC_CHAR', an ISO/ASCII character
'short()'
     convert to 'NC_SHORT', a signed 2-byte integer
'int()'
     convert to 'NC_INT', a signed 4-byte integer
'float()'
     convert to 'NC_FLOAT', a single-precision (4-byte) floating-point
     number
'double()'
     convert to 'NC_DOUBLE', a double-precision (8-byte) floating-point
     number
'*netCDF4 Types*'
'ubyte()'
     convert to 'NC_UBYTE', an unsigned 1-byte integer
'ushort()'
     convert to 'NC_USHORT', an unsigned 2-byte integer
'uint()'
     convert to 'NC_UINT', an unsigned 4-byte integer
'int64()'
     convert to 'NC_INT64', a signed 8-byte integer
'uint64()'
     convert to 'NC_UINT64', an unsigned 8-byte integer

   You can also use the 'convert()' method to do type conversion.  This
takes an integer agument.  For convenience, 'ncap2' defines the netCDF
pre-processor tokens as RAM variables.  For example you may wish to
convert a non-floating point variable to the same type as another
variable.
     lon_type=lon.type();
     if(time.type() != NC_DOUBLE && time.type() != NC_FLOAT)
        time=time.convert(lon_type);

*Intrinsic Mathematical Methods
* The list of mathematical methods is system dependant.  For the full
list *note Intrinsic mathematical methods::

   All the mathematical methods take a single argument except 'atan2()'
and 'pow()' which take two.  If the operand type is less than _float_
then the result will be of type _float_.  Arguments of type _double_
yield results of type _double_.  Like the other methods, you are free to
use the mathematical methods as functions.

     n1=pow(2,3.0f)    // n1 type float
     n2=atan2(2,3.0)   // n2 type double
     n3=1/(three_dmn_var_dbl.cos().pow(2))-tan(three_dmn_var_dbl)^2; // n3 type double


File: nco.info,  Node: RAM variables,  Next: Where statement,  Prev: Methods and functions,  Up: ncap2 netCDF Arithmetic Processor

4.1.13 RAM variables
--------------------

Unlike regular variables, RAM variables are never written to disk.
Hence using RAM variables in place of regular variables (especially
within loops) significantly increases execution speed.  Variables that
are frequently accessed within 'for' or 'where' clauses provide the
greatest opportunities for optimization.  To declare and define a RAM
variable simply prefix the variable name with an asterisk ('*') when the
variable is declared/initialized.  To delete RAM variables (and recover
their memory) use the 'ram_delete()' method.  To write a RAM variable to
disk (like a regular variable) use 'ram_write()'.
     *temp[$time,$lat,$lon]=10.0;     // Cast
     *temp_avg=temp.avg($time);      // Regular assign
     temp.ram_delete();              // Delete RAM variable
     temp_avg.ram_write();           // Write Variable to output

     // Create and increment a RAM variable from "one" in Input
     *one++;
     // Create RAM variables from the variables three and four in Input.
     // Multiply three by 10 and add it to four.
     *four+=*three*=10; // three=30, four=34


File: nco.info,  Node: Where statement,  Next: Loops,  Prev: RAM variables,  Up: ncap2 netCDF Arithmetic Processor

4.1.14 Where statement
----------------------

The 'where()' statement combines the definition and application of a
mask and can lead to succinct code.  The syntax of a 'where()' statement
is:
     // Single assign ('elsewhere' is optional)
     where(mask) 
        var1=expr1;
     elsewhere
        var1=expr2;	   	
     
     // Multiple assigns
     where(mask){
         var1=expr1;
         var2=expr2;
         ...
     }elsewhere{
         var1=expr3
         var2=expr4
         var3=expr5;
         ...
     }

   * The only expression allowed in the predicate of a where is assign,
     i.e., 'var=expr'.  This assign differs from a regular 'ncap2'
     assign.  The LHS var must already exist in Input or Output.  The
     RHS expression must evaluate to a scalar or a variable/attribute of
     the same size as the LHS variable.
   * Consider when both the LHS and RHS are variables: For every element
     where mask condition is True, the corresponding LHS variable
     element is re-assigned to its partner element on the RHS. In the
     elsewhere part the mask is logically inverted and the assign
     process proceeds as before.
   * If the mask dimensions are a subset of the LHS variable's
     dimensions, then it is made to conform; if it cannot be made to
     conform then script execution halts.
   * Missing values in the mask evaluate to False in the where
     code/block statement and to True in the elsewhere block/statement.
   * LHS variable elements set to missing value are treated just like
     any other elements and can be re-assigned as the mask dictates
   * LHS variable cannot include subscripts.  If they do script
     execution will terminate.  See below example for work-araound.

   Consider the variables 'float lon_2D_rct(lat,lon);' and 'float
var_msk(lat,lon);'.  Suppose we wish to multiply by two the elements for
which 'var_msk' equals 1:
     where(var_msk == 1) lon_2D_rct=2*lon_2D_rct;
   Suppose that we have the variable 'int RDM(time)' and that we want to
set its values less than 8 or greater than 80 to 0:
     where(RDM < 8 || RDM > 80) RDM=0;

   To use 'where' on a variable hyperslab, define and use a temporary
variable, e.g.,
     *var_tmp=var2(:,0,:,:); 
     where (var1 < 0.5) var_tmp=1234; 
     var2(;,0,:,;)=var_tmp;
     ram_delete(var_tmp);

   Consider irregularly gridded data, described using rank 2
coordinates: 'double lat(south_north,east_west)', 'double
lon(south_north,east_west)', 'double
temperature(south_north,east_west)'.  This type of structure is often
found in regional weather/climate model (such as WRF) output, and in
satellite swath data.  For this reason we call it "Swath-like Data", or
SLD.  To find the average temperature in a region bounded by
[LAT_MIN,LAT_MAX] and [LON_MIN,LON_MAX]:
     temperature_msk[$south_north,$east_west]=0.0;
     where((lat >= lat_min && lat <= lat_max) && (lon >= lon_min && lon <= lon_max))
       temperature_msk=temperature;	
     elsewhere
       temperature_msk=temperature@_FillValue;
     
     temp_avg=temperature_msk.avg();
     temp_max=temperature.max();

   For North American Regional Reanalysis (NARR) data (example dataset
(http://dust.ess.uci.edu/diwg/narr_uwnd.199605.nc)) the procedure looks
like this
     ncap2 -O -v -S ~/narr.nco ${DATA}/hdf/narr_uwnd.199605.nc ~/foo.nc
   where 'narr.nco' is an 'ncap2' script like this:
     /* North American Regional Reanalysis (NARR) Statistics
        NARR stores grids with 2-D latitude and longitude, aka Swath-like Data (SLD) 
        Here we work with three variables:
        lat(y,x), lon(y,x), and uwnd(time,level,y,x);
        To study sub-regions of SLD, we use masking techniques:
        1. Define mask as zero times variable to be masked
           Then mask automatically inherits variable attributes
           And average below will inherit mask attributes
        2. Optionally, create mask as RAM variable (as below with asterisk *)
           NCO does not write RAM variable to output
           Masks are often unwanted, and can be big, so this speeds execution
        3. Example could be extended to preserve mean lat and lon of sub-region
           Follow uwnd example to do this: lat_msk=0.0*lat ... lat_avg=lat.avg($y,$x) */
     *uwnd_msk=0.0*uwnd;
     where((lat >= 35.6 && lat <= 37.0) && (lon >= -100.5 && lon <= -99.0))
       uwnd_msk=uwnd;
     elsewhere
       uwnd_msk=uwnd@_FillValue;
     
     // Average only over horizontal dimensions x and y (preserve level and time)
     uwnd_avg=uwnd_msk.avg($y,$x); 
   Stripped of comments and formatting, this example is a
three-statement script executed by a one-line command.  NCO needs only
this meagre input to unpack and copy the input data and attributes,
compute the statistics, and then define and write the output file.
Unless the comments pointed out that wind variable ('uwnd') was
four-dimensional and the latitude/longitude grid variables were both
two-dimensional, there would be no way to tell.  This shows how NCO
hides from the user the complexity of analyzing multi-dimensional SLD.
We plan to extend such SLD features to more operators soon.


File: nco.info,  Node: Loops,  Next: Include files,  Prev: Where statement,  Up: ncap2 netCDF Arithmetic Processor

4.1.15 Loops
------------

'ncap2' supplies 'for()' loops and 'while()' loops.  They are completely
unoptimized so use them only with RAM variables unless you want thrash
your disk to death.  To break out of a loop use the 'break' command.  To
iterate to the next cycle use the 'continue' command.

     // Set elements in variable double temp(time,lat) 
     // If element < 0 set to 0, if element > 100 set to 100
     *sz_idx=$time.size;
     *sz_jdx=$lat.size;
     
     for(*idx=0;idx<sz_idx;idx++)
       for(*jdx=0;jdx<sz_jdx;jdx++)
         if(temp(idx,jdx) > 100) temp(idx,jdx)=100.0; 
           else if(temp(idx,jdx) < 0) temp(idx,jdx)=0.0;
     
     // Are values of co-ordinate variable double lat(lat) monotonic?
     *sz=$lat.size;
     
     for(*idx=1;idx<sz;idx++)
       if(lat(idx)-lat(idx-1) < 0.0) break;
     
     if(idx == sz) print("lat co-ordinate is monotonic\n");
        else print("lat co-ordinate is NOT monotonic\n");
     
     // Sum odd elements	
     *idx=0;
     *sz=$lat_nw.size;
     *sum=0.0;
     
     while(idx<sz){
       if(lat(idx)%2) sum+=lat(idx);
       idx++;
     }
     
     ram_write(sum);
     print("Total of odd elements ");print(sum);print("\n"); 


File: nco.info,  Node: Include files,  Next: Sort methods,  Prev: Loops,  Up: ncap2 netCDF Arithmetic Processor

4.1.16 Include files
--------------------

The syntax of an INCLUDE-FILE is:
     #include "script.nco"
     #include "/opt/SOURCES/nco/data/tst.nco"
   If the filename is relative and not absolute then the directory
searched is relative to the run-time directory.  It is possible to nest
include files to an arbitrary depth.  A handy use of inlcude files is to
store often used constants.  Use RAM variables if you do not want these
constants written to nc-file.

   OUTPUT-FILE.
     // script.nco
     // Sample file to #include in ncap2 script
     *pi=3.1415926535; // RAM variable, not written to output
     *h=6.62607095e-34; // RAM variable, not written to output
     e=2.71828; // Regular (disk) variable, written to output

   As of NCO version 4.6.3 (December, 2016), The user can specify the
directory(s) to be searched by specifing them in the UNIX environment
var 'NCO_PATH'.  The format used is identical to the UNIX 'PATH'.  The
directory(s) are only searched if the include filename is relative.

     export NCO_PATH=":/home/henryb/bin/:/usr/local/scripts:/opt/SOURCES/nco/data:"


File: nco.info,  Node: Sort methods,  Next: UDUnits script,  Prev: Include files,  Up: ncap2 netCDF Arithmetic Processor

4.1.17 'sort' methods
---------------------

In ncap2 there are multiple ways to sort data.  Beginning with NCO 4.1.0
(March, 2012), ncap2 support six sorting functions:
     var_out=sort(var_in,&srt_map); // Ascending sort
     var_out=asort(var_in,&srt_map); // Accending sort
     var_out=dsort(var_in,&srt_map); // Desending sort
     var_out=remap(var_in,srt_map); // Apply srt_map to var_in
     var_out=unmap(var_in,srt_map); // Reverse what srt_map did to var_in
     dsr_map=invert_map(srt_map); // Produce "de-sort" map that inverts srt_map
   The first two functions, 'sort()' and 'asort()' sort, in ascending
order, all the elements of VAR_IN (which can be a variable or attribute)
without regard to any dimensions.  The third function, 'dsort()' does
the same but sorts in descending order.  Remember that ascending and
descending sorts are specified by 'asort()' and 'dsort()', respectively.

   These three functions are overloaded to take a second, optional
argument called the sort map SRT_MAP, which should be supplied as a
call-by-reference variable, i.e., preceded with an ampersand.  If the
sort map does not yet exist, then it will be created and returned as an
integer type the same shape as the input variable.

   The output VAR_OUT of each sort function is a sorted version of the
input, VAR_IN.  The output VAR_OUT of the two mapping functions the
result of applying (with 'remap()' or un-applying (with 'unmap()') the
sort map SRT_MAP to the input VAR_IN.  To apply the sort map with
'remap()' the size of the variable must be exactly divisible by the size
of the sort map.

   The final function 'invert_map()' returns the so-called de-sorting
map DSR_MAP which is the inverse of the input map SRT_MAP.  This gives
the user access to both the forward and inverse sorting maps:
     a1[$time]={10,2,3,4,6,5,7,3,4,1};
     a1_sort=sort(a1);
     print(a1_sort);
     // 1, 2, 3, 3, 4, 4, 5, 6, 7, 10;
     
     a2[$lon]={2,1,4,3};
     a2_sort=sort(a2,&a2_map);
     print(a2);
     // 1, 2, 3, 4
     print(a2_map);
     // 1, 0, 3, 2;

   If the map variable does not exist prior to the 'sort()' call, then
it will be created with the same shape as the input variable and be of
type 'NC_INT'.  If the map variable already exists, then the only
restriction is that it be of at least the same size as the input
variable.  To apply a map use 'remap(var_in,srt_map)'.
     defdim("nlat",5);
     
     a3[$lon]={2,5,3,7};
     a4[$nlat,$lon]={
      1, 2, 3, 4, 
      5, 6, 7, 8,
      9,10,11,12,
      13,14,15,16,
      17,18,19,20};
     
     a3_sort=sort(a3,&a3_map);
     print(a3_map);
     // 0, 2, 1, 3;
     
     a4_sort=remap(a4,a3_map);
     print(a4_sort);
     // 1, 3, 2, 4,
     // 5, 7, 6, 8,
     // 9,11,10,12,
     // 13,15,14,16,
     // 17,19,18,20;
     
     a3_map2[$nlat]={4,3,0,2,1};
     
     a4_sort2=remap(a4,a3_map2);
     print(a4_sort2);
     // 3, 5, 4, 2, 1
     // 8, 10, 9,7, 6, 
     // 13,15,14,12,11, 
     // 18,20,19,17,16
   As in the above example you may create your own sort map.  To sort in
descending order, apply the 'reverse()' method after the 'sort()'.

   Here is an extended example of how to use 'ncap2' features to
hyperslab an irregular region based on the values of a variable not a
coordinate.  The distinction is crucial: hyperslabbing based on
dimensional indices or coordinate values is straightforward.  Using the
values of single or multi-dimensional variable to define a hyperslab is
quite different.
     cat > ~/ncap2_foo.nco << 'EOF'
     // Purpose: Save irregular 1-D regions based on variable values

     // Included in NCO User Guide at http://nco.sf.net/nco.html#sort

     /* NB: Single quotes around EOF above turn off shell parameter
         expansion in "here documents". This in turn prevents the
         need for protecting dollarsign characters in NCO scripts with
         backslashes when the script is cut-and-pasted (aka "moused")
         from an editor or e-mail into a shell console window */

     /* Copy coordinates and variable(s) of interest into RAM variable(s)
        Benefits:
        1. ncap2 defines writes all variables on LHS of expression to disk
           Only exception is RAM variables, which are stored in RAM only
           Repeated operations on regular variables takes more time,
           because changes are written to disk copy after every change.
           RAM variables are only changed in RAM so script works faster
           RAM variables can be written to disk at end with ram_write()
        2. Script permutes variables of interest during processing
           Safer to work with copies that have different names
           This discourages accidental, mistaken use of permuted versions
        3. Makes this script a more generic template:
           var_in instead of specific variable names everywhere */
     *var_in=one_dmn_rec_var;
     *crd_in=time;
     *dmn_in_sz=$time.size; // [nbr] Size of input arrays

     /* Create all other "intermediate" variables as RAM variables
        to prevent them from cluttering the output file.
        Mask flag and sort map are same size as variable of interest */
     *msk_flg=var_in;
     *srt_map=var_in;

     /* In this example we mask for all values evenly divisible by 3
        This is the key, problem-specific portion of the template
        Replace this where() condition by that for your problem
        Mask variable is Boolean: 1=Meets condition, 0=Fails condition */
     where(var_in % 3 == 0) msk_flg=1; elsewhere msk_flg=0;

     // print("msk_flg = ");print(msk_flg); // For debugging...

     /* The sort() routine is overloaded, and takes one or two arguments
        The second argument (optional) is the "sort map" (srt_map below)
        Pass the sort map by reference, i.e., prefix with an ampersand
        If the sort map does not yet exist, then it will be created and
        returned as an integer type the same shape as the input variable.
        The output of sort(), on the LHS, is a sorted version of the input
        msk_flg is not needed in its original order after sort()
        Hence we use msk_flg as both input to and output from sort()
        Doing this prevents the need to define a new, unneeded variable */
     msk_flg=sort(msk_flg,&srt_map);

     // Count number of valid points in mask by summing the one's
     *msk_nbr=msk_flg.total();

     // Define output dimension equal in size to number of valid points
     defdim("crd_out",msk_nbr);

     /* Now sort the variable of interest using the sort map and remap()
        The output, on the LHS, is the input re-arranged so that all points
        meeting the mask condition are contiguous at the end of the array
        Use same srt_map to hyperslab multiple variables of the same shape
        Remember to apply srt_map to the coordinate variables */
     crd_in=remap(crd_in,srt_map);
     var_in=remap(var_in,srt_map);

     /* Hyperslab last msk_nbr values of variable(s) of interest */
     crd_out[crd_out]=crd_in((dmn_in_sz-msk_nbr):(dmn_in_sz-1));
     var_out[crd_out]=var_in((dmn_in_sz-msk_nbr):(dmn_in_sz-1));

     /* NB: Even though we created all variables possible as RAM variables,
        the original coordinate of interest, time, is written to the ouput.
        I'm not exactly sure why. For now, delete it from the output with:
        ncks -O -x -v time ~/foo.nc ~/foo.nc
        */
     EOF
     ncap2 -O -v -S ~/ncap2_foo.nco ~/nco/data/in.nc ~/foo.nc
     ncks -O -x -v time ~/foo.nc ~/foo.nc
     ncks ~/foo.nc

   Here is an extended example of how to use 'ncap2' features to sort
multi-dimensional arrays based on the coordinate values along a single
dimension.
     cat > ~/ncap2_foo.nco << 'EOF'
     /* Purpose: Sort multi-dimensional array based on coordinate values
        This example sorts the variable three_dmn_rec_var(time,lat,lon)
        based on the values of the time coordinate. */

     // Included in NCO User Guide at http://nco.sf.net/nco.html#sort

     // Randomize the time coordinate
     time=10.0*gsl_rng_uniform(time);
     //print("original randomized time =\n");print(time);

     /* The sort() routine is overloaded, and takes one or two arguments
        The first argument is a one dimensional array
        The second argument (optional) is the "sort map" (srt_map below)
        Pass the sort map by reference, i.e., prefix with an ampersand
        If the sort map does not yet exist, then it will be created and
        returned as an integer type the same shape as the input variable.
        The output of sort(), on the LHS, is a sorted version of the input */

     time=sort(time,&srt_map);
     //print("sorted time (ascending order) and associated sort map =\n");print(time);print(srt_map);

     /* sort() always sorts in ascending order
        The associated sort map therefore re-arranges the original,
        randomized time array into ascending order.
        There are two methods to obtain the descending order the user wants
        1) We could solve the problem in ascending order (the default)
        and then apply the reverse() method to re-arrange the results.
        2) We could change the sort map to return things in descending
        order of time and solve the problem directly in descending order. */

     // Following shows how to do method one:

     /* Expand the sort map to srt_map_3d, the size of the data array
        1. Use data array to provide right shape for the expanded sort map
        2. Coerce data array into an integer so srt_map_3d is an integer
        3. Multiply data array by zero so 3-d map elements are all zero
        4. Add the 1-d sort map to the 3-d sort map (NCO automatically resizes)
        5. Add the spatial (lat,lon) offsets to each time index
        6. de-sort using the srt_map_3d
        7. Use reverse to obtain descending in time order
        Loops could accomplish the same thing (exercise left for reader)
        However, loops are slow for large datasets */

     /* Following index manipulation requires understanding correspondence
        between 1-d (unrolled, memory order of storage) and access into that
        memory as a multidimensional (3-d, in this case) rectangular array.
        Key idea to understand is how dimensionality affects offsets */
     // Copy 1-d sort map into 3-d sort map
     srt_map_3d=(0*int(three_dmn_rec_var))+srt_map;
     // Multiply base offset by factorial of lesser dimensions
     srt_map_3d*=$lat.size*$lon.size;
     lon_idx=array(0,1,$lon);
     lat_idx=array(0,1,$lat)*$lon.size;
     lat_lon_idx[$lat,$lon]=lat_idx+lon_idx;
     srt_map_3d+=lat_lon_idx;

     print("sort map 3d =\n");print(srt_map_3d);

     // Use remap() to re-map the data
     three_dmn_rec_var=remap(three_dmn_rec_var,srt_map_3d);

     // Finally, reverse data so time coordinate is descending
     time=time.reverse($time);
     //print("sorted time (descending order) =\n");print(time);
     three_dmn_rec_var=three_dmn_rec_var.reverse($time);

     // Method two: Key difference is srt_map=$time.size-srt_map-1;
     EOF
     ncap2 -O -v -S ~/ncap2_foo.nco ~/nco/data/in.nc ~/foo.nc


File: nco.info,  Node: UDUnits script,  Next: Vpointer,  Prev: Sort methods,  Up: ncap2 netCDF Arithmetic Processor

4.1.18 UDUnits script
---------------------

As of NCO version 4.6.3 (December, 2016), ncap2 includes support for
UDUnits conversions.  The function is called 'udunits'.  Its syntax is
     varOut=udunits(varIn, "UnitsOutString")

   The 'udunits()' function looks for the attribute of 'varIn@units' and
fails if it is not found.  A quirk of this function that due to
attribute propagation 'varOut@units' will be overwritten by
'varIn@units'.  It is best to re-initialize this attribute AFTER the
call.  In addition if 'varIn@units' is of the form '"time_interval since
basetime"' then the calendar attribute 'varIn@calendar' will read it.
If it does not exist then the calendar used defaults to mixed
Gregorian/Julian as defined by UDUnits.

   If 'varIn' is not a floating point type then it is promoted to
'NC_DOUBLE' for the system call in the Udunits library; and then demoted
back to to its original type after.
     T[lon]={0.0,100.0,150.0,200.0};
     T@units="Celsius";
     // Overwrite variable
     T=udunits(T,"kelvin"); 
     print(T);  
     // 273.15, 373.15, 423.15, 473.15 ;
     T@units="kelvin";
     
     // Rebase coordinate days to hours 
     timeOld=time;
     print(timeOld);
     // 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ;
     timeOld@units="days since 2012-01-30";
     
     @units="hours since 2012-02-01 01:00";
     timeNew=udunits(timeOld, @units);
     timeNew@units=@units;
     print(timeNew);
     // -25, -1, 23, 47, 71, 95, 119, 143, 167, 191 ;
     
     tOld=time;
     // nb in this calendar NO Leap year
     tOld@calendar="365_day";
     tOld@units="minutes since 2012-02-28 23:58:00.00";
     
     @units="seconds since 2012-03-01 00:00";
     tNew=udunits(tOld, @units);
     tNew@units=@units;
     print(tNew);
     // -60, 0, 60, 120, 180, 240, 300, 360, 420, 480 

strftime() The 'var_str=strtime(var_time,fmt_sng)' method takes a
time-based variable and a format string and returns an NC_STRING
variable (of the same shape as var_time) of time-stamps in the form
specified by 'fmt_sng'.  In order to run this command output type must
be netCDF-4.
     ncap2 -4  -v -O -s 'time_str=strftime(time,"%Y-%m-%d");' in.nc foo.nc
     
     time_str="1964-03-13", "1964-03-14", "1964-03-15", "1964-03-16", 
              "1964-03-17", "1964-03-18", "1964-03-19", "1964-03-20", 
              "1964-03-21", "1964-03-22" ;
     

   Under the hood there are a few steps invoved.
First the method reads 'var_time@units' and 'var_time@calendar' (if
present) then converts var_time to 'seconds since 1970-01-01'.
It then converts these possibly UTC seconds to the standard struture
'struct *tm'.
Finally strftime is called with fmt_sng and the *tm struct.
The c-standard 'strftime' is used as defined in 'time.h'
If the method is called without fmt_sng then the following default is
used: '"%Y-%m-%d %H:%M:%S"'
The method 'regular' takes a single var argument and uses the above
default string

     ncap2 -4  -v -O -s 'time_str=regular(time);' in.nc foo.nc
     
     time_str = "1964-03-13 21:09:00", "1964-03-14 21:09:00", "1964-03-15 21:09:00", 
                "1964-03-16 21:09:00", "1964-03-17 21:09:00", "1964-03-18 21:09:00", 
                "1964-03-19 21:09:00", "1964-03-20 21:09:00", "1964-03-21 21:09:00", 
                "1964-03-22 21:09:00" ;

Another working example
     ncap2 -v -O -s 'ts=strftime(frametime(0),"%Y-%m-%d/envlog_netcdf_L1_ua-mac_%Y-%m-%d.nc");' in.nc out.nc
     ts="2017-08-11/envlog_netcdf_L1_ua-mac_2017-08-11.nc" 


File: nco.info,  Node: Vpointer,  Next: Irregular grids,  Prev: UDUnits script,  Up: ncap2 netCDF Arithmetic Processor

4.1.19 Vpointer
---------------

A variable-pointer or _vpointer_ is a pointer to a variable or
attribute.  It is most useful when one needs to apply a set of
operations on a list of variables.  For example, after regular
processing one may wish to set the '_FillVlaue' of all 'NC_FLOAT'
variables to a particular value, or to create min/max attributes for all
3D variables of type 'NC_DOUBLE'.  A vpointer is not a 'pointer' to a
memory location in the C/C++ sense.  Rather the vpointer is a text
attribute that contains the name of a variable.  To use the pointer
simply prefix the pointer with '*'.  Then, most places where you use
'VAR_ID' you can use *vpointer_nm.  There are a variety of ways to
maintain a list of strings in 'ncap2'.  The easiest method is to use an
'NC_STRING' attribute.

   Below is a simple illustration that uses a vpointer of type
'NC_CHAR'.  Remember an attribute starting with '@' implies 'global',
e.g., '@vpx' is short for 'global@vpx'.
     idx=9;
     idy=20;
     t2=time;
     
     global@vpx="idx";
     
     // Increment idx by one
     *global@vpx++;  
     print(idx);
     
     // Multiply by 5
     *@vpx*=5; // idx now 50
     print(idx);
     
     // Add 200 (long method)
     *@vpx=*@vpx+200; //idx now 250
     print(idx);
     
     @vpy="idy";
     
     // Add idx idy to get idz
     idz=*@vpx+*@vpy; // idz == 270
     print(idz);
     
     // We can also reference variables in the input file
     // Can use an existing attribute pointer since attributes are not written
     // to the netCDF file until after the script has finished.
     @vpx="three_dmn_var";
     
     // We can convert this variable to type NC_DOUBLE and
     // write it to ouptut all at once
     *@vpx=*@vpx.double();

   The following script writes to the output files all variables that
are of type 'NC_DOUBLE' and that have at least two dimensions.  It then
changes their '_FillValue' to '1.0E-9'.  The function 'get_vars_in()'
creates an 'NC_STRING' attribute that contains all of the variable names
in the input file.  Note that a vpointer must be a plain attribute, NOT
an a attribute expression.  Thus in the below script using '*all(idx)'
would be a fundamental mistake.  In the below example the vpointer
'var_nm' is of type 'NC_STRING'.
     @all=get_vars_in();
     
     *sz=@all.size();
     *idx=0;
     
     for(idx=0;idx<sz;idx++){
       // @var_nm is of type NC_STRING
       @var_nm=@all(idx);
      
       if(*@var_nm.type() == NC_DOUBLE && *@var_nm.ndims() >= 2){
          *@var_nm=*@var_nm; 
          *@var_nm.change_miss(1e-9d);
       }
     }

   The following script writes to the output file all 3D/4D variables of
type 'NC_FLOAT'.  Then for each variable it calculates a 'range'
attribute that contains the maximum and minimum values, and a 'total'
attribute that is the sum of all the elements.  In this example
vpointers are used to 'point' to attributes.
     @all=get_vars_in();
     *sz=@all.size();
     
     for(*idx=0;idx<sz;idx++){
       @var_nm=@all(idx);
       if(*@var_nm.ndims() >= 3){
         *@var_nm=*@var_nm.float();
         // The push function also takes a call-by-ref att -if it  doesnt already exist then it is created
         // the call below is pushing a NC_STRING to an att so the end result is a list of NC_STRINGS   
         push(&@prc,@var_nm); 
       }
     } 
     
     *sz=@prc.size();
     
     for(*idx=0;idx<sz;idx++){
       @var_nm=@prc(idx);
     
       // we can work with att pointers as well 
       // sprint - ouptut is of type NC_CHAR
       @att_total=sprint(@var_nm,"%s@total"); 
       @att_range=sprint(@var_nm,"%s@range"); 
     
       // if you are still confused then print out the atts 
       print(@att_total); 
       print(@att_range);
      
       *@att_total= *@var_nm.total();
       *@att_range={ min(*@var_nm), max(*@var_nm)}; 
     } 

This is an ncdump of one of the variables that has been processed by the
above script
     float three_dmn_var_int(time, lat, lon) ;
     three_dmn_var_int:_FillValue = -99.f ;
     three_dmn_var_int:long_name = "three dimensional record variable of type int" ;
     three_dmn_var_int:range = 1.f, 80.f ;
     three_dmn_var_int:total = 2701.f ;
     three_dmn_var_int:units = "watt meter-2" ;


File: nco.info,  Node: Irregular grids,  Next: Bilinear interpolation,  Prev: Vpointer,  Up: ncap2 netCDF Arithmetic Processor

4.1.20 Irregular Grids
----------------------

NCO is capable of analyzing datasets for many different underlying
coordinate grid types.  netCDF was developed for and initially used with
grids comprised of orthogonal dimensions forming a rectangular
coordinate system.  We call such grids _standard_ grids.  It is
increasingly common for datasets to use metadata to describe much more
complex grids.  Let us first define three important coordinate grid
properties: rectangularity, regularity, and fxm.

   Grids are _regular_ if the spacing between adjacent is constant.  For
example, a 4-by-5 degree latitude-longitude grid is regular because the
spacings between adjacent latitudes (4 degrees) are constant as are the
(5 degrees) spacings between adjacent longitudes.  Spacing in
_irregular_ grids depends on the location along the coordinate.  Grids
such as Gaussian grids have uneven spacing in latitude (points cluster
near the equator) and so are irregular.

   Grids are _rectangular_ if the number of elements in any dimension is
not a function of any other dimension.  For example, a T42 Gaussian
latitude-longitude grid is rectangular because there are the same number
of longitudes (128) for each of the (64) latitudes.  Grids are
_non-rectangular_ if the elements in any dimension depend on another
dimension.  Non-rectangular grids present many special challenges to
analysis software like NCO.

   Wrapped coordinates (*note Wrapped Coordinates::), such as longitude,
are independent of these grid properties (regularity, rectangularity).

   The preferred NCO technique to analyze data on non-standard
coordinate grids is to create a region mask with 'ncap2', and then to
use the mask within 'ncap2' for variable-specific processing, and/or
with other operators (e.g., 'ncwa', 'ncdiff') for entire file
processing.

   Before describing the construction of masks, let us review how
irregularly gridded geoscience data are described.  Say that latitude
and longitude are stored as R-dimensional arrays and the product of the
dimension sizes is the total number of elements N in the other
variables.  Geoscience applications tend to use R=1, R=2, and R=3.

   If the grid is has no simple representation (e.g., discontinuous)
then it makes sense to store all coordinates as 1D arrays with the same
size as the number of grid points.  These gridpoints can be completely
independent of all the other (own weight, area, etc.).

   R=1: lat(number_of_gridpoints) and lon(number_of_gridpoints)

   If the horizontal grid is time-invariant then R=2 is common:

   R=2: lat(south_north,east_west) and lon(south_north,east_west)

   The Weather and Research Forecast (WRF) model uses R=3:

   R=3: lat(time,south_north,east_west), lon(time,south_north,east_west)

   and so supports grids that change with time.

   Grids with R > 1 often use missing values to indicated empty points.
For example, so-called "staggered grids" will use fewer east_west points
near the poles and more near the equator.  netCDF only accepts
rectangular arrays so space must be allocated for the maximum number of
east_west points at all latitudes.  Then the application writes missing
values into the unused points near the poles.

   We demonstrate the 'ncap2' analysis technique for irregular regions
by constructing a mask for an R=2 grid.  We wish to find, say, the mean
temperature within [LAT_MIN,LAT_MAX] and [LON_MIN,LON_MAX]:
     ncap2 -s 'mask_var= (lat >= lat_min && lat <= lat_max) && \
                         (lon >= lon_min && lon <= lon_max);' in.nc out.nc
   Arbitrarily shaped regions can be defined by more complex conditional
statements.  Once defined, masks can be applied to specific variables,
and to entire files:
     ncap2 -s 'temperature_avg=(temperature*mask_var).avg()' in.nc out.nc
     ncwa -a lat,lon -m mask_var -w area in.nc out.nc
   Crafting such commands on the command line is possible though
unwieldy.  In such cases, a script is often cleaner and allows you to
document the procedure:
     cat > ncap2.in << 'EOF'
     mask_var = (lat >= lat_min && lat <= lat_max) && (lon >= lon_min && > lon <= lon_max);
     if(mask_var.total() > 0){ // Check that mask contains some valid values
       temperature_avg=(temperature*mask_var).avg(); // Average temperature
       temperature_max=(temperature*mask_var).max(); // Maximum temperature
     }
     EOF
     ncap2 -S ncap2.in in.nc out.nc

   Grids like those produced by the WRF model are complex because one
must use global metadata to determine the grid staggering and offsets to
translate 'XLAT' and 'XLONG' into real latitudes, longitudes, and
missing points.  The WRF grid documentation should describe this.  For
WRF files creating regional masks looks, in general, like
     mask_var = (XLAT >= lat_min && XLAT <= lat_max) && (XLONG >= lon_min && XLONG <= lon_max);

   A few notes: Irregular regions are the union of arrays of lat/lon
min/max's.  The mask procedure is identical for all R.


File: nco.info,  Node: Bilinear interpolation,  Next: GSL special functions,  Prev: Irregular grids,  Up: ncap2 netCDF Arithmetic Processor

4.1.21 Bilinear interpolation
-----------------------------

As of version 4.0.0 NCO has internal routines to perform bilinear
interpolation on gridded data sets.  In mathematics, bilinear
interpolation is an extension of linear interpolation for interpolating
functions of two variables on a regular grid.  The idea is to perform
linear interpolation first in one direction, and then again in the other
direction.

   Suppose we have an irregular grid of data 'temperature[lat,lon]',
with co-ordinate vars 'lat[lat], lon[lon]'.  We wish to find the
temperature at an arbitary point [X,Y] within the grid.  If we can
locate lat_min,lat_max and lon_min,lon_max such that 'lat_min <= X <=
lat_max' and 'lon_min <= Y <= lon_max' then we can interpolate in two
dimensions the temperature at [X,Y].

   The general form of the 'ncap2' interpolation function is
     var_out=bilinear_interp(grid_in,grid_out,grid_out_x,grid_out_y,grid_in_x,grid_in_y)
   where
'grid_in'
     Input function data.  Usually a two dimensional variable.  It must
     be of size 'grid_in_x.size()*grid_in_y.size()'
'grid_out'
     This variable is the shape of 'var_out'.  Usually a two dimensional
     variable.  It must be of size 'grid_out_x.size()*grid_out_y.size()'
'grid_out_x'
     X output values
'grid_out_y'
     Y output values
'grid_in_x'
     X input values values.  Must be monotonic (increasing or
     decreasing).
'grid_in_y'
     Y input values values.  Must be monotonic (increasing or
     decreasing).
Prior to calculations all arguments are converted to type 'NC_DOUBLE'.
After calculations 'var_out' is converted to the input type of
'grid_in'.

   Suppose the first part of an 'ncap2' script is
     defdim("X",4);
     defdim("Y",5);
     
     // Temperature
     T_in[$X,$Y]=
      {100, 200, 300, 400, 500,
       101, 202, 303, 404, 505,
       102, 204, 306, 408, 510,
       103, 206, 309, 412, 515.0 };
     
     // Coordinate variables
     x_in[$X]={0.0,1.0,2.0,3.01};
     y_in[$Y]={1.0,2.0,3.0,4.0,5};
   Now we interpolate with the following variables:
     defdim("Xn",3);
     defdim("Yn",4); 
     T_out[$Xn,$Yn]=0.0;
     x_out[$Xn]={0.0,0.02,3.01};
     y_out[$Yn]={1.1,2.0,3,4};
     
     var_out=bilinear_interp(T_in,T_out,x_out,y_out,x_in,y_in);
     print(var_out);
     // 110, 200, 300, 400,
     // 110.022, 200.04, 300.06, 400.08,
     // 113.3, 206, 309, 412 ;

   It is possible to interpolate a single point:
     var_out=bilinear_interp(T_in,0.0,3.0,4.99,x_in,y_in);
     print(var_out);
     // 513.920594059406

*Wrapping and Extrapolation*
The function 'bilinear_interp_wrap()' takes the same arguments as
'bilinear_interp()' but performs wrapping (Y) and extrapolation (X) for
points off the edge of the grid.  If the given range of longitude is say
(25-335) and we have a point at 20 degrees, then the endpoints of the
range are used for the interpolation.  This is what wrapping means.  For
wrapping to occur Y must be longitude and must be in the range (0,360)
or (-180,180).  There are no restrictions on the longitude (X) values,
though typically these are in the range (-90,90).  This 'ncap2' script
illustrates both wrapping and extrapolation of end points:
     defdim("lat_in",6);
     defdim("lon_in",5);
     
     // Coordinate input vars
     lat_in[$lat_in]={-80,-40,0,30,60.0,85.0};
     lon_in[$lon_in]={30, 110, 190, 270, 350.0};
     
     T_in[$lat_in,$lon_in]=
       {10,40,50,30,15,   
         12,43,52,31,16,   
         14,46,54,32,17,   
         16,49,56,33,18,   
         18,52,58,34,19,   
         20,55,60,35,20.0 };
        
     defdim("lat_out",4);
     defdim("lon_out",3);
     
     // Coordinate variables
     lat_out[$lat_out]={-90,0,70,88.0};   
     lon_out[$lon_out]={0,190,355.0};
     
     T_out[$lat_out,$lon_out]=0.0;
     
     T_out=bilinear_interp_wrap(T_in,T_out,lat_out,lon_out,lat_in,lon_in);
     print(T_out); 
     // 13.4375, 49.5, 14.09375,
     // 16.25, 54, 16.625,
     // 19.25, 58.8, 19.325,
     // 20.15, 60.24, 20.135 ;


File: nco.info,  Node: GSL special functions,  Next: GSL interpolation,  Prev: Bilinear interpolation,  Up: ncap2 netCDF Arithmetic Processor

4.1.22 GSL special functions
----------------------------

As of version 3.9.6 (released January, 2009), NCO can link to the GNU
Scientific Library (GSL).  'ncap2' can access most GSL special functions
including Airy, Bessel, error, gamma, beta, hypergeometric, and Legendre
functions and elliptical integrals.  GSL must be version 1.4 or later.
To list the GSL functions available with your NCO build, use 'ncap2 -f |
grep ^gsl'.

The function names used by ncap2 mirror their GSL names.  The NCO
wrappers for GSL functions automatically call the error-handling version
of the GSL function when available (1).  This allows NCO to return a
missing value when the GSL library encounters a domain error or a
floating-point exception.  The slow-down due to calling the
error-handling version of the GSL numerical functions was found to be
negligible (please let us know if you find otherwise).

Consider the gamma function.
The GSL function prototype is
'int gsl_sf_gamma_e(const double x, gsl_sf_result * result)' The 'ncap2'
script would be:
     lon_in[lon]={-1,0.1,0,2,0.3};
     lon_out=gsl_sf_gamma(lon_in);
     lon_out= _, 9.5135, 4.5908, 2.9915 

The first value is set to '_FillValue' since the gamma function is
undefined for negative integers.  If the input variable has a missing
value then this value is used.  Otherwise, the default double fill value
is used (defined in the netCDF header 'netcdf.h' as 'NC_FILL_DOUBLE =
9.969e+36').

Consider a call to a Bessel function with GSL prototype
'int gsl_sf_bessel_Jn_e(int n, double x, gsl_sf_result * result)'

   An 'ncap2' script would be
     lon_out=gsl_sf_bessel_Jn(2,lon_in);
     lon_out=0.11490, 0.0012, 0.00498, 0.011165
   This computes the Bessel function of order N=2 for every value in
'lon_in'.  The Bessel order argument, an integer, can also be a
non-scalar variable, i.e., an array.
     n_in[lon]={0,1,2,3};
     lon_out=gsl_sf_bessel_Jn(n_in,0.5);
     lon_out= 0.93846, 0.24226, 0.03060, 0.00256

Arguments to GSL wrapper functions in 'ncap2' must conform to one
another, i.e., they must share the same sub-set of dimensions.  For
example: 'three_out=gsl_sf_bessel_Jn(n_in,three_dmn_var_dbl)' is valid
because the variable 'three_dmn_var_dbl' has a LON dimension, so 'n_in'
in can be broadcast to conform to 'three_dmn_var_dbl'.  However
'time_out=gsl_sf_bessel_Jn(n_in,time)' is invalid.

   Consider the elliptical integral with prototype 'int
gsl_sf_ellint_RD_e(double x, double y, double z, gsl_mode_t mode,
gsl_sf_result * result)'
     three_out=gsl_sf_ellint_RD(0.5,time,three_dmn_var_dbl);

The three arguments are all conformable so the above 'ncap2' call is
valid.  The mode argument in the function prototype controls the
convergence of the algorithm.  It also appears in the Airy Function
prototypes.  It can be set by defining the environment variable
'GSL_PREC_MODE'.  If unset it defaults to the value 'GSL_PREC_DOUBLE'.
See the GSL manual for more details.
     export GSL_PREC_MODE=0 // GSL_PREC_DOUBLE
     export GSL_PREC_MODE=1 // GSL_PREC_SINGLE
     export GSL_PREC_MODE=2 // GSL_PREC_APPROX

The 'ncap2' wrappers to the array functions are slightly different.
Consider the following GSL prototype
'int gsl_sf_bessel_Jn_array(int nmin, int nmax, double x, double
*result_array)'
     b1=lon.double();
     x=0.5;
     status=gsl_sf_bessel_Jn_array(1,4,x,&b1);
     print(status);
     b1=0.24226,0.0306,0.00256,0.00016;
This calculates the Bessel function of X=0.5 for N=1 to 4.  The first
three arguments are scalar values.  If a non-scalar variable is supplied
as an argument then only the first value is used.  The final argument is
the variable where the results are stored (NB: the '&' indicates this is
a call by reference).  This final argument must be of type 'double' and
must be of least size NMAX-NMIN+1.  If either of these conditions is not
met then then the function returns an error message.  The
function/wrapper returns a status flag.  Zero indicates success.

Consider another array function
'int gsl_sf_legendre_Pl_array(int lmax, double x, double
*result_array);'
     a1=time.double();
     x=0.3;
     status=gsl_sf_legendre_Pl_array(a1.size()-1, x,&a1);
     print(status);
This call calculates P_L(0.3) for L=0..9.  Note that |X|<=1, otherwise
there will be a domain error.  See the GSL documentation for more
details.

The GSL functions implemented in NCO are listed in the table below.
This table is correct for GSL version 1.10.  To see what functions are
available on your build run the command 'ncap2 -f |grep ^gsl' .  To see
this table along with the GSL C-function prototypes look at the
spreadsheet *doc/nco_gsl.ods*.


*GSL NAME*                *I*  *NCAP FUNCTION CALL*
gsl_sf_airy_Ai_e          Y    gsl_sf_airy_Ai(dbl_expr)
gsl_sf_airy_Bi_e          Y    gsl_sf_airy_Bi(dbl_expr)
gsl_sf_airy_Ai_scaled_e   Y    gsl_sf_airy_Ai_scaled(dbl_expr)
gsl_sf_airy_Bi_scaled_e   Y    gsl_sf_airy_Bi_scaled(dbl_expr)
gsl_sf_airy_Ai_deriv_e    Y    gsl_sf_airy_Ai_deriv(dbl_expr)
gsl_sf_airy_Bi_deriv_e    Y    gsl_sf_airy_Bi_deriv(dbl_expr)
gsl_sf_airy_Ai_deriv_scaled_eY gsl_sf_airy_Ai_deriv_scaled(dbl_expr)
gsl_sf_airy_Bi_deriv_scaled_eY gsl_sf_airy_Bi_deriv_scaled(dbl_expr)
gsl_sf_airy_zero_Ai_e     Y    gsl_sf_airy_zero_Ai(uint_expr)
gsl_sf_airy_zero_Bi_e     Y    gsl_sf_airy_zero_Bi(uint_expr)
gsl_sf_airy_zero_Ai_deriv_eY   gsl_sf_airy_zero_Ai_deriv(uint_expr)
gsl_sf_airy_zero_Bi_deriv_eY   gsl_sf_airy_zero_Bi_deriv(uint_expr)
gsl_sf_bessel_J0_e        Y    gsl_sf_bessel_J0(dbl_expr)
gsl_sf_bessel_J1_e        Y    gsl_sf_bessel_J1(dbl_expr)
gsl_sf_bessel_Jn_e        Y    gsl_sf_bessel_Jn(int_expr,dbl_expr)
gsl_sf_bessel_Jn_array    Y    status=gsl_sf_bessel_Jn_array(int,int,double,&var_out)
gsl_sf_bessel_Y0_e        Y    gsl_sf_bessel_Y0(dbl_expr)
gsl_sf_bessel_Y1_e        Y    gsl_sf_bessel_Y1(dbl_expr)
gsl_sf_bessel_Yn_e        Y    gsl_sf_bessel_Yn(int_expr,dbl_expr)
gsl_sf_bessel_Yn_array    Y    gsl_sf_bessel_Yn_array
gsl_sf_bessel_I0_e        Y    gsl_sf_bessel_I0(dbl_expr)
gsl_sf_bessel_I1_e        Y    gsl_sf_bessel_I1(dbl_expr)
gsl_sf_bessel_In_e        Y    gsl_sf_bessel_In(int_expr,dbl_expr)
gsl_sf_bessel_In_array    Y    status=gsl_sf_bessel_In_array(int,int,double,&var_out)
gsl_sf_bessel_I0_scaled_e Y    gsl_sf_bessel_I0_scaled(dbl_expr)
gsl_sf_bessel_I1_scaled_e Y    gsl_sf_bessel_I1_scaled(dbl_expr)
gsl_sf_bessel_In_scaled_e Y    gsl_sf_bessel_In_scaled(int_expr,dbl_expr)
gsl_sf_bessel_In_scaled_arrayY staus=gsl_sf_bessel_In_scaled_array(int,int,double,&var_out)
gsl_sf_bessel_K0_e        Y    gsl_sf_bessel_K0(dbl_expr)
gsl_sf_bessel_K1_e        Y    gsl_sf_bessel_K1(dbl_expr)
gsl_sf_bessel_Kn_e        Y    gsl_sf_bessel_Kn(int_expr,dbl_expr)
gsl_sf_bessel_Kn_array    Y    status=gsl_sf_bessel_Kn_array(int,int,double,&var_out)
gsl_sf_bessel_K0_scaled_e Y    gsl_sf_bessel_K0_scaled(dbl_expr)
gsl_sf_bessel_K1_scaled_e Y    gsl_sf_bessel_K1_scaled(dbl_expr)
gsl_sf_bessel_Kn_scaled_e Y    gsl_sf_bessel_Kn_scaled(int_expr,dbl_expr)
gsl_sf_bessel_Kn_scaled_arrayY status=gsl_sf_bessel_Kn_scaled_array(int,int,double,&var_out)
gsl_sf_bessel_j0_e        Y    gsl_sf_bessel_J0(dbl_expr)
gsl_sf_bessel_j1_e        Y    gsl_sf_bessel_J1(dbl_expr)
gsl_sf_bessel_j2_e        Y    gsl_sf_bessel_j2(dbl_expr)
gsl_sf_bessel_jl_e        Y    gsl_sf_bessel_jl(int_expr,dbl_expr)
gsl_sf_bessel_jl_array    Y    status=gsl_sf_bessel_jl_array(int,double,&var_out)
gsl_sf_bessel_jl_steed_arrayY  gsl_sf_bessel_jl_steed_array
gsl_sf_bessel_y0_e        Y    gsl_sf_bessel_Y0(dbl_expr)
gsl_sf_bessel_y1_e        Y    gsl_sf_bessel_Y1(dbl_expr)
gsl_sf_bessel_y2_e        Y    gsl_sf_bessel_y2(dbl_expr)
gsl_sf_bessel_yl_e        Y    gsl_sf_bessel_yl(int_expr,dbl_expr)
gsl_sf_bessel_yl_array    Y    status=gsl_sf_bessel_yl_array(int,double,&var_out)
gsl_sf_bessel_i0_scaled_e Y    gsl_sf_bessel_I0_scaled(dbl_expr)
gsl_sf_bessel_i1_scaled_e Y    gsl_sf_bessel_I1_scaled(dbl_expr)
gsl_sf_bessel_i2_scaled_e Y    gsl_sf_bessel_i2_scaled(dbl_expr)
gsl_sf_bessel_il_scaled_e Y    gsl_sf_bessel_il_scaled(int_expr,dbl_expr)
gsl_sf_bessel_il_scaled_arrayY status=gsl_sf_bessel_il_scaled_array(int,double,&var_out)
gsl_sf_bessel_k0_scaled_e Y    gsl_sf_bessel_K0_scaled(dbl_expr)
gsl_sf_bessel_k1_scaled_e Y    gsl_sf_bessel_K1_scaled(dbl_expr)
gsl_sf_bessel_k2_scaled_e Y    gsl_sf_bessel_k2_scaled(dbl_expr)
gsl_sf_bessel_kl_scaled_e Y    gsl_sf_bessel_kl_scaled(int_expr,dbl_expr)
gsl_sf_bessel_kl_scaled_arrayY status=gsl_sf_bessel_kl_scaled_array(int,double,&var_out)
gsl_sf_bessel_Jnu_e       Y    gsl_sf_bessel_Jnu(dbl_expr,dbl_expr)
gsl_sf_bessel_Ynu_e       Y    gsl_sf_bessel_Ynu(dbl_expr,dbl_expr)
gsl_sf_bessel_sequence_Jnu_eN  gsl_sf_bessel_sequence_Jnu
gsl_sf_bessel_Inu_scaled_eY    gsl_sf_bessel_Inu_scaled(dbl_expr,dbl_expr)
gsl_sf_bessel_Inu_e       Y    gsl_sf_bessel_Inu(dbl_expr,dbl_expr)
gsl_sf_bessel_Knu_scaled_eY    gsl_sf_bessel_Knu_scaled(dbl_expr,dbl_expr)
gsl_sf_bessel_Knu_e       Y    gsl_sf_bessel_Knu(dbl_expr,dbl_expr)
gsl_sf_bessel_lnKnu_e     Y    gsl_sf_bessel_lnKnu(dbl_expr,dbl_expr)
gsl_sf_bessel_zero_J0_e   Y    gsl_sf_bessel_zero_J0(uint_expr)
gsl_sf_bessel_zero_J1_e   Y    gsl_sf_bessel_zero_J1(uint_expr)
gsl_sf_bessel_zero_Jnu_e  N    gsl_sf_bessel_zero_Jnu
gsl_sf_clausen_e          Y    gsl_sf_clausen(dbl_expr)
gsl_sf_hydrogenicR_1_e    N    gsl_sf_hydrogenicR_1
gsl_sf_hydrogenicR_e      N    gsl_sf_hydrogenicR
gsl_sf_coulomb_wave_FG_e  N    gsl_sf_coulomb_wave_FG
gsl_sf_coulomb_wave_F_arrayN   gsl_sf_coulomb_wave_F_array
gsl_sf_coulomb_wave_FG_arrayN  gsl_sf_coulomb_wave_FG_array
gsl_sf_coulomb_wave_FGp_arrayN gsl_sf_coulomb_wave_FGp_array
gsl_sf_coulomb_wave_sphF_arrayNgsl_sf_coulomb_wave_sphF_array
gsl_sf_coulomb_CL_e       N    gsl_sf_coulomb_CL
gsl_sf_coulomb_CL_array   N    gsl_sf_coulomb_CL_array
gsl_sf_coupling_3j_e      N    gsl_sf_coupling_3j
gsl_sf_coupling_6j_e      N    gsl_sf_coupling_6j
gsl_sf_coupling_RacahW_e  N    gsl_sf_coupling_RacahW
gsl_sf_coupling_9j_e      N    gsl_sf_coupling_9j
gsl_sf_coupling_6j_INCORRECT_eNgsl_sf_coupling_6j_INCORRECT
gsl_sf_dawson_e           Y    gsl_sf_dawson(dbl_expr)
gsl_sf_debye_1_e          Y    gsl_sf_debye_1(dbl_expr)
gsl_sf_debye_2_e          Y    gsl_sf_debye_2(dbl_expr)
gsl_sf_debye_3_e          Y    gsl_sf_debye_3(dbl_expr)
gsl_sf_debye_4_e          Y    gsl_sf_debye_4(dbl_expr)
gsl_sf_debye_5_e          Y    gsl_sf_debye_5(dbl_expr)
gsl_sf_debye_6_e          Y    gsl_sf_debye_6(dbl_expr)
gsl_sf_dilog_e            N    gsl_sf_dilog
gsl_sf_complex_dilog_xy_e N    gsl_sf_complex_dilog_xy_e
gsl_sf_complex_dilog_e    N    gsl_sf_complex_dilog
gsl_sf_complex_spence_xy_eN    gsl_sf_complex_spence_xy_e
gsl_sf_multiply_e         N    gsl_sf_multiply
gsl_sf_multiply_err_e     N    gsl_sf_multiply_err
gsl_sf_ellint_Kcomp_e     Y    gsl_sf_ellint_Kcomp(dbl_expr)
gsl_sf_ellint_Ecomp_e     Y    gsl_sf_ellint_Ecomp(dbl_expr)
gsl_sf_ellint_Pcomp_e     Y    gsl_sf_ellint_Pcomp(dbl_expr,dbl_expr)
gsl_sf_ellint_Dcomp_e     Y    gsl_sf_ellint_Dcomp(dbl_expr)
gsl_sf_ellint_F_e         Y    gsl_sf_ellint_F(dbl_expr,dbl_expr)
gsl_sf_ellint_E_e         Y    gsl_sf_ellint_E(dbl_expr,dbl_expr)
gsl_sf_ellint_P_e         Y    gsl_sf_ellint_P(dbl_expr,dbl_expr,dbl_expr)
gsl_sf_ellint_D_e         Y    gsl_sf_ellint_D(dbl_expr,dbl_expr,dbl_expr)
gsl_sf_ellint_RC_e        Y    gsl_sf_ellint_RC(dbl_expr,dbl_expr)
gsl_sf_ellint_RD_e        Y    gsl_sf_ellint_RD(dbl_expr,dbl_expr,dbl_expr)
gsl_sf_ellint_RF_e        Y    gsl_sf_ellint_RF(dbl_expr,dbl_expr,dbl_expr)
gsl_sf_ellint_RJ_e        Y    gsl_sf_ellint_RJ(dbl_expr,dbl_expr,dbl_expr,dbl_expr)
gsl_sf_elljac_e           N    gsl_sf_elljac
gsl_sf_erfc_e             Y    gsl_sf_erfc(dbl_expr)
gsl_sf_log_erfc_e         Y    gsl_sf_log_erfc(dbl_expr)
gsl_sf_erf_e              Y    gsl_sf_erf(dbl_expr)
gsl_sf_erf_Z_e            Y    gsl_sf_erf_Z(dbl_expr)
gsl_sf_erf_Q_e            Y    gsl_sf_erf_Q(dbl_expr)
gsl_sf_hazard_e           Y    gsl_sf_hazard(dbl_expr)
gsl_sf_exp_e              Y    gsl_sf_exp(dbl_expr)
gsl_sf_exp_e10_e          N    gsl_sf_exp_e10
gsl_sf_exp_mult_e         Y    gsl_sf_exp_mult(dbl_expr,dbl_expr)
gsl_sf_exp_mult_e10_e     N    gsl_sf_exp_mult_e10
gsl_sf_expm1_e            Y    gsl_sf_expm1(dbl_expr)
gsl_sf_exprel_e           Y    gsl_sf_exprel(dbl_expr)
gsl_sf_exprel_2_e         Y    gsl_sf_exprel_2(dbl_expr)
gsl_sf_exprel_n_e         Y    gsl_sf_exprel_n(int_expr,dbl_expr)
gsl_sf_exp_err_e          Y    gsl_sf_exp_err(dbl_expr,dbl_expr)
gsl_sf_exp_err_e10_e      N    gsl_sf_exp_err_e10
gsl_sf_exp_mult_err_e     N    gsl_sf_exp_mult_err
gsl_sf_exp_mult_err_e10_e N    gsl_sf_exp_mult_err_e10
gsl_sf_expint_E1_e        Y    gsl_sf_expint_E1(dbl_expr)
gsl_sf_expint_E2_e        Y    gsl_sf_expint_E2(dbl_expr)
gsl_sf_expint_En_e        Y    gsl_sf_expint_En(int_expr,dbl_expr)
gsl_sf_expint_E1_scaled_e Y    gsl_sf_expint_E1_scaled(dbl_expr)
gsl_sf_expint_E2_scaled_e Y    gsl_sf_expint_E2_scaled(dbl_expr)
gsl_sf_expint_En_scaled_e Y    gsl_sf_expint_En_scaled(int_expr,dbl_expr)
gsl_sf_expint_Ei_e        Y    gsl_sf_expint_Ei(dbl_expr)
gsl_sf_expint_Ei_scaled_e Y    gsl_sf_expint_Ei_scaled(dbl_expr)
gsl_sf_Shi_e              Y    gsl_sf_Shi(dbl_expr)
gsl_sf_Chi_e              Y    gsl_sf_Chi(dbl_expr)
gsl_sf_expint_3_e         Y    gsl_sf_expint_3(dbl_expr)
gsl_sf_Si_e               Y    gsl_sf_Si(dbl_expr)
gsl_sf_Ci_e               Y    gsl_sf_Ci(dbl_expr)
gsl_sf_atanint_e          Y    gsl_sf_atanint(dbl_expr)
gsl_sf_fermi_dirac_m1_e   Y    gsl_sf_fermi_dirac_m1(dbl_expr)
gsl_sf_fermi_dirac_0_e    Y    gsl_sf_fermi_dirac_0(dbl_expr)
gsl_sf_fermi_dirac_1_e    Y    gsl_sf_fermi_dirac_1(dbl_expr)
gsl_sf_fermi_dirac_2_e    Y    gsl_sf_fermi_dirac_2(dbl_expr)
gsl_sf_fermi_dirac_int_e  Y    gsl_sf_fermi_dirac_int(int_expr,dbl_expr)
gsl_sf_fermi_dirac_mhalf_eY    gsl_sf_fermi_dirac_mhalf(dbl_expr)
gsl_sf_fermi_dirac_half_e Y    gsl_sf_fermi_dirac_half(dbl_expr)
gsl_sf_fermi_dirac_3half_eY    gsl_sf_fermi_dirac_3half(dbl_expr)
gsl_sf_fermi_dirac_inc_0_eY    gsl_sf_fermi_dirac_inc_0(dbl_expr,dbl_expr)
gsl_sf_lngamma_e          Y    gsl_sf_lngamma(dbl_expr)
gsl_sf_lngamma_sgn_e      N    gsl_sf_lngamma_sgn
gsl_sf_gamma_e            Y    gsl_sf_gamma(dbl_expr)
gsl_sf_gammastar_e        Y    gsl_sf_gammastar(dbl_expr)
gsl_sf_gammainv_e         Y    gsl_sf_gammainv(dbl_expr)
gsl_sf_lngamma_complex_e  N    gsl_sf_lngamma_complex
gsl_sf_taylorcoeff_e      Y    gsl_sf_taylorcoeff(int_expr,dbl_expr)
gsl_sf_fact_e             Y    gsl_sf_fact(uint_expr)
gsl_sf_doublefact_e       Y    gsl_sf_doublefact(uint_expr)
gsl_sf_lnfact_e           Y    gsl_sf_lnfact(uint_expr)
gsl_sf_lndoublefact_e     Y    gsl_sf_lndoublefact(uint_expr)
gsl_sf_lnchoose_e         N    gsl_sf_lnchoose
gsl_sf_choose_e           N    gsl_sf_choose
gsl_sf_lnpoch_e           Y    gsl_sf_lnpoch(dbl_expr,dbl_expr)
gsl_sf_lnpoch_sgn_e       N    gsl_sf_lnpoch_sgn
gsl_sf_poch_e             Y    gsl_sf_poch(dbl_expr,dbl_expr)
gsl_sf_pochrel_e          Y    gsl_sf_pochrel(dbl_expr,dbl_expr)
gsl_sf_gamma_inc_Q_e      Y    gsl_sf_gamma_inc_Q(dbl_expr,dbl_expr)
gsl_sf_gamma_inc_P_e      Y    gsl_sf_gamma_inc_P(dbl_expr,dbl_expr)
gsl_sf_gamma_inc_e        Y    gsl_sf_gamma_inc(dbl_expr,dbl_expr)
gsl_sf_lnbeta_e           Y    gsl_sf_lnbeta(dbl_expr,dbl_expr)
gsl_sf_lnbeta_sgn_e       N    gsl_sf_lnbeta_sgn
gsl_sf_beta_e             Y    gsl_sf_beta(dbl_expr,dbl_expr)
gsl_sf_beta_inc_e         N    gsl_sf_beta_inc
gsl_sf_gegenpoly_1_e      Y    gsl_sf_gegenpoly_1(dbl_expr,dbl_expr)
gsl_sf_gegenpoly_2_e      Y    gsl_sf_gegenpoly_2(dbl_expr,dbl_expr)
gsl_sf_gegenpoly_3_e      Y    gsl_sf_gegenpoly_3(dbl_expr,dbl_expr)
gsl_sf_gegenpoly_n_e      N    gsl_sf_gegenpoly_n
gsl_sf_gegenpoly_array    Y    gsl_sf_gegenpoly_array
gsl_sf_hyperg_0F1_e       Y    gsl_sf_hyperg_0F1(dbl_expr,dbl_expr)
gsl_sf_hyperg_1F1_int_e   Y    gsl_sf_hyperg_1F1_int(int_expr,int_expr,dbl_expr)
gsl_sf_hyperg_1F1_e       Y    gsl_sf_hyperg_1F1(dbl_expr,dbl_expr,dbl_expr)
gsl_sf_hyperg_U_int_e     Y    gsl_sf_hyperg_U_int(int_expr,int_expr,dbl_expr)
gsl_sf_hyperg_U_int_e10_e N    gsl_sf_hyperg_U_int_e10
gsl_sf_hyperg_U_e         Y    gsl_sf_hyperg_U(dbl_expr,dbl_expr,dbl_expr)
gsl_sf_hyperg_U_e10_e     N    gsl_sf_hyperg_U_e10
gsl_sf_hyperg_2F1_e       Y    gsl_sf_hyperg_2F1(dbl_expr,dbl_expr,dbl_expr,dbl_expr)
gsl_sf_hyperg_2F1_conj_e  Y    gsl_sf_hyperg_2F1_conj(dbl_expr,dbl_expr,dbl_expr,dbl_expr)
gsl_sf_hyperg_2F1_renorm_eY    gsl_sf_hyperg_2F1_renorm(dbl_expr,dbl_expr,dbl_expr,dbl_expr)
gsl_sf_hyperg_2F1_conj_renorm_eYgsl_sf_hyperg_2F1_conj_renorm(dbl_expr,dbl_expr,dbl_expr,dbl_expr)
gsl_sf_hyperg_2F0_e       Y    gsl_sf_hyperg_2F0(dbl_expr,dbl_expr,dbl_expr)
gsl_sf_laguerre_1_e       Y    gsl_sf_laguerre_1(dbl_expr,dbl_expr)
gsl_sf_laguerre_2_e       Y    gsl_sf_laguerre_2(dbl_expr,dbl_expr)
gsl_sf_laguerre_3_e       Y    gsl_sf_laguerre_3(dbl_expr,dbl_expr)
gsl_sf_laguerre_n_e       Y    gsl_sf_laguerre_n(int_expr,dbl_expr,dbl_expr)
gsl_sf_lambert_W0_e       Y    gsl_sf_lambert_W0(dbl_expr)
gsl_sf_lambert_Wm1_e      Y    gsl_sf_lambert_Wm1(dbl_expr)
gsl_sf_legendre_Pl_e      Y    gsl_sf_legendre_Pl(int_expr,dbl_expr)
gsl_sf_legendre_Pl_array  Y    status=gsl_sf_legendre_Pl_array(int,double,&var_out)
gsl_sf_legendre_Pl_deriv_arrayNgsl_sf_legendre_Pl_deriv_array
gsl_sf_legendre_P1_e      Y    gsl_sf_legendre_P1(dbl_expr)
gsl_sf_legendre_P2_e      Y    gsl_sf_legendre_P2(dbl_expr)
gsl_sf_legendre_P3_e      Y    gsl_sf_legendre_P3(dbl_expr)
gsl_sf_legendre_Q0_e      Y    gsl_sf_legendre_Q0(dbl_expr)
gsl_sf_legendre_Q1_e      Y    gsl_sf_legendre_Q1(dbl_expr)
gsl_sf_legendre_Ql_e      Y    gsl_sf_legendre_Ql(int_expr,dbl_expr)
gsl_sf_legendre_Plm_e     Y    gsl_sf_legendre_Plm(int_expr,int_expr,dbl_expr)
gsl_sf_legendre_Plm_array Y    status=gsl_sf_legendre_Plm_array(int,int,double,&var_out)
gsl_sf_legendre_Plm_deriv_arrayNgsl_sf_legendre_Plm_deriv_array
gsl_sf_legendre_sphPlm_e  Y    gsl_sf_legendre_sphPlm(int_expr,int_expr,dbl_expr)
gsl_sf_legendre_sphPlm_arrayY  status=gsl_sf_legendre_sphPlm_array(int,int,double,&var_out)
gsl_sf_legendre_sphPlm_deriv_arrayNgsl_sf_legendre_sphPlm_deriv_array
gsl_sf_legendre_array_sizeN    gsl_sf_legendre_array_size
gsl_sf_conicalP_half_e    Y    gsl_sf_conicalP_half(dbl_expr,dbl_expr)
gsl_sf_conicalP_mhalf_e   Y    gsl_sf_conicalP_mhalf(dbl_expr,dbl_expr)
gsl_sf_conicalP_0_e       Y    gsl_sf_conicalP_0(dbl_expr,dbl_expr)
gsl_sf_conicalP_1_e       Y    gsl_sf_conicalP_1(dbl_expr,dbl_expr)
gsl_sf_conicalP_sph_reg_e Y    gsl_sf_conicalP_sph_reg(int_expr,dbl_expr,dbl_expr)
gsl_sf_conicalP_cyl_reg_e Y    gsl_sf_conicalP_cyl_reg(int_expr,dbl_expr,dbl_expr)
gsl_sf_legendre_H3d_0_e   Y    gsl_sf_legendre_H3d_0(dbl_expr,dbl_expr)
gsl_sf_legendre_H3d_1_e   Y    gsl_sf_legendre_H3d_1(dbl_expr,dbl_expr)
gsl_sf_legendre_H3d_e     Y    gsl_sf_legendre_H3d(int_expr,dbl_expr,dbl_expr)
gsl_sf_legendre_H3d_array N    gsl_sf_legendre_H3d_array
gsl_sf_legendre_array_sizeN    gsl_sf_legendre_array_size
gsl_sf_log_e              Y    gsl_sf_log(dbl_expr)
gsl_sf_log_abs_e          Y    gsl_sf_log_abs(dbl_expr)
gsl_sf_complex_log_e      N    gsl_sf_complex_log
gsl_sf_log_1plusx_e       Y    gsl_sf_log_1plusx(dbl_expr)
gsl_sf_log_1plusx_mx_e    Y    gsl_sf_log_1plusx_mx(dbl_expr)
gsl_sf_mathieu_a_array    N    gsl_sf_mathieu_a_array
gsl_sf_mathieu_b_array    N    gsl_sf_mathieu_b_array
gsl_sf_mathieu_a          N    gsl_sf_mathieu_a
gsl_sf_mathieu_b          N    gsl_sf_mathieu_b
gsl_sf_mathieu_a_coeff    N    gsl_sf_mathieu_a_coeff
gsl_sf_mathieu_b_coeff    N    gsl_sf_mathieu_b_coeff
gsl_sf_mathieu_ce         N    gsl_sf_mathieu_ce
gsl_sf_mathieu_se         N    gsl_sf_mathieu_se
gsl_sf_mathieu_ce_array   N    gsl_sf_mathieu_ce_array
gsl_sf_mathieu_se_array   N    gsl_sf_mathieu_se_array
gsl_sf_mathieu_Mc         N    gsl_sf_mathieu_Mc
gsl_sf_mathieu_Ms         N    gsl_sf_mathieu_Ms
gsl_sf_mathieu_Mc_array   N    gsl_sf_mathieu_Mc_array
gsl_sf_mathieu_Ms_array   N    gsl_sf_mathieu_Ms_array
gsl_sf_pow_int_e          N    gsl_sf_pow_int
gsl_sf_psi_int_e          Y    gsl_sf_psi_int(int_expr)
gsl_sf_psi_e              Y    gsl_sf_psi(dbl_expr)
gsl_sf_psi_1piy_e         Y    gsl_sf_psi_1piy(dbl_expr)
gsl_sf_complex_psi_e      N    gsl_sf_complex_psi
gsl_sf_psi_1_int_e        Y    gsl_sf_psi_1_int(int_expr)
gsl_sf_psi_1_e            Y    gsl_sf_psi_1(dbl_expr)
gsl_sf_psi_n_e            Y    gsl_sf_psi_n(int_expr,dbl_expr)
gsl_sf_synchrotron_1_e    Y    gsl_sf_synchrotron_1(dbl_expr)
gsl_sf_synchrotron_2_e    Y    gsl_sf_synchrotron_2(dbl_expr)
gsl_sf_transport_2_e      Y    gsl_sf_transport_2(dbl_expr)
gsl_sf_transport_3_e      Y    gsl_sf_transport_3(dbl_expr)
gsl_sf_transport_4_e      Y    gsl_sf_transport_4(dbl_expr)
gsl_sf_transport_5_e      Y    gsl_sf_transport_5(dbl_expr)
gsl_sf_sin_e              N    gsl_sf_sin
gsl_sf_cos_e              N    gsl_sf_cos
gsl_sf_hypot_e            N    gsl_sf_hypot
gsl_sf_complex_sin_e      N    gsl_sf_complex_sin
gsl_sf_complex_cos_e      N    gsl_sf_complex_cos
gsl_sf_complex_logsin_e   N    gsl_sf_complex_logsin
gsl_sf_sinc_e             N    gsl_sf_sinc
gsl_sf_lnsinh_e           N    gsl_sf_lnsinh
gsl_sf_lncosh_e           N    gsl_sf_lncosh
gsl_sf_polar_to_rect      N    gsl_sf_polar_to_rect
gsl_sf_rect_to_polar      N    gsl_sf_rect_to_polar
gsl_sf_sin_err_e          N    gsl_sf_sin_err
gsl_sf_cos_err_e          N    gsl_sf_cos_err
gsl_sf_angle_restrict_symm_eN  gsl_sf_angle_restrict_symm
gsl_sf_angle_restrict_pos_eN   gsl_sf_angle_restrict_pos
gsl_sf_angle_restrict_symm_err_eNgsl_sf_angle_restrict_symm_err
gsl_sf_angle_restrict_pos_err_eNgsl_sf_angle_restrict_pos_err
gsl_sf_zeta_int_e         Y    gsl_sf_zeta_int(int_expr)
gsl_sf_zeta_e             Y    gsl_sf_zeta(dbl_expr)
gsl_sf_zetam1_e           Y    gsl_sf_zetam1(dbl_expr)
gsl_sf_zetam1_int_e       Y    gsl_sf_zetam1_int(int_expr)
gsl_sf_hzeta_e            Y    gsl_sf_hzeta(dbl_expr,dbl_expr)
gsl_sf_eta_int_e          Y    gsl_sf_eta_int(int_expr)
gsl_sf_eta_e              Y    gsl_sf_eta(dbl_expr)

   ---------- Footnotes ----------

   (1) These are the GSL standard function names postfixed with '_e'.
NCO calls these functions automatically, without the NCO command having
to specifically indicate the '_e' function suffix.


File: nco.info,  Node: GSL interpolation,  Next: GSL least-squares fitting,  Prev: GSL special functions,  Up: ncap2 netCDF Arithmetic Processor

4.1.23 GSL interpolation
------------------------

As of version 3.9.9 (released July, 2009), NCO has wrappers to the GSL
interpolation functions.

Given a set of data points (x1,y1)...(xn, yn) the GSL functions computes
a continuous interpolating function Y(x) such that Y(xi) = yi.  The
interpolation is piecewise smooth, and its behavior at the end-points is
determined by the type of interpolation used.  For more information
consult the GSL manual.

Interpolation with 'ncap2' is a two stage process.  In the first stage,
a RAM variable is created from the chosen interpolating function and the
data set.  This RAM variable holds in memory a GSL interpolation object.
In the second stage, points along the interpolating function are
calculated.  If you have a very large data set or are interpolating many
sets then consider deleting the RAM variable when it is redundant.  Use
the command 'ram_delete(var_nm)'.

A simple example

     x_in[$lon]={1.0,2.0,3.0,4.0};
     y_in[$lon]={1.1,1.2,1.5,1.8};
     
     // Ram variable is declared and defined here 
     gsl_interp_cspline(&ram_sp,x_in,y_in);
     
     x_out[$lon_grd]={1.1,2.0,3.0,3.1,3.99};
     
     y_out=gsl_spline_eval(ram_sp,x_out);
     y2=gsl_spline_eval(ram_sp,1.3);
     y3=gsl_spline_eval(ram_sp,0.0);
     ram_delete(ram_sp);
     
     print(y_out); // 1.10472, 1.2, 1.4, 1.42658, 1.69680002 
     print(y2);    // 1.12454 
     print(y3);    // '_' 

Note in the above example y3 is set to 'missing value' because 0.0 isn't
within the input X range.

   *GSL Interpolation Types*
All the interpolation functions have been implemented.  These are:
gsl_interp_linear()
gsl_interp_polynomial()
gsl_interp_cspline()
gsl_interp_cspline_periodic()
gsl_interp_akima()
gsl_interp_akima_periodic()



   * Evaluation of Interpolating Types *
*Implemented*
gsl_spline_eval()
*Unimplemented*
gsl_spline_deriv()
gsl_spline_deriv2()
gsl_spline_integ()


File: nco.info,  Node: GSL least-squares fitting,  Next: GSL statistics,  Prev: GSL interpolation,  Up: ncap2 netCDF Arithmetic Processor

4.1.24 GSL least-squares fitting
--------------------------------

Least Squares fitting is a method of calculating a straight line through
a set of experimental data points in the XY plane.  The data maybe
weighted or unweighted.  For more information please refer to the GSL
manual.

These GSL functions fall into three categories:
*A)* Fitting data to Y=c0+c1*X
*B)* Fitting data (through the origin) Y=c1*X
*C)* Multi-parameter fitting (not yet implemented)

   *Section A*

'status=*gsl_fit_linear*
(data_x,stride_x,data_y,stride_y,n,&co,&c1,&cov00,&cov01,&cov11,&sumsq)
'

*Input variables*: data_x, stride_x, data_y, stride_y, n
From the above variables an X and Y vector both of length 'n' are
derived.  If data_x or data_y is less than type double then it is
converted to type 'double'.  It is up to you to do bounds checking on
the input data.  For example if stride_x=3 and n=8 then the size of
data_x must be at least 24

*Output variables*: c0, c1, cov00, cov01, cov11,sumsq
The '&' prefix indicates that these are call-by-reference variables.  If
any of the output variables don't exist prior to the call then they are
created on the fly as scalar variables of type 'double'.  If they
already exist then their existing value is overwritten.  If the function
call is successful then 'status=0'.

   'status=
*gsl_fit_wlinear*(data_x,stride_x,data_w,stride_w,data_y,stride_y,n,&co,&c1,&cov00,&cov01,&cov11,&chisq)
'

Similar to the above call except it creates an additional weighting
vector from the variables data_w, stride_w, n

   ' data_y_out=*gsl_fit_linear_est*(data_x,c0,c1,cov00,cov01,cov11) '

This function calculates y values along the line Y=c0+c1*X


   *Section B*

'status=*gsl_fit_mul*(data_x,stride_x,data_y,stride_y,n,&c1,&cov11,&sumsq)
'

*Input variables*: data_x, stride_x, data_y, stride_y, n
From the above variables an X and Y vector both of length 'n' are
derived.  If data_x or data_y is less than type 'double' then it is
converted to type 'double'.

*Output variables*: c1,cov11,sumsq

   'status=
*gsl_fit_wmul*(data_x,stride_x,data_w,stride_w,data_y,stride_y,n,&c1,&cov11,&sumsq)
'

Similar to the above call except it creates an additional weighting
vector from the variables data_w, stride_w, n

   ' data_y_out=*gsl_fit_mul_est*(data_x,c0,c1,cov11) '

This function calculates y values along the line Y=c1*X


The below example shows *gsl_fit_linear()* in action

     defdim("d1",10);
     xin[d1]={1,2,3,4,5,6,7,8,9,10.0};
     yin[d1]={3.1,6.2,9.1,12.2,15.1,18.2,21.3,24.0,27.0,30.0};
     gsl_fit_linear(xin,1,yin,1,$d1.size,&c0,&c1,&cov00,&cov01,&cov11,&sumsq);
     print(c0);  // 0.2
     print(c1);  // 2.98545454545
     
     defdim("e1",4);
     xout[e1]={1.0,3.0,4.0,11};
     yout[e1]=0.0;
     
     yout=gsl_fit_linear_est(xout, c0,c1, cov00,cov01, cov11, sumsq);
     
     print(yout);  // 3.18545454545 ,9.15636363636, ,12.1418181818 ,33.04



The following code does linear regression of sst(time,lat,lon) for each
time-step

     // Declare variables
     c0[$lat, $lon]=0.; // Intercept
     c1[$lat, $lon]=0.; // Slope
     sdv[$lat, $lon]=0.; // Standard deviation
     covxy[$lat, $lon]=0.; // Covariance
     for (i=0;i<$lat.size;i++) // Loop over lat
     {
       for (j=0;j<$lon.size;j++) // Loop over lon
       {
           // Linear regression function
           gsl_fit_linear(time,1,sst(:, i, j),1, $time.size, &tc0, &tc1, &cov00, &cov01,&cov11,&sumsq); 
           c0(i,j) = tc0; // Output results
           c1(i,j) = tc1; // Output results
           // Covariance function
           covxy(i,j) = gsl_stats_covariance(time,1,$time.size,double(sst(:,i,j)),1,$time.size); 
           // Standard deviation function
           sdv(i,j) = gsl_stats_sd(sst(:,i,j), 1, $time.size); 
       }
      }
     // slope (c1) missing values are set to '0', change to -999. (variable c0 intercept value)
     where( c0 == -999 ) c1 = -999;




File: nco.info,  Node: GSL statistics,  Next: GSL random number generation,  Prev: GSL least-squares fitting,  Up: ncap2 netCDF Arithmetic Processor

4.1.25 GSL statistics
---------------------

Wrappers for most of the GSL Statistical functions have been
implemented.  The GSL function names include a type specifier (except
for type double functions).  To obtain the equivalent NCO name simply
remove the type specifier; then depending on the data type the
appropriate GSL function is called.  The weighed statistical functions
e.g., ' gsl_stats_wvariance()' are only defined in GSL for
floating-point types; so your data must of type 'float' or 'double'
otherwise ncap2 will emit an error message.  To view the implemented
functions use the shell command 'ncap2 -f|grep _stats'

GSL Functions
     short gsl_stats_max (short data[], size_t stride, size_t n);
     double gsl_stats_int_mean (int data[], size_t stride, size_t n);
     double gsl_stats_short_sd_with_fixed_mean (short data[], size_t stride, size_t n, double mean);
     double gsl_stats_wmean (double w[], size_t wstride, double data[], size_t stride, size_t n);
     double gsl_stats_quantile_from_sorted_data (double sorted_data[], size_t stride, size_t n, double f) ;

Equivalent ncap2 wrapper functions
     short gsl_stats_max (var_data, data_stride, n);
     double gsl_stats_mean (var_data, data_stride, n);
     double gsl_stats_sd_with_fixed_mean (var_data, data_stride, n, var_mean);
     double gsl_stats_wmean (var_weight, weight_stride, var_data, data_stride, n, var_mean);
     double gsl_stats_quantile_from_sorted_data (var_sorted_data, data_stride, n, var_f) ;

GSL has no notion of missing values or dimensionality beyond one.  If
your data has missing values which you want ignored in the calculations
then use the 'ncap2' built in aggregate functions(*note Methods and
functions::).  The GSL functions operate on a vector of values created
from the var_data/stride/n arguments.  The ncap wrappers check that
there is no bounding error with regard to the size of the data and the
final value in the vector.
     a1[time]={1,2,3,4,5,6,7,8,9,10};
     
     a1_avg=gsl_stats_mean(a1,1,10);
     print(a1_avg); // 5.5
     
     a1_var=gsl_stats_variance(a1,4,3);
     print(a1_var); // 16.0
     
     // bounding error, vector attempts to access element a1(10)
     a1_sd=gsl_stats_sd(a1,5,3); 

For functions with the signature *func_nm(var_data,data_stride,n)*, one
may omit the second or third arguments.  The default value for STRIDE is
'1'.  The default value for N is '1+(data.size()-1)/stride'.

     // Following statements are equvalent
     n2=gsl_stats_max(a1,1,10)
     n2=gsl_stats_max(a1,1);
     n2=gsl_stats_max(a1);

     // Following statements are equvalent
     n3=gsl_stats_median_from_sorted_data(a1,2,5);
     n3=gsl_stats_median_from_sorted_data(a1,2);

     // Following statements are NOT equvalent
     n4=gsl_stats_kurtosis(a1,3,2);
     n4=gsl_stats_kurtosis(a1,3); //default n=4

   The following example illustrates some of the weighted functions.
The data are randomly generated.  In this case the value of the weight
for each datum is either 0.0 or 1.0
     defdim("r1",2000);
     data[r1]=1.0;

     // Fill with random numbers [0.0,10.0)
     data=10.0*gsl_rng_uniform(data);

     // Create a weighting variable
     weight=(data>4.0);

     wmean=gsl_stats_wmean(weight,1,data,1,$r1.size);
     print(wmean);

     wsd=gsl_stats_wsd(weight,1,data,1,$r1.size);
     print(wsd);

     // number of values in data that are greater than 4
     weight_size=weight.total();
     print(weight_size);

     // print min/max of data
     dmin=data.gsl_stats_min();
     dmax=data.gsl_stats_max();
     print(dmin);print(dmax);


File: nco.info,  Node: GSL random number generation,  Next: Examples ncap2,  Prev: GSL statistics,  Up: ncap2 netCDF Arithmetic Processor

4.1.26 GSL random number generation
-----------------------------------

The GSL library has a large number of random number generators.  In
addition there are a large set of functions for turning uniform random
numbers into discrete or continuous probabilty distributions.  The
random number generator algorithms vary in terms of quality numbers
output, speed of execution and maximum number output.  For more
information see the GSL documentation.  The algorithm and seed are set
via environment variables, these are picked up by the 'ncap2' code.

*Setup*
The number algorithm is set by the environment variable 'GSL_RNG_TYPE'.
If this variable isn't set then the default rng algorithm is
gsl_rng_19937.  The seed is set with the environment variable
'GSL_RNG_SEED'.  The following wrapper functions in ncap2 provide
information about the chosen algorithm.

'gsl_rng_min()'
     the minimum value returned by the rng algorithm.
'gsl_rng_max()'
     the maximum value returned by the rng algorithm.

*Uniformly Distributed Random Numbers*
'gsl_rng_get(var_in)'
     This function returns var_in with integers from the chosen rng
     algorithm.  The min and max values depend uoon the chosen rng
     algorthm.
'gsl_rng_uniform_int(var_in)'
     This function returns var_in with random integers from 0 to n-1.
     The value n must be less than or equal to the maximum value of the
     chosen rng algorithm.
'gsl_rng_uniform(var_in)'
     This function returns var_in with double-precision numbers in the
     range [0.0,1).  The range includes 0.0 and excludes 1.0.
'gsl_rng_uniform_pos(var_in)'
     This function returns var_in with double-precision numbers in the
     range (0.0,1), excluding both 0.0 and 1.0.

Below are examples of 'gsl_rng_get()' and 'gsl_rng_uniform_int()' in
action.

     export GSL_RNG_TYPE=ranlux
     export GSL_RNG_SEED=10
     ncap2 -v -O -s 'a1[time]=0;a2=gsl_rng_get(a1);' in.nc foo.nc
     // 10 random numbers from the range 0 - 16777215
     // a2=9056646, 12776696, 1011656, 13354708, 5139066, 1388751, 11163902, 7730127, 15531355, 10387694 ;

     ncap2 -v -O -s 'a1[time]=21;a2=gsl_rng_uniform_int(a1).sort();' in.nc foo.nc
     // 10 random numbers from the range 0 - 20
     a2 = 1, 1, 6, 9, 11, 13, 13, 15, 16, 19 ;


The following example produces an 'ncap2' runtime error.  This is
because the chose rng algorithm has a maximum value greater than '
NC_MAX_INT=2147483647 '; the wrapper functions to 'gsl_rng_get()' and
'gsl_rng_uniform_int()' return variable of type 'NC_INT'.  Please be
aware of this when using random number distribution functions functions
from the GSL library which return 'unsigned int'.  Examples of these are
'gsl_ran_geometric()' and 'gsl_ran_pascal()'.

     export GSL_RNG_TYPE=mt19937
     ncap2 -v -O -s 'a1[time]=0;a2=gsl_rng_get(a1);' in.nc foo.nc

To find the maximum value of the chosen rng algorithm use the following
code snippet.
     ncap2 -v -O -s 'rng_max=gsl_rng_max();print(rng_max)' in.nc foo.nc

*Random Number Distributions*
The GSL library has a rich set of random number disribution functions.
The library also provides cumulative distribution functions and inverse
cumulative distribution functions sometimes referred to a quantile
functions.  To see whats available on your build use the shell command
'ncap2 -f|grep -e _ran -e _cdf'.

The following examples all return variables of type 'NC_INT'
     defdim("out",15);
     a1[$out]=0.5;
     a2=gsl_ran_binomial(a1,30).sort();
     //a2 = 10, 11, 12, 12, 13, 14, 14, 15, 15, 16, 16, 16, 16, 17, 22 ;
     a3=gsl_ran_geometric(a2).sort();
     //a2 = 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 4, 5 ;
     a4=gsl_ran_pascal(a2,50);
     //a5 = 37, 40, 40, 42, 43, 45, 46, 49, 52, 58, 60, 62, 62, 65, 67 ;

The following all return variables of type 'NC_DOUBLE';
     defdim("b1",1000);
     b1[$b1]=0.8;
     b2=gsl_ran_exponential(b1);
     b2_avg=b2.avg();
     print(b2_avg);
     // b2_avg = 0.756047976787

     b3=gsl_ran_gaussian(b1);
     b3_avg=b3.avg();
     b3_rms=b3.rms();
     print(b3_avg);
     // b3_avg = -0.00903446534258;
     print(b3_rms);
     // b3_rms = 0.81162979889;

     b4[$b1]=10.0;
     b5[$b1]=20.0;
     b6=gsl_ran_flat(b4,b5);
     b6_avg=b6.avg();
     print(b6_avg);
     // b6_avg=15.0588129413


File: nco.info,  Node: Examples ncap2,  Next: Intrinsic mathematical methods,  Prev: GSL random number generation,  Up: ncap2 netCDF Arithmetic Processor

4.1.27 Examples ncap2
---------------------

See the 'ncap.in' and 'ncap2.in' scripts released with NCO for more
complete demonstrations of 'ncap2' functionality (script available
on-line at <http://nco.sf.net/ncap2.in>).

   Define new attribute NEW for existing variable ONE as twice the
existing attribute DOUBLE_ATT of variable ATT_VAR:
     ncap2 -s 'one@new=2*att_var@double_att' in.nc out.nc

   Average variables of mixed types (result is of type 'double'):
     ncap2 -s 'average=(var_float+var_double+var_int)/3' in.nc out.nc

   Multiple commands may be given to 'ncap2' in three ways.  First, the
commands may be placed in a script which is executed, e.g., 'tst.nco'.
Second, the commands may be individually specified with multiple '-s'
arguments to the same 'ncap2' invocation.  Third, the commands may be
chained into a single '-s' argument to 'ncap2'.  Assuming the file
'tst.nco' contains the commands 'a=3;b=4;c=sqrt(a^2+b^2);', then the
following 'ncap2' invocations produce identical results:
     ncap2 -v -S tst.nco in.nc out.nc
     ncap2 -v -s 'a=3' -s 'b=4' -s 'c=sqrt(a^2+b^2)' in.nc out.nc
     ncap2 -v -s 'a=3;b=4;c=sqrt(a^2+b^2)' in.nc out.nc
   The second and third examples show that 'ncap2' does not require that
a trailing semi-colon ';' be placed at the end of a '-s' argument,
although a trailing semi-colon ';' is always allowed.  However,
semi-colons are required to separate individual assignment statements
chained together as a single '-s' argument.

   'ncap2' may be used to "grow" dimensions, i.e., to increase dimension
sizes without altering existing data.  Say 'in.nc' has 'ORO(lat,lon)'
and the user wishes a new file with 'new_ORO(new_lat,new_lon)' that
contains zeros in the undefined portions of the new grid.
     defdim("new_lat",$lat.size+1); // Define new dimension sizes
     defdim("new_lon",$lon.size+1);
     new_ORO[$new_lat,$new_lon]=0.0f; // Initialize to zero
     new_ORO(0:$lat.size-1,0:$lon.size-1)=ORO; // Fill valid data
   The commands to define new coordinate variables 'new_lat' and
'new_lon' in the output file follow a similar pattern.  One would might
store these commands in a script 'grow.nco' and then execute the script
with
     ncap2 -v -S grow.nco in.nc out.nc

   Imagine you wish to create a binary flag based on the value of an
array.  The flag should have value 1.0 where the array exceeds 1.0, and
value 0.0 elsewhere.  This example creates the binary flag 'ORO_flg' in
'out.nc' from the continuous array named 'ORO' in 'in.nc'.
     ncap2 -s 'ORO_flg=(ORO > 1.0)' in.nc out.nc
   Suppose your task is to change all values of 'ORO' which equal 2.0 to
the new value 3.0:
     ncap2 -s 'ORO_msk=(ORO==2.0);ORO=ORO_msk*3.0+!ORO_msk*ORO' in.nc out.nc
   This creates and uses 'ORO_msk' to mask the subsequent arithmetic
operation.  Values of 'ORO' are only changed where 'ORO_msk' is true,
i.e., where 'ORO' equals 2.0
Using the 'where' statement the above code simplifies to :
     ncap2 -s 'where(ORO == 2.0) ORO=3.0;' in.nc foo.nc

   This example uses 'ncap2' to compute the covariance of two variables.
Let the variables U and V be the horizontal wind components.  The
"covariance" of U and V is defined as the time mean product of the
deviations of U and V from their respective time means.  Symbolically,
the covariance [U'V'] = [UV]-[U][V] where [X] denotes the time-average
of X and X' denotes the deviation from the time-mean.  The covariance
tells us how much of the correlation of two signals arises from the
signal fluctuations versus the mean signals.  Sometimes this is called
the "eddy covariance".  We will store the covariance in the variable
'uprmvprm'.
     ncwa -O -a time -v u,v in.nc foo.nc # Compute time mean of u,v
     ncrename -O -v u,uavg -v v,vavg foo.nc # Rename to avoid conflict
     ncks -A -v uavg,vavg foo.nc in.nc # Place time means with originals
     ncap2 -O -s 'uprmvprm=u*v-uavg*vavg' in.nc in.nc # Covariance
     ncra -O -v uprmvprm in.nc foo.nc # Time-mean covariance
   The mathematically inclined will note that the same covariance would
be obtained by replacing the step involving 'ncap2' with
     ncap2 -O -s 'uprmvprm=(u-uavg)*(v-vavg)' foo.nc foo.nc # Covariance

   As of NCO version 3.1.8 (December, 2006), 'ncap2' can compute
averages, and thus covariances, by itself:
     ncap2 -s 'uavg=u.avg($time);vavg=v.avg($time);uprmvprm=u*v-uavg*vavg' \
           -s 'uprmvrpmavg=uprmvprm.avg($time)' in.nc foo.nc
   We have not seen a simpler method to script and execute powerful
arithmetic than 'ncap2'.

   'ncap2' utilizes many meta-characters (e.g., '$', '?', ';', '()',
'[]') that can confuse the command-line shell if not quoted properly.
The issues are the same as those which arise in utilizing extended
regular expressions to subset variables (*note Subsetting Files::).  The
example above will fail with no quotes and with double quotes.  This is
because shell globbing tries to "interpolate" the value of '$time' from
the shell environment unless it is quoted:
     ncap2 -s 'uavg=u.avg($time)'  in.nc foo.nc # Correct (recommended)
     ncap2 -s  uavg=u.avg('$time') in.nc foo.nc # Correct (and dangerous)
     ncap2 -s  uavg=u.avg($time)   in.nc foo.nc # Wrong ($time = '')
     ncap2 -s "uavg=u.avg($time)"  in.nc foo.nc # Wrong ($time = '')
   Without the single quotes, the shell replaces '$time' with an empty
string.  The command 'ncap2' receives from the shell is 'uavg=u.avg()'.
This causes 'ncap2' to average over all dimensions rather than just the
TIME dimension, and unintended consequence.

   We recommend using single quotes to protect 'ncap2' command-line
scripts from the shell, even when such protection is not strictly
necessary.  Expert users may violate this rule to exploit the ability to
use shell variables in 'ncap2' command-line scripts (*note CCSM
Example::).  In such cases it may be necessary to use the shell
backslash character '\' to protect the 'ncap2' meta-character.

   A dimension of size one is said to be _degenerate_.  Whether a
degenerate record dimension is desirable or not depends on the
application.  Often a degenerate TIME dimension is useful, e.g., for
concatentating, but it may cause problems with arithmetic.  Such is the
case in the above example, where the first step employs 'ncwa' rather
than 'ncra' for the time-averaging.  Of course the numerical results are
the same with both operators.  The difference is that, unless '-b' is
specified, 'ncwa' writes no TIME dimension to the output file, while
'ncra' defaults to keeping TIME as a degenerate (size 1) dimension.
Appending 'u' and 'v' to the output file would cause 'ncks' to try to
expand the degenerate time axis of 'uavg' and 'vavg' to the size of the
non-degenerate TIME dimension in the input file.  Thus the append ('ncks
-A') command would be undefined (and should fail) in this case.  Equally
important is the '-C' argument (*note Subsetting Coordinate Variables::)
to 'ncwa' to prevent any scalar TIME variable from being written to the
output file.  Knowing when to use 'ncwa -a time' rather than the default
'ncra' for time-averaging takes, well, time.


File: nco.info,  Node: Intrinsic mathematical methods,  Next: Operator precedence and associativity,  Prev: Examples ncap2,  Up: ncap2 netCDF Arithmetic Processor

4.1.28 Intrinsic mathematical methods
-------------------------------------

'ncap2' supports the standard mathematical functions supplied with most
operating systems.  Standard calculator notation is used for addition
'+', subtraction '-', multiplication '*', division '/', exponentiation
'^', and modulus '%'.  The available elementary mathematical functions
are:
'abs(x)'
     "Absolute value" Absolute value of X.  Example: abs(-1) = 1
'acos(x)'
     "Arc-cosine" Arc-cosine of X where X is specified in radians.
     Example: acos(1.0) = 0.0
'acosh(x)'
     "Hyperbolic arc-cosine" Hyperbolic arc-cosine of X where X is
     specified in radians.  Example: acosh(1.0) = 0.0
'asin(x)'
     "Arc-sine" Arc-sine of X where X is specified in radians.  Example:
     asin(1.0) = 1.57079632679489661922
'asinh(x)'
     "Hyperbolic arc-sine" Hyperbolic arc-sine of X where X is specified
     in radians.  Example: asinh(1.0) = 0.88137358702
'atan(x)'
     "Arc-tangent" Arc-tangent of X where X is specified in radians
     between -pi/2 and pi/2.  Example: atan(1.0) =
     0.78539816339744830961

'atan2(y,x)'
     "Arc-tangent2" Arc-tangent of Y/X :Example atan2(1,3) = 0.321689857

'atanh(x)'
     "Hyperbolic arc-tangent" Hyperbolic arc-tangent of X where X is
     specified in radians between -pi/2 and pi/2.  Example:
     atanh(3.14159265358979323844) = 1.0
'ceil(x)'
     "Ceil" Ceiling of X.  Smallest integral value not less than
     argument.  Example: ceil(0.1) = 1.0
'cos(x)'
     "Cosine" Cosine of X where X is specified in radians.  Example:
     cos(0.0) = 1.0
'cosh(x)'
     "Hyperbolic cosine" Hyperbolic cosine of X where X is specified in
     radians.  Example: cosh(0.0) = 1.0
'erf(x)'
     "Error function" Error function of X where X is specified between
     -1 and 1.  Example: erf(1.0) = 0.842701
'erfc(x)'
     "Complementary error function" Complementary error function of X
     where X is specified between -1 and 1.  Example: erfc(1.0) =
     0.15729920705
'exp(x)'
     "Exponential" Exponential of X, e^x.  Example: exp(1.0) =
     2.71828182845904523536
'floor(x)'
     "Floor" Floor of X.  Largest integral value not greater than
     argument.  Example: floor(1.9) = 1
'gamma(x)'
     "Gamma function" Gamma function of X, Gamma(x).  The well-known and
     loved continuous factorial function.  Example: gamma(0.5) =
     sqrt(pi)
'gamma_inc_P(x)'
     "Incomplete Gamma function" Incomplete Gamma function of parameter
     A and variable X, gamma_inc_P(a,x).  One of the four incomplete
     gamma functions.  Example: gamma_inc_P(1,1) = 1-1/e
'ln(x)'
     "Natural Logarithm" Natural logarithm of X, ln(x).  Example:
     ln(2.71828182845904523536) = 1.0
'log(x)'
     "Natural Logarithm" Exact synonym for 'ln(x)'.
'log10(x)'
     "Base 10 Logarithm" Base 10 logarithm of X, log10(x).  Example:
     log(10.0) = 1.0
'nearbyint(x)'
     "Round inexactly" Nearest integer to X is returned in
     floating-point format.  No exceptions are raised for "inexact
     conversions".  Example: nearbyint(0.1) = 0.0
'pow(x,y)'
     "Power" Value of X is raised to the power of Y.  Exceptions are
     raised for "domain errors".  Due to type-limitations in the
     C language 'pow' function, integer arguments are promoted (*note
     Type Conversion::) to type 'NC_FLOAT' before evaluation.  Example:
     pow(2,3) = 8
'rint(x)'
     "Round exactly" Nearest integer to X is returned in floating-point
     format.  Exceptions are raised for "inexact conversions".  Example:
     rint(0.1) = 0
'round(x)'
     "Round" Nearest integer to X is returned in floating-point format.
     Round halfway cases away from zero, regardless of current IEEE
     rounding direction.  Example: round(0.5) = 1.0
'sin(x)'
     "Sine" Sine of X where X is specified in radians.  Example:
     sin(1.57079632679489661922) = 1.0
'sinh(x)'
     "Hyperbolic sine" Hyperbolic sine of X where X is specified in
     radians.  Example: sinh(1.0) = 1.1752
'sqrt(x)'
     "Square Root" Square Root of X, sqrt(x).  Example: sqrt(4.0) = 2.0
'tan(x)'
     "Tangent" Tangent of X where X is specified in radians.  Example:
     tan(0.78539816339744830961) = 1.0
'tanh(x)'
     "Hyperbolic tangent" Hyperbolic tangent of X where X is specified
     in radians.  Example: tanh(1.0) = 0.761594155956
'trunc(x)'
     "Truncate" Nearest integer to X is returned in floating-point
     format.  Round halfway cases toward zero, regardless of current
     IEEE rounding direction.  Example: trunc(0.5) = 0.0
The complete list of mathematical functions supported is
platform-specific.  Functions mandated by ANSI C are _guaranteed_ to be
present and are indicated with an asterisk (1).  and are indicated with
an asterisk.  Use the '-f' (or 'fnc_tbl' or 'prn_fnc_tbl') switch to
print a complete list of functions supported on your platform.  (2)


   ---------- Footnotes ----------

   (1) ANSI C compilers are guaranteed to support double-precision
versions of these functions.  These functions normally operate on netCDF
variables of type 'NC_DOUBLE' without having to perform intrinsic
conversions.  For example, ANSI compilers provide 'sin' for the sine of
C-type 'double' variables.  The ANSI standard does not require, but many
compilers provide, an extended set of mathematical functions that apply
to single ('float') and quadruple ('long double') precision variables.
Using these functions (e.g., 'sinf' for 'float', 'sinl' for 'long
double'), when available, is (presumably) more efficient than casting
variables to type 'double', performing the operation, and then
re-casting.  NCO uses the faster intrinsic functions when they are
available, and uses the casting method when they are not.

   (2) Linux supports more of these intrinsic functions than other OSs.


File: nco.info,  Node: Operator precedence and associativity,  Next: ID Quoting,  Prev: Intrinsic mathematical methods,  Up: ncap2 netCDF Arithmetic Processor

4.1.29 Operator precedence and associativity
--------------------------------------------

This page lists the 'ncap2' operators in order of precedence (highest to
lowest).  Their associativity indicates in what order operators of equal
precedence in an expression are applied.

Operator      Description                                   Associativity
---------------------------------------------------------------------------
'++ --'       Postfix Increment/Decrement                   Right to
                                                            Left
'()'          Parentheses (function call)
'.'           Method call
'++ --'       Prefix Increment/Decrement                    Right to
                                                            Left
'+ -'         Unary Plus/Minus
'!'           Logical Not
'^'           Power of Operator                             Right to
                                                            Left
'* / %'       Multiply/Divide/Modulus                       Left To
                                                            Right
'+ -'         Addition/Subtraction                          Left To
                                                            Right
'>> <<'       Fortran style array clipping                  Left to
                                                            Right
'< <='        Less than/Less than or equal to               Left to
                                                            Right
'> >='        Greater than/Greater than or equal to
'== !='       Equal to/Not equal to                         Left to
                                                            Right
'&&'          Logical AND                                   Left to
                                                            Right
'||'          Logical OR                                    Left to
                                                            Right
'?:'          Ternary Operator                              Right to
                                                            Left
'='           Assignment                                    Right to
                                                            Left
'+= -='       Addition/subtraction assignment
'*= /='       Multiplication/division assignment


File: nco.info,  Node: ID Quoting,  Next: make_bounds() function,  Prev: Operator precedence and associativity,  Up: ncap2 netCDF Arithmetic Processor

4.1.30 ID Quoting
-----------------

In this section a name refers to a variable, attribute, or dimension
name.  The allowed characters in a valid netCDF name vary from release
to release.  (See end section).  To use metacharacters in a name, or to
use a method name as a variable name, the name must be quoted wherever
it occurs.

The default NCO name is specified by the regular expressions:

     DGT:     ('0'..'9');
     LPH:     ( 'a'..'z' | 'A'..'Z' | '_' );
     name:    (LPH)(LPH|DGT)+

The first character of a valid name must be alphabetic or the
underscore.  Any subsequent characters must be alphanumeric or
underscore.  ( e.g., a1,_23, hell_is_666 )

The valid characters in a quoted name are specified by the regular
expressions:
     LPHDGT:  ( 'a'..'z' | 'A'..'Z' | '_' | '0'..'9');
     name:    (LPHDGT|'-'|'+'|'.'|'('|')'|':' )+  ;

Quote a variable:
'avg' , '10_+10','set_miss' '+-90field' , '-test'=10.0d

Quote a attribute:
'three@10', 'set_mss@+10', '666@hell', 't1@+units'="kelvin"

Quote a dimension:
'$10', '$t1-', '$-odd', c1['$10','$t1-']=23.0d


   The following comments are from the netCDF library definitions and
detail the naming conventions for each release.  netcdf-3.5.1
netcdf-3.6.0-p1
netcdf-3.6.1
netcdf-3.6.2
     /*
      * ( [a-zA-Z]|[0-9]|'_'|'-'|'+'|'.'|'|':'|'@'|'('|')' )+
      * Verify that name string is valid CDL syntax, i.e., all characters are
      * alphanumeric, '-', '_', '+', or '.'.
      * Also permit ':', '@', '(', or ')' in names for chemists currently making 
      * use of these characters, but don't document until ncgen and ncdump can 
      * also handle these characters in names.
      */

netcdf-3.6.3
netcdf-4.0 Final 2008/08/28
     /*
      * Verify that a name string is valid syntax.  The allowed name
      * syntax (in RE form) is:
      *
      * ([a-zA-Z_]|{UTF8})([^\x00-\x1F\x7F/]|{UTF8})*
      *
      * where UTF8 represents a multibyte UTF-8 encoding.  Also, no
      * trailing spaces are permitted in names.  This definition
      * must be consistent with the one in ncgen.l.  We do not allow '/'
      * because HDF5 does not permit slashes in names as slash is used as a
      * group separator.  If UTF-8 is supported, then a multi-byte UTF-8
      * character can occur anywhere within an identifier.  We later
      * normalize UTF-8 strings to NFC to facilitate matching and queries.
      */ 


File: nco.info,  Node: make_bounds() function,  Next: solar_zenith_angle function,  Prev: ID Quoting,  Up: ncap2 netCDF Arithmetic Processor

4.1.31 make_bounds() function
-----------------------------

The 'ncap2' custom function 'make_bounds()' takes any monotonic 1D
coordinate variable with regular or irregular (e.g., Gaussian) spacing
and creates a bounds variable.

   _<bounds_var_out>=make_bounds( <coordinate_var_in>, <dim in>,
<string>)_

1ST ARGUMENT
     The name of the input coordinate variable.
2ND ARGUMENT
     The dimension name of the second dimension of the output variable.
     The size of this dimension should always be 2.  If the dimension
     does not yet exist create it first using 'defdim()'.
3RD ARGUMENT
     The string value of a "bounds" attribute that is created in the
     input coordinate variable.  This must be the variable name to
     contain the bounds.
   Typical usage:
     defdim("nv",2);
     longitude_bounds=make_bounds(longitude,$nv,"longitude_bounds");

   Another common CF convention:
     defdim("nv",2);
     climatology_bounds=make_bounds(time,$nv,"climatology_bounds");


File: nco.info,  Node: solar_zenith_angle function,  Prev: make_bounds() function,  Up: ncap2 netCDF Arithmetic Processor

4.1.32 solar_zenith_angle function
----------------------------------

_<zenith_out>=solar_zenith_angle( <time_in>, <latitude in>)_

   This function takes two arguments, mean local solar time and
latitude.  Calculation and output is done with type 'NC_DOUBLE'.  The
calendar attribute for <time_in> in is NOT read and is assumed to be
Gregorian (this is the calendar that UDUnits uses).  As part of the
calculation <time_in> is converted to days since start of year.  For
some input units e.g., seconds, this function may produce gobbledygook.
The output <zenith_out> is in 'degrees'.  For more details of the
algorithm used please examine the function 'solar_geometry()' in
'fmc_all_cls.cc'.  Note that this routine does not account for the
equation of time, and so can be in error by the angular equivalent of up
to about fifteen minutes time depending on the day of year.

     my_time[time]={10.50, 11.0, 11.50, 12.0, 12.5, 13.0, 13.5, 14.0, 14.50, 15.00};
     my_time@units="hours since 2017-06-21";

     // Assume we are at Equator
     latitude=0.0;

     // 32.05428, 27.61159, 24.55934, 23.45467, 24.55947, 27.61184, 32.05458, 37.39353, 43.29914, 49.55782 ;
     zenith=solar_zenith_angle(my_time,latitude);


File: nco.info,  Node: ncatted netCDF Attribute Editor,  Next: ncbo netCDF Binary Operator,  Prev: ncap2 netCDF Arithmetic Processor,  Up: Reference Manual

4.2 'ncatted' netCDF Attribute Editor
=====================================

SYNTAX
     ncatted [-a ATT_DSC] [-a ...] [-D DBG]
     [-h] [--hdr_pad NBR] [--hpss]
     [-l PATH] [-O] [-o OUTPUT-FILE] [-p PATH]
     [-R] [-r] [--ram_all] [-t] INPUT-FILE [[OUTPUT-FILE]]

DESCRIPTION

   'ncatted' edits attributes in a netCDF file.  If you are editing
attributes then you are spending too much time in the world of metadata,
and 'ncatted' was written to get you back out as quickly and painlessly
as possible.  'ncatted' can "append", "create", "delete", "modify", and
"overwrite" attributes (all explained below).  'ncatted' allows each
editing operation to be applied to every variable in a file.  This saves
time when changing attribute conventions throughout a file.  'ncatted'
is for _writing_ attributes.  To _read_ attribute values in plain text,
use 'ncks -m -M', or define something like 'ncattget' as a shell command
(*note Filters for ncks::).

   Because repeated use of 'ncatted' can considerably increase the size
of the 'history' global attribute (*note History Attribute::), the '-h'
switch is provided to override automatically appending the command to
the 'history' global attribute in the OUTPUT-FILE.

   According to the 'netCDF User Guide', altering metadata in netCDF
files does not incur the penalty of recopying the entire file when the
new metadata occupies less space than the old metadata.  Thus 'ncatted'
may run much faster (at least on netCDF3 files) if judicious use of
header padding (*note Metadata Optimization::) was made when producing
the INPUT-FILE.  Similarly, using the '--hdr_pad' option with 'ncatted'
helps ensure that future metadata changes to OUTPUT-FILE occur as
swiftly as possible.

   When 'ncatted' is used to change the '_FillValue' attribute, it
changes the associated missing data self-consistently.  If the internal
floating-point representation of a missing value, e.g., 1.0e36, differs
between two machines then netCDF files produced on those machines will
have incompatible missing values.  This allows 'ncatted' to change the
missing values in files from different machines to a single value so
that the files may then be concatenated, e.g., by 'ncrcat', without
losing information.  *Note Missing Values::, for more information.

   To master 'ncatted' one must understand the meaning of the structure
that describes the attribute modification, ATT_DSC specified by the
required option '-a' or '--attribute'.  This option is repeatable and
may be used multiple time in a single 'ncatted' invocation to increase
the efficiency of altering multiple attributes.  Each ATT_DSC contains
five elements.  This makes using 'ncatted' somewhat complicated, though
powerful.  The ATT_DSC fields are in the following order:

   ATT_DSC = ATT_NM, VAR_NM, MODE, ATT_TYPE, ATT_VAL

ATT_NM
     Attribute name.  Example: 'units' As of NCO 4.5.1 (July, 2015),
     'ncatted' accepts regular expressions (*note Subsetting Files::)
     for attribute names (it has "always" accepted regular expressions
     for variable names).  Regular expressions will select all matching
     attribute names.
VAR_NM
     Variable name.  Example: 'pressure', ''^H2O''.  Regular expressions
     (*note Subsetting Files::) are accepted and will select all
     matching variable (and/or group) names.  The names 'global' and
     'group' have special meaning.
MODE
     Edit mode abbreviation.  Example: 'a'.  See below for complete
     listing of valid values of MODE.
ATT_TYPE
     Attribute type abbreviation.  Example: 'c'.  See below for complete
     listing of valid values of ATT_TYPE.
ATT_VAL
     Attribute value.  Example: 'pascal'.
There should be no empty space between these five consecutive arguments.
The description of these arguments follows in their order of appearance.

   The value of ATT_NM is the name of the attribute to edit.  The
meaning of this should be clear to all 'ncatted' users.  Both ATT_NM and
VAR_NM may be specified as regular expressions.  If ATT_NM is omitted
(i.e., left blank) and "Delete" mode is selected, then all attributes
associated with the specified variable will be deleted.

   The value of VAR_NM is the name of the variable containing the
attribute (named ATT_NM) that you want to edit.  There are three very
important and useful exceptions to this rule.  The value of VAR_NM can
also be used to direct 'ncatted' to edit global attributes, or to repeat
the editing operation for every group or variable in a file.  A value of
VAR_NM of 'global' indicates that ATT_NM refers to a global (i.e.,
root-level) attribute, rather than to a particular variable's attribute.
This is the method 'ncatted' supports for editing global attributes.
A value of VAR_NM of 'group' indicates that ATT_NM refers to all groups,
rather than to a particular variable's or group's attribute.  The
operation will proceed to edit group metadata for every group.  Finally,
if VAR_NM is left blank, then 'ncatted' attempts to perform the editing
operation on every variable in the file.  This option may be convenient
to use if you decide to change the conventions you use for describing
the data.  As of NCO 4.6.0 (May, 2016), 'ncatted' accepts the '-t' (or
long-option equivalent '--typ_mch' or '--type_match') option.  This
causes 'ncatted' to perform the editing operation only on variables that
are the same type as the specified attribute.

   The value of MODE is a single character abbreviation ('a', 'c', 'd',
'm', 'n', or 'o') standing for one of five editing modes:
'a'
     "Append".  Append value ATT_VAL to current VAR_NM attribute ATT_NM
     value ATT_VAL, if any.  If VAR_NM does not already have an existing
     attribute ATT_NM, it is created with the value ATT_VAL.
'c'
     "Create".  Create variable VAR_NM attribute ATT_NM with ATT_VAL if
     ATT_NM does not yet exist.  If VAR_NM already has an attribute
     ATT_NM, there is no effect, so the existing attribute is preserved
     without change.
'd'
     "Delete".  Delete current VAR_NM attribute ATT_NM.  If VAR_NM does
     not have an attribute ATT_NM, there is no effect.  If ATT_NM is
     omitted (left blank), then all attributes associated with the
     specified variable are automatically deleted.  When "Delete" mode
     is selected, the ATT_TYPE and ATT_VAL arguments are superfluous and
     may be left blank.
'm'
     "Modify".  Change value of current VAR_NM attribute ATT_NM to value
     ATT_VAL.  If VAR_NM does not have an attribute ATT_NM, there is no
     effect.
'n'
     "Nappend".  Append value ATT_VAL to VAR_NM attribute ATT_NM value
     ATT_VAL if ATT_NM already exists.  If VAR_NM does not have an
     attribute ATT_NM, there is no effect.  In other words, if ATT_NM
     already exist, Nappend behaves like Append otherwise it does
     nothing.  The mnenomic is "non-create append".  Nappend mode was
     added to 'ncatted' in version 4.6.0 (May, 2016).
'o'
     "Overwrite".  Write attribute ATT_NM with value ATT_VAL to variable
     VAR_NM, overwriting existing attribute ATT_NM, if any.  This is the
     default mode.

   The value of ATT_TYPE is a single character abbreviation ('f', 'd',
'l', 'i', 's', 'c', 'b', 'u') or a short string standing for one of the
twelve primitive netCDF data types:
'f'
     "Float".  Value(s) specified in ATT_VAL will be stored as netCDF
     intrinsic type 'NC_FLOAT'.
'd'
     "Double".  Value(s) specified in ATT_VAL will be stored as netCDF
     intrinsic type 'NC_DOUBLE'.
'i, l'
     "Integer" or (its now deprecated synonym) "Long".  Value(s)
     specified in ATT_VAL will be stored as netCDF intrinsic type
     'NC_INT'.
's'
     "Short".  Value(s) specified in ATT_VAL will be stored as netCDF
     intrinsic type 'NC_SHORT'.
'c'
     "Char".  Value(s) specified in ATT_VAL will be stored as netCDF
     intrinsic type 'NC_CHAR'.
'b'
     "Byte".  Value(s) specified in ATT_VAL will be stored as netCDF
     intrinsic type 'NC_BYTE'.
'ub'
     "Unsigned Byte".  Value(s) specified in ATT_VAL will be stored as
     netCDF intrinsic type 'NC_UBYTE'.
'us'
     "Unsigned Short".  Value(s) specified in ATT_VAL will be stored as
     netCDF intrinsic type 'NC_USHORT'.
'u, ui, ul'
     "Unsigned Int".  Value(s) specified in ATT_VAL will be stored as
     netCDF intrinsic type 'NC_UINT'.
'll, int64'
     "Int64".  Value(s) specified in ATT_VAL will be stored as netCDF
     intrinsic type 'NC_INT64'.
'ull, uint64'
     "Uint64".  Value(s) specified in ATT_VAL will be stored as netCDF
     intrinsic type 'NC_UINT64'.
'sng, string'
     "String".  Value(s) specified in ATT_VAL will be stored as netCDF
     intrinsic type 'NC_STRING'.  Note that 'ncatted' handles type
     'NC_STRING' attributes correctly beginning with version 4.3.3
     released in July, 2013.  Earlier versions fail when asked to handle
     'NC_STRING' attributes.
In "Delete" mode the specification of ATT_TYPE is optional (and is
ignored if supplied).

   The value of ATT_VAL is what you want to change attribute ATT_NM to
contain.  The specification of ATT_VAL is optional in "Delete" (and is
ignored) mode.  Attribute values for all types besides 'NC_CHAR' must
have an attribute length of at least one.  Thus ATT_VAL may be a single
value or one-dimensional array of elements of type 'att_type'.  If the
ATT_VAL is not set or is set to empty space, and the ATT_TYPE is
'NC_CHAR', e.g., '-a units,T,o,c,""' or '-a units,T,o,c,', then the
corresponding attribute is set to have zero length.  When specifying an
array of values, it is safest to enclose ATT_VAL in single or double
quotes, e.g., '-a levels,T,o,s,"1,2,3,4"' or '-a
levels,T,o,s,'1,2,3,4''.  The quotes are strictly unnecessary around
ATT_VAL except when ATT_VAL contains characters which would confuse the
calling shell, such as spaces, commas, and wildcard characters.

   NCO processing of 'NC_CHAR' attributes is a bit like Perl in that it
attempts to do what you want by default (but this sometimes causes
unexpected results if you want unusual data storage).  If the ATT_TYPE
is 'NC_CHAR' then the argument is interpreted as a string and it may
contain C-language escape sequences, e.g., '\n', which NCO will
interpret before writing anything to disk.  NCO translates valid escape
sequences and stores the appropriate ASCII code instead.  Since two byte
escape sequences, e.g., '\n', represent one-byte ASCII codes, e.g.,
ASCII 10 (decimal), the stored string attribute is one byte shorter than
the input string length for each embedded escape sequence.  The most
frequently used C-language escape sequences are '\n' (for linefeed) and
'\t' (for horizontal tab).  These sequences in particular allow
convenient editing of formatted text attributes.  The other valid ASCII
codes are '\a', '\b', '\f', '\r', '\v', and '\\'.  *Note ncks netCDF
Kitchen Sink::, for more examples of string formatting (with the 'ncks'
'-s' option) with special characters.

   Analogous to 'printf', other special characters are also allowed by
'ncatted' if they are "protected" by a backslash.  The characters '"',
''', '?', and '\' may be input to the shell as '\"', '\'', '\?', and
'\\'.  NCO simply strips away the leading backslash from these
characters before editing the attribute.  No other characters require
protection by a backslash.  Backslashes which precede any other
character (e.g., '3', 'm', '$', '|', '&', '@', '%', '{', and '}') will
not be filtered and will be included in the attribute.

   Note that the NUL character '\0' which terminates C language strings
is assumed and need not be explicitly specified.  If '\0' is input, it
is translated to the NUL character.  However, this will make the
subsequent portion of the string, if any, invisible to C standard
library string functions.  And that may cause unintended consequences.
Because of these context-sensitive rules, one must use 'ncatted' with
care in order to store data, rather than text strings, in an attribute
of type 'NC_CHAR'.

   Note that 'ncatted' interprets character attributes (i.e., attributes
of type 'NC_CHAR') as strings.  EXAMPLES

   Append the string 'Data version 2.0.\n' to the global attribute
'history':
     ncatted -a history,global,a,c,'Data version 2.0\n' in.nc
   Note the use of embedded C language 'printf()'-style escape
sequences.

   Change the value of the 'long_name' attribute for variable 'T' from
whatever it currently is to "temperature":
     ncatted -a long_name,T,o,c,temperature in.nc

   Many model and observational datasets use missing values that are not
annotated in the standard manner.  For example, at the time (2015-2018)
of this writing, the MPAS ocean and ice models use
-9.99999979021476795361e+33 as the missing value, yet do not store a
'_FillValue' attribute with any variables.  To prevent arithmetic from
treating these values as normal, designate this value as the
'_FillValue' attribute:
     ncatted    -a _FillValue,,o,d,-9.99999979021476795361e+33 in.nc
     ncatted -t -a _FillValue,,o,d,-9.99999979021476795361e+33 in.nc
     ncatted -t -a _FillValue,,o,d,-9.99999979021476795361e+33 \
                -a _FillValue,,o,f,1.0e36 -a _FillValue,,o,i,-999 in.nc
   The first example adds the attribute to all variables.  The '-t'
switch causes the second example to add the attribute only to double
precision variables.  This is often more useful, and can be used to
provide distinct missing value attributes to each numeric type, as in
the third example.

   NCO arithmetic operators may not work as expected on IEEE NaN (short
for Not-a-Number) and NaN-like numbers such as positive infinity and
negative infinity (1).  One way to work-around this problem is to change
IEEE NaNs to normal missing values.  As of NCO 4.1.0 (March, 2012),
'ncatted' works with NaNs (though none of the arithmetic operators do).
This limited support enables users to change NaN to a normal number
before performing arithmetic or propagating a NaN-tainted dataset.
First set the missing value (i.e., the value of the '_FillValue'
attribute) for the variable(s) in question to the IEEE NaN value.
     ncatted -a _FillValue,,o,f,NaN in.nc
   Then change the missing value from the IEEE NaN value to a normal
IEEE number, like 1.0e36 (or to whatever the original missing value
was).
     ncatted -a _FillValue,,m,f,1.0e36 in.nc
   Some NASA MODIS datasets provide a real-world example.
     ncatted -O -a _FillValue,,m,d,1.0e36 -a missing_value,,m,d,1.0e36 \
             MODIS_L2N_20140304T1120.nc MODIS_L2N_20140304T1120_noNaN.nc

   Delete all existing 'units' attributes:
     ncatted -a units,,d,, in.nc
The value of VAR_NM was left blank in order to select all variables in
the file.  The values of ATT_TYPE and ATT_VAL were left blank because
they are superfluous in "Delete" mode.

   Delete all attributes associated with the 'tpt' variable, and delete
all global attributes
     ncatted -a ,tpt,d,, -a ,global,d,, in.nc
The value of ATT_NM was left blank in order to select all attributes
associated with the variable.  To delete all global attributes, simply
replace 'tpt' with 'global' in the above.

   Modify all existing 'units' attributes to 'meter second-1':
     ncatted -a units,,m,c,'meter second-1' in.nc

   Add a 'units' attribute of 'kilogram kilogram-1' to all variables
whose first three characters are 'H2O':
     ncatted -a units,'^H2O',c,c,'kilogram kilogram-1' in.nc

   Overwrite the 'quanta' attribute of variable 'energy' to an array of
four integers.
     ncatted -a quanta,energy,o,s,'010,101,111,121' in.nc

   As of NCO 3.9.6 (January, 2009), 'ncatted' accepts "extended regular
expressions" as arguments for variable names, and, since NCO 4.5.1
(July, 2015), for attribute names.
     ncatted -a isotope,'^H2O*',c,s,'18' in.nc
     ncatted -a '.?_iso19115$','^H2O*',d,, in.nc
   The first example creates 'isotope' attributes for all variables
whose names contain 'H2O'.  The second deletes all attributes whose
names end in '_iso19115' from all variables whose names contain 'H2O'.
See *note Subsetting Files:: for more details on using regular
expressions.

   As of NCO 4.3.8 (November, 2013), 'ncatted' accepts full and partial
group paths in names of attributes, variables, dimensions, and groups.
     # Overwrite units attribute of specific 'lon' variable
     ncatted -O -a units,/g1/lon,o,c,'degrees_west' in_grp.nc
     # Overwrite units attribute of all 'lon' variables
     ncatted -O -a units,lon,o,c,'degrees_west' in_grp.nc
     # Delete units attribute of all 'lon' variables
     ncatted -O -a units,lon,d,, in_grp.nc
     # Overwrite units attribute with new type for specific 'lon' variable
     ncatted -O -a units,/g1/lon,o,sng,'degrees_west' in_grp.nc
     # Add new_att attribute to all variables
     ncatted -O -a new_att,,c,sng,'new variable attribute' in_grp.nc
     # Add new_grp_att group attribute to all groups
     ncatted -O -a new_grp_att,group,c,sng,'new group attribute' in_grp.nc
     # Add new_grp_att group attribute to single group
     ncatted -O -a g1_grp_att,g1,c,sng,'new group attribute' in_grp.nc
     # Add new_glb_att global attribute to root group
     ncatted -O -a new_glb_att,global,c,sng,'new global attribute' in_grp.nc
   Note that regular expressions work well in conjuction with group path
support.  In other words, the variable name (including group path
component) and the attribute names may both be extended regular
expressions.

   Demonstrate input of C-language escape sequences (e.g., '\n') and
other special characters (e.g., '\"')
     ncatted -h -a special,global,o,c,
     '\nDouble quote: \"\nTwo consecutive double quotes: \"\"\n
     Single quote: Beyond my shell abilities!\nBackslash: \\\n
     Two consecutive backslashes: \\\\\nQuestion mark: \?\n' in.nc
   Note that the entire attribute is protected from the shell by single
quotes.  These outer single quotes are necessary for interactive use,
but may be omitted in batch scripts.

   ---------- Footnotes ----------

   (1) NaN is a special floating point value (not a string).  Arithmetic
comparisons to NaN and NaN-like numbers always return False, contrary to
the behavior of all other numbers.  This behavior is difficult to
intuit, yet IEEE 754 mandates it.  To correctly handle NaNs during
arithmetic, code must use special math library macros (e.g.,
'isnormal()') to determine whether any operand is special.  If so,
additional special logic must handle the arithmetic.  This is in
addition to the normal handling incurred to correctly handle missing
values.  Handling field and missing values (either or both of which may
be NaN) in binary operators thus incurs four-to-eight extra code paths.
Each code path slows down arithmetic relative to normal numbers.  This
makes supporting NaN arithmetic costly and inefficient.  Hence NCO
supports NaN only to the extent necessary to replace it with a normal
number.  Although using NaN for the missing value (or any value) in
datasets is legal in netCDF, we strongly discourage it.  We recommend
avoiding NaN entirely.


File: nco.info,  Node: ncbo netCDF Binary Operator,  Next: ncclimo netCDF Climatology Generator,  Prev: ncatted netCDF Attribute Editor,  Up: Reference Manual

4.3 'ncbo' netCDF Binary Operator
=================================

SYNTAX
     ncbo [-3] [-4] [-5] [-6] [-7] [-A] [-C] [-c]
     [--cnk_byt SZ_BYT] [--cnk_csh SZ_BYT] [--cnk_dmn NM,SZ_LMN]
     [--cnk_map MAP] [--cnk_min SZ_BYT] [--cnk_plc PLC] [--cnk_scl SZ_LMN]
     [-D DBG] [-d DIM,[MIN][,[MAX][,[STRIDE]]] [-F] [--fl_fmt FL_FMT]
     [-G GPE_DSC] [-g GRP[,...]] [--glb ...] [-h] [--hdr_pad NBR] [--hpss]
     [-L DFL_LVL] [-l PATH] [--no_cll_msr] [--no_frm_trm] [--no_tmp_fl]
     [-O] [-o FILE_3] [-p PATH] [-R] [-r] [--ram_all]
     [-t THR_NBR] [--unn] [-v VAR[,...]] [-X ...] [-x] [-y OP_TYP]
     FILE_1 FILE_2 [FILE_3]

DESCRIPTION

   'ncbo' performs binary operations on variables in FILE_1 and the
corresponding variables (those with the same name) in FILE_2 and stores
the results in FILE_3.  The binary operation operates on the entire
files (modulo any excluded variables).  *Note Missing Values::, for
treatment of missing values.  One of the four standard arithmetic binary
operations currently supported must be selected with the '-y OP_TYP'
switch (or long options '--op_typ' or '--operation').  The valid binary
operations for 'ncbo', their definitions, corresponding values of the
OP_TYP key, and alternate invocations are:
"Addition"
     Definition: FILE_3 = FILE_1 + FILE_2
     Alternate invocation: 'ncadd'
     OP_TYP key values: 'add', '+', 'addition'
     Examples: 'ncbo --op_typ=add 1.nc 2.nc 3.nc', 'ncadd 1.nc 2.nc
     3.nc'
"Subtraction"
     Definition: FILE_3 = FILE_1 - FILE_2
     Alternate invocations: 'ncdiff', 'ncsub', 'ncsubtract'
     OP_TYP key values: 'sbt', '-', 'dff', 'diff', 'sub', 'subtract',
     'subtraction'
     Examples: 'ncbo --op_typ=- 1.nc 2.nc 3.nc', 'ncdiff 1.nc 2.nc 3.nc'
"Multiplication"
     Definition: FILE_3 = FILE_1 * FILE_2
     Alternate invocations: 'ncmult', 'ncmultiply'
     OP_TYP key values: 'mlt', '*', 'mult', 'multiply', 'multiplication'
     Examples: 'ncbo --op_typ=mlt 1.nc 2.nc 3.nc', 'ncmult 1.nc 2.nc
     3.nc'
"Division"
     Definition: FILE_3 = FILE_1 / FILE_2
     Alternate invocation: 'ncdivide'
     OP_TYP key values: 'dvd', '/', 'divide', 'division'
     Examples: 'ncbo --op_typ=/ 1.nc 2.nc 3.nc', 'ncdivide 1.nc 2.nc
     3.nc'
Care should be taken when using the shortest form of key values, i.e.,
'+', '-', '*', and '/'.  Some of these single characters may have
special meanings to the shell (1).  Place these characters inside quotes
to keep them from being interpreted (globbed) by the shell (2).  For
example, the following commands are equivalent
     ncbo --op_typ=* 1.nc 2.nc 3.nc # Dangerous (shell may try to glob)
     ncbo --op_typ='*' 1.nc 2.nc 3.nc # Safe ('*' protected from shell)
     ncbo --op_typ="*" 1.nc 2.nc 3.nc # Safe ('*' protected from shell)
     ncbo --op_typ=mlt 1.nc 2.nc 3.nc
     ncbo --op_typ=mult 1.nc 2.nc 3.nc
     ncbo --op_typ=multiply 1.nc 2.nc 3.nc
     ncbo --op_typ=multiplication 1.nc 2.nc 3.nc
     ncmult 1.nc 2.nc 3.nc # First do 'ln -s ncbo ncmult'
     ncmultiply 1.nc 2.nc 3.nc # First do 'ln -s ncbo ncmultiply'
   No particular argument or invocation form is preferred.  Users are
encouraged to use the forms which are most intuitive to them.

   Normally, 'ncbo' will fail unless an operation type is specified with
'-y' (equivalent to '--op_typ').  You may create exceptions to this rule
to suit your particular tastes, in conformance with your site's policy
on "symbolic links" to executables (files of a different name point to
the actual executable).  For many years, 'ncdiff' was the main binary
file operator.  As a result, many users prefer to continue invoking
'ncdiff' rather than memorizing a new command ('ncbo -y SBT') which
behaves identically to the original 'ncdiff' command.  However, from a
software maintenance standpoint, maintaining a distinct executable for
each binary operation (e.g., 'ncadd') is untenable, and a single
executable, 'ncbo', is desirable.  To maintain backward compatibility,
therefore, NCO automatically creates a symbolic link from 'ncbo' to
'ncdiff'.  Thus 'ncdiff' is called an "alternate invocation" of 'ncbo'.
'ncbo' supports many additional alternate invocations which must be
manually activated.  Should users or system adminitrators decide to
activate them, the procedure is simple.  For example, to use 'ncadd'
instead of 'ncbo --op_typ=add', simply create a symbolic link from
'ncbo' to 'ncadd' (3).  The alternatate invocations supported for each
operation type are listed above.  Alternatively, users may always define
'ncadd' as an "alias" to 'ncbo --op_typ=add' (4).

   It is important to maintain portability in NCO scripts.  Therefore we
recommend that site-specfic invocations (e.g., 'ncadd') be used only in
interactive sessions from the command-line.  For scripts, we recommend
using the full invocation (e.g., 'ncbo --op_typ=add').  This ensures
portability of scripts between users and sites.

   'ncbo' operates (e.g., adds) variables in FILE_2 with the
corresponding variables (those with the same name) in FILE_1 and stores
the results in FILE_3.  Variables in FILE_1 or FILE_2 are "broadcast" to
conform to the corresponding variable in the other input file if
necessary(5).  Now 'ncbo' is completely symmetric with respect to FILE_1
and FILE_2, i.e., FILE_1 - FILE_2 = - (FILE_2 - FILE_1.

   Broadcasting a variable means creating data in non-existing
dimensions by copying data in existing dimensions.  For example, a two
dimensional variable in FILE_2 can be subtracted from a four, three, or
two (not one or zero) dimensional variable (of the same name) in
'file_1'.  This functionality allows the user to compute anomalies from
the mean.  In the future, we will broadcast variables in FILE_1, if
necessary to conform to their counterparts in FILE_2.  Thus, presently,
the number of dimensions, or "rank", of any processed variable in FILE_1
must be greater than or equal to the rank of the same variable in
FILE_2.  Of course, the size of all dimensions common to both FILE_1 and
FILE_2 must be equal.

   When computing anomalies from the mean it is often the case that
FILE_2 was created by applying an averaging operator to a file with
initially the same dimensions as FILE_1 (often FILE_1 itself).  In these
cases, creating FILE_2 with 'ncra' rather than 'ncwa' will cause the
'ncbo' operation to fail.  For concreteness say the record dimension in
'file_1' is 'time'.  If FILE_2 was created by averaging FILE_1 over the
'time' dimension with the 'ncra' operator (rather than with the 'ncwa'
operator), then FILE_2 will have a 'time' dimension of size 1 rather
than having no 'time' dimension at all (6).  In this case the input
files to 'ncbo', FILE_1 and FILE_2, will have unequally sized 'time'
dimensions which causes 'ncbo' to fail.  To prevent this from occurring,
use 'ncwa' to remove the 'time' dimension from FILE_2.  See the example
below.

   'ncbo' never operates on coordinate variables or variables of type
'NC_CHAR' or 'NC_STRING'.  This ensures that coordinates like (e.g.,
latitude and longitude) are physically meaningful in the output file,
FILE_3.  This behavior is hardcoded.  'ncbo' applies special rules to
some CF-defined (and/or NCAR CCSM or NCAR CCM fields) such as 'ORO'.
See *note CF Conventions:: for a complete description.  Finally, we note
that 'ncflint' (*note ncflint netCDF File Interpolator::) is designed
for file interpolation.  As such, it also performs file subtraction,
addition, multiplication, albeit in a more convoluted way than 'ncbo'.

   Beginning with NCO version 4.3.1 (May, 2013), 'ncbo' supports "group
broadcasting".  Group broadcasting means processing data based on group
patterns in the input file(s) and automatically transferring or
transforming groups to the output file.  Consider the case where FILE_1
contains multiple groups each with the variable V1, while FILE_2
contains V1 only in its top-level (i.e., root) group.  Then 'ncbo' will
replicate the group structure of FILE_1 in the output file, FILE_3.
Each group in FILE_3 contains the output of the corresponding group in
FILE_1 operating on the data in the single group in FILE_2.  An example
is provided below.

EXAMPLES

   Say files '85_0112.nc' and '86_0112.nc' each contain 12 months of
data.  Compute the change in the monthly averages from 1985 to 1986:
     ncbo   86_0112.nc 85_0112.nc 86m85_0112.nc
     ncdiff 86_0112.nc 85_0112.nc 86m85_0112.nc
     ncbo --op_typ=sub 86_0112.nc 85_0112.nc 86m85_0112.nc
     ncbo --op_typ='-' 86_0112.nc 85_0112.nc 86m85_0112.nc
These commands are all different ways of expressing the same thing.

   The following examples demonstrate the broadcasting feature of
'ncbo'.  Say we wish to compute the monthly anomalies of 'T' from the
yearly average of 'T' for the year 1985.  First we create the 1985
average from the monthly data, which is stored with the record dimension
'time'.
     ncra 85_0112.nc 85.nc
     ncwa -O -a time 85.nc 85.nc
The second command, 'ncwa', gets rid of the 'time' dimension of size 1
that 'ncra' left in '85.nc'.  Now none of the variables in '85.nc' has a
'time' dimension.  A quicker way to accomplish this is to use 'ncwa'
from the beginning:
     ncwa -a time 85_0112.nc 85.nc
We are now ready to use 'ncbo' to compute the anomalies for 1985:
     ncdiff -v T 85_0112.nc 85.nc t_anm_85_0112.nc
Each of the 12 records in 't_anm_85_0112.nc' now contains the monthly
deviation of 'T' from the annual mean of 'T' for each gridpoint.

   Say we wish to compute the monthly gridpoint anomalies from the zonal
annual mean.  A "zonal mean" is a quantity that has been averaged over
the longitudinal (or X) direction.  First we use 'ncwa' to average over
longitudinal direction 'lon', creating '85_x.nc', the zonal mean of
'85.nc'.  Then we use 'ncbo' to subtract the zonal annual means from the
monthly gridpoint data:
     ncwa -a lon 85.nc 85_x.nc
     ncdiff 85_0112.nc 85_x.nc tx_anm_85_0112.nc
This examples works assuming '85_0112.nc' has dimensions 'time' and
'lon', and that '85_x.nc' has no 'time' or 'lon' dimension.

   Group broadcasting simplifies evaluation of multiple models against
observations.  Consider the input file 'cmip5.nc' which contains
multiple top-level groups 'cesm', 'ecmwf', and 'giss', each of which
contains the surface air temperature field 'tas'.  We wish to compare
these models to observations stored in 'obs.nc' which contains 'tas'
only in its top-level (i.e., root) group.  It is often the case that
many models and/or model simulations exist, whereas only one
observational dataset does.  We evaluate the models and obtain the bias
(difference) between models and observations by subtracting 'obs.nc'
from 'cmip5.nc'.  Then 'ncbo' "broadcasts" (i.e., replicates) the
observational data to match the group structure of 'cmip5.nc',
subtracts, and then stores the results in the output file, 'bias.nc'
which has the same group structure as 'cmip5.nc'.
     % ncbo -O cmip5.nc obs.nc bias.nc
     % ncks -H -v tas -d time,3 bias.nc
     /cesm/tas
     time[3] tas[3]=-1
     /ecmwf/tas
     time[3] tas[3]=0
     /giss/tas
     time[3] tas[3]=1

   As a final example, say we have five years of monthly data (i.e.,
60 months) stored in '8501_8912.nc' and we wish to create a file which
contains the twelve month seasonal cycle of the average monthly anomaly
from the five-year mean of this data.  The following method is just one
permutation of many which will accomplish the same result.  First use
'ncwa' to create the five-year mean:
     ncwa -a time 8501_8912.nc 8589.nc
Next use 'ncbo' to create a file containing the difference of each
month's data from the five-year mean:
     ncbo 8501_8912.nc 8589.nc t_anm_8501_8912.nc
Now use 'ncks' to group together the five January anomalies in one file,
and use 'ncra' to create the average anomaly for all five Januarys.
These commands are embedded in a shell loop so they are repeated for all
twelve months:
     for idx in {1..12}; do # Bash Shell (version 3.0+) 
       idx=`printf "%02d" ${idx}` # Zero-pad to preserve order
       ncks -F -d time,${idx},,12 t_anm_8501_8912.nc foo.${idx}
       ncra foo.${idx} t_anm_8589_${idx}.nc
     done
     for idx in 01 02 03 04 05 06 07 08 09 10 11 12; do # Bourne Shell
       ncks -F -d time,${idx},,12 t_anm_8501_8912.nc foo.${idx}
       ncra foo.${idx} t_anm_8589_${idx}.nc
     done
     foreach idx (01 02 03 04 05 06 07 08 09 10 11 12) # C Shell
       ncks -F -d time,${idx},,12 t_anm_8501_8912.nc foo.${idx}
       ncra foo.${idx} t_anm_8589_${idx}.nc
     end
Note that 'ncra' understands the 'stride' argument so the two commands
inside the loop may be combined into the single command
     ncra -F -d time,${idx},,12 t_anm_8501_8912.nc foo.${idx}
Finally, use 'ncrcat' to concatenate the 12 average monthly anomaly
files into one twelve-record file which contains the entire seasonal
cycle of the monthly anomalies:
     ncrcat t_anm_8589_??.nc t_anm_8589_0112.nc

   ---------- Footnotes ----------

   (1) A naked (i.e., unprotected or unquoted) '*' is a wildcard
character.  A naked '-' may confuse the command line parser.  A naked
'+' and '/' are relatively harmless.

   (2) The widely used shell Bash correctly interprets all these special
characters even when they are not quoted.  That is, Bash does not
prevent NCO from correctly interpreting the intended arithmetic
operation when the following arguments are given (without quotes) to
'ncbo': '--op_typ=+', '--op_typ=-', '--op_typ=*', and '--op_typ=/'

   (3) The command to do this is 'ln -s -f ncbo ncadd'

   (4) The command to do this is 'alias ncadd='ncbo --op_typ=add''

   (5) Prior to NCO version 4.3.1 (May, 2013), 'ncbo' would only
broadcast variables in FILE_2 to conform to FILE_1.  Variables in FILE_1
were _never_ broadcast to conform to the dimensions in FILE_2.

   (6) This is because 'ncra' collapses the record dimension to a size
of 1 (making it a "degenerate" dimension), but does not remove it,
while, unless '-b' is given, 'ncwa' removes all averaged dimensions.  In
other words, by default 'ncra' changes variable size though not rank,
while, 'ncwa' changes both variable size and rank.

