This is nco.info, produced by makeinfo version 6.7 from nco.texi.

INFO-DIR-SECTION netCDF
START-INFO-DIR-ENTRY
* NCO::        User Guide for the netCDF Operator suite
END-INFO-DIR-ENTRY

This file documents NCO, a collection of utilities to manipulate and
analyze netCDF files.

   Copyright (C) 1995-2020 Charlie Zender

   This is the first edition of the 'NCO User Guide',
and is consistent with version 2 of 'texinfo.tex'.

   Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.  The
license is available online at <http://www.gnu.org/copyleft/fdl.html>

   The original author of this software, Charlie Zender, wants to
improve it with the help of your suggestions, improvements, bug-reports,
and patches.
Charlie Zender <surname at uci dot edu> (yes, my surname is zender)
3200 Croul Hall
Department of Earth System Science
University of California, Irvine
Irvine, CA 92697-3100


File: nco.info,  Node: ncclimo netCDF Climatology Generator,  Next: ncecat netCDF Ensemble Concatenator,  Prev: ncbo netCDF Binary Operator,  Up: Reference Manual

4.4 'ncclimo' netCDF Climatology Generator
==========================================

SYNTAX
     ncclimo [-3] [-4] [-5] [-6] [-7]
     [-a DEC_MD] [-C CLM_MD] [-c CASEID]
     [-d DBG_LVL] [--d2f]  [--dpt_fl=DPT_FL] [-E YR_PRV] [-e YR_END]
     [-f FML_NM] [--fl_fmt=FL_FMT] [--glb_avg] [-h HST_NM] [-i DRC_IN]
     [-j JOB_NBR] [-L DFL_LVL] [-l LNK_FLG] [-m MDL_NM] [-n NCO_OPT]
     [--no_cll_msr] [--no_frm_trm] [--no_ntv_tms] [--no_stg_grd] [--no_stdin]
     [-O DRC_RGR] [-o DRC_OUT] [-p PAR_TYP] [--ppc=PPC_PRC]
     [-R RGR_OPT] [-r RGR_MAP]
     [-S YR_PRV] [-s YR_SRT] [--seasons=CSN_LST] [--sgs_frc=SGS_FRC]
     [-t THR_NBR] [--tpd=TPD_DLY] [-v VAR_LST] [--version]
     [--vrt_fl=VRT_FL] [--vrt_xtr=VRT_XTR]
     [-X DRC_XTN] [-x DRC_PRV] [--xcl_var]
     [-Y RGR_XTN] [-y RGR_PRV] [--ypf=YPF_MAX]

DESCRIPTION

   In climatology generation mode, 'ncclimo' ingests "raw" data
consisting of a monthly or annual timeseries of files and from these
produces climatological daily, monthly, seasonal, and/or annual means.
Alternatively, in timeseries reshaping (aka "splitter") mode, 'ncclimo'
will subset and temporally split the input raw data timeseries into
per-variable files spanning the entire period.  'ncclimo' can optionally
(call 'ncremap' to) regrid all output files in either mode.  Unlike the
rest of NCO, 'ncclimo' and 'ncremap' are shell scripts, not compiled
binaries(1).  As of NCO 4.9.2 (February, 2020), the 'ncclimo' and
'ncremap' scripts export the environment variable
'HDF5_USE_FILE_LOCKING' with a value of 'FALSE'.  This prevents failures
of these operators that can occur with some versions of the underlying
HDF library that attempt to lock files on file systems that cannot or do
not support it.

   There are five required options ('-c', '-s', '-e', '-i', and '-o'))
to generate climatologies, and many more options are available to
customize the processing.  Options are similar to 'ncremap' options.
Standard 'ncclimo' usage for climatology generation looks like
     ncclimo            -c caseid -s srt_yr -e end_yr -i drc_in -o drc_out
     ncclimo -m mdl_nm  -c caseid -s srt_yr -e end_yr -i drc_in -o drc_out
     ncclimo -v var_lst -c caseid -s srt_yr -e end_yr -i drc_in -o drc_out
     ncclimo --case=caseid --start=srt_yr --end=end_yr --input=drc_in --output=drc_out
   In climatology generation mode, 'ncclimo' constructs the list of
input filenames from the argument to the date and model-type options.
'ncclimo' automatically switches to timeseries reshaping mode if it
receives a list of files from 'stdin', or, alternatively, placed as
positional arguments (after the last command-line option), or if neither
of these is done and no CASEID is specified, in which case it assumes
all '*.nc' files in DRC_IN constitute the input file list.

   Options for 'ncclimo' and 'ncremap' come in both short
(single-letter) and long forms.  The handful of long-option synonyms for
each option allows the user to imbue the commands with a level of
verbosity and precision that suits her taste.  A complete description of
all options is given below, in alphabetical order of the short option
letter.  Long option synonyms are given just after the letter.  When
invoked without options, 'ncclimo' and 'ncremap' print a succinct table
of all options and some examples.  All valid options for both operators
are listed in their command syntax above but, for brevity, options that
'ncclimo' passes straight through to 'ncremap' are only fully described
in the table of 'ncremap' options.
'-a DEC_MD (--dec_md, --dcm_md, --december_mode, --dec_mode)'
     December mode determines the type of DJF average.  The two valid
     options are 'scd' (default) and 'sdd'.  SCD-mode stands for
     "Seasonally Continuous December".  The first month used is December
     of the year before the start year specified with '-s'.  The last
     month is November of the end year specified with '-e'.  SDD-mode
     stands for "Seasonally Discontinuous December".  The first month
     used is January of the specified start year.  The last month is
     December of the end year specified with '-e'.

'-C CLM_MD (--clm_md, --climatology_mode, --mode, --climatology)'
     Climatology mode.  Valid values are for CLM_MD are 'ann', 'mth',
     and 'dly'.  The value indicates the timespan of each input file for
     annual and monthly climatologies.  The default mode is 'mth', which
     means input files are monthly averages.  Use 'ann' when the input
     files are a series of annual means (a common temporal resolution
     for ice-sheet simulations).  The value 'dly' is used for all input
     files whose temporal resolution is daily or finer.  The data could
     be a daily average, or diurnally-resolved, e.g., 3-hourly.

     The climatology generator and splitter do not require that
     daily-mode input files begin or end on daily boundaries.  These
     tools hyperslab the input files using the date information required
     to performed their analysis.  This facilitates analyzing datasets
     with varying numbers of days per input file.

     Explicitly specifying '--clm_md=mth' serves a secondary purpose,
     namely invoking the default setting on systems that control
     'stdin'.  When 'ncclimo' detects that 'stdin' is not attached to
     the terminal (keyboard) it automatically expects a list of files on
     'stdin'.  Some environments, however, hijack 'stdin' for their
     purposes and thereby confuse 'ncclimo' into expecting a list
     argument.  Users have encountered this issue when attempting to run
     'ncclimo' in Python parallel environments, via inclusion in
     'crontab', and in 'nohup'-mode (whatever that is!).  In such cases,
     explicitly specify '--clm_md=mth' (or 'ann' or 'day') to persuade
     'ncclimo' to run a normal climatology.

'-c CASEID (--case, --caseid, --case_id)'
     Simulation name, or any input filename for non-CESM'ish files.  The
     use of CASEID is required in climate generation mode (unless
     equivalent information is provided through other options), where
     CASEID is used to construct both input and output filenames.  For
     CESM'ish input files like
     'famipc5_ne30_v0.3_00001.cam.h0.1980-01.nc', specify '-c
     famipc5_ne30_v0.3_00001'.  The '.cam.' and '.h0.' bits are added
     internally to produce the input filenames.  Modify these via the
     '-m MDL_NM' and '-h HST_NM' switches if needed.  For input files
     named slightly differently than standard CESM'ish names, supply the
     filename (excluding the path component) as the CASEID and 'ncclimo'
     will attempt to parse it by matching it to a database of know
     regular expressions common to model output.  These are all of the
     format PREFIX[.-]YYYY[-]MM[-]DD.SUFFIX.  The particular formats
     current supported, as of NCO version 4.7.3 (March, 2018) are:
     PREFIX'_YYYYMM'.SUFFIX, PREFIX'.YYYY-MM'.SUFFIX, and
     PREFIX'.YYYY-MM-01'.SUFFIX.  For example, input files like
     'merra2_198001.nc' (i.e., the six digits that precede the suffix
     are YYYYMM-format), specify '-c merra2_198001.nc' and the prefix
     ('merra2') will be automatically abstracted and used to template
     and generate all the filenames based on the specified YR_SRT and
     YR_END.  Please tell us any dataset filename regular expressions
     that you would like added to 'ncclimo''s internal database.

'-D DBG_LVL (--dbg_lvl, --dbg, --debug, --debug_level)'
     Specifies a debugging level similar to the rest of NCO.  If DBG_LVL
     = 1, 'ncclimo' prints more extensive diagnostics of its behavior.
     If DBG_LVL = 2, 'ncclimo' prints the commands it would execute at
     any higher or lower debugging level, but does not execute these
     commands.  If DBG_LVL > 2, 'ncclimo' prints the diagnostic
     information, executes all commands, and passes-through the
     debugging level to the regridder ('ncks') for additional
     diagnostics.

'--d2f (--d2f, --d2s, --dbl_flt, --dbl_sgl, --double_float)'
     This switch (which takes no argument) causes 'ncclimo' to invoke
     'ncremap' with the same switch, so that 'ncremap' converts all
     double precision non-coordinate variables to single precision in
     the regridded file.  This switch has no effect on files that are
     not regridded.  To demote the precision in such files, use 'ncpdq'
     to apply the 'dbl_flt' packing map to the file directly.

'--dpt_fl=DPT_FL (--dpt_fl, --depth_file, --mpas_fl, --mpas_depth)'
     The '--dpt_fl=DPT_FL' triggers the addition of a depth coordinate
     to MPAS ocean datasets that will undergo regridding.  'ncclimo'
     passes this option through to 'ncremap', and this option has no
     effect when 'ncclimo' does not invoke 'ncremap'.  The 'ncremap'
     documentation contains the full description of this option.

'-e END_YR (--end_yr, --yr_end, --end_year, --year_end, --end)'
     End year (example: 2000).  Unless the option '-a sdd' is specified,
     the last month used is November of the specified end year.  If '-a
     sdd' is specified, the last month is December of the specified end
     year.

'-f FML_NM (--fml_nm, --family, --family_name)'
     Family name (nickname) of output files.  In climate generation
     mode, output climo file names are constructed by default with the
     same CASEID as the input files.  The FML_NM, if supplied, replaces
     CASEID in output climo names, which are of the form
     FML_NM_XX_YYYYMM_YYYYMM.nc where XX is the month or seasonal
     abbreviation.  Use '-f FML_NM' to simplify long names, avoid
     overlap, etc.  Example values of FML_NM are 'control',
     'experiment', and (for a single-variable climo) 'FSNT'.  In
     timeseries reshaping mode, FML_NM will be used, if supplied, as an
     additional string in the output filename.  For example, specifying
     '-f control' would cause 'T_000101_000912.nc' to be instead named
     'T_control_000101_000912.nc'.

'-h HST_NM (--hst_nm, --history_name, --history)'
     History volume name of file used to generate climatologies.  This
     referring to the HST_NM character sequence used to construct input
     file names: 'caseid.mdl_nm.'HST_NM'.YYYY-MM.nc'.  By default input
     climo file names are constructed from the CASEID of the input
     files, together with the model name MDL_NM (specified with '-m')
     and the date range.  Use '-h HST_NM' to specify alternative history
     volumes.  Examples include 'h0' (default, works for CAM,
     CLM/CTSM/ELM), 'h1', and 'h' (for CISM).

'-i DRC_IN (--drc_in, --in_drc, --dir_in, --input)'
     Directory containing all monthly mean files to read as input to the
     climatology.  The use of DRC_IN is mandatory in climate generation
     mode and is optional in timeseries reshaping mode.  In timeseries
     reshaping mode, 'ncclimo' uses all netCDF files (meaning files with
     suffixes '.nc', '.nc3', '.nc4', '.nc5', '.nc6', '.nc7', '.cdf',
     '.hdf', '.he5', or '.h5') in DRC_IN to create the list of input
     files when no list is provided through 'stdin' or as positional
     arguments to the command-line.

'-j JOB_NBR (--job_nbr, --job_number, --jobs)'
     The JOB_NBR parameter controls the parallelism granularity of both
     timeseries reshaping (aka splitting) and climatology generation.
     These modes parallelize over different types of tasks, so we
     describe the effects of JOB_NBR separately, first for
     climatologies, then for splitting.  However, for both modes,
     JOB_NBR specifies the total number of simultaneous processes to run
     during in parallel either on the local node for Background
     parallelism, or across all the nodes for MPI parallelism (i.e.,
     JOB_NBR is the total across all nodes, it is not the number per
     node).

     For climatology generation, JOB_NBR specifies the number of
     averaging tasks to perform simultaneously on the local node for
     Background parallelism, or spread across all nodes for
     MPI-parallelism.  By default 'ncclimo' sets JOB_NBR = 12 for both
     parallelism modes.  This number ensures that monthly averages for
     all individual months complete more-or-less simultaneously, so that
     all seasonal averages can then be computed.  However, many nodes
     are too small to simultaneously average multiple distinct months
     (January, February, etc.).  Hence JOB_NBR may be set to any factor
     of 12, i.e., 1, 2, 3, 4, 6, or 12.  For Background parallelism,
     setting JOB_NBR = 4 causes four-months to be averaged at one time.
     After three batches of four-months complete, the climatology
     generator then moves on to seasonal averaging and regridding.  For
     MPI-parallelism, set JOB_NBR >= ND_NBR otherwise some nodes will be
     idle for the entire time.  For the biggest jobs, when a
     single-month nearly exhausts the RAM on a node, set JOB_NBR =
     ND_NBR so that each node gets only one job at a time.  If a node
     can handle average three distinct months simultaneously, then try
     JOB_NBR = 3*ND_NBR.  Never set JOB_NBR > 12 in climatology modes,
     since there are at most only twelve jobs that can be performed in
     parallel.

     For splitting, JOB_NBR specifies the number of simultaneous
     subsetting processes to spawn during parallel execution for both
     Background and MPI-parallelism.  In both parallelism modes
     'ncclimo' spawns processes in batches of JOB_NBR jobs, then waits
     for those processes to complete.  Once a batch finishes, 'ncclimo'
     spawns the next batch.  For Background-parallelism, all jobs are
     spawned to the local node.  For MPI-parallelism, all jobs are
     spawned in round-robin fashion to all available nodes until JOB_NBR
     jobs are running.  Rinse, lather, repeat until all variables have
     been split.  The splitter chooses its default value of JOB_NBR
     based on on the parallelism mode.  For Background parallelism,
     JOB_NBR defaults to the number of variables to be split, so that
     not specifying JOB_NBR results in launching VAR_NBR simultaneous
     splitter tasks.  This scales well to over a hundred variables in
     our tests (2).  In practice, splitting timeseries consumes minimal
     memory, since 'ncrcat' (which underlies the splitter) only holds
     one record (timestep) of a variable in memory *note Memory
     Requirements::.

     However, if splitting consumes so much RAM (e.g., because variables
     are large and/or the number of jobs is large) that a single node
     can perform only one or a few subsetting jobs at a time, then it is
     reasonable value to employ MPI to split the datasets.  For
     MPI-parallelism, JOB_NBR defaults to the number of nodes requested.
     This helps prevent users from overloading nodes with too many jobs.
     Usually, however, nodes can usually subset (and then regrid, if
     requested) multiple variables simultaneously.  In summary, by
     default JOB_NBR = VAR_NBR in Background mode, and JOB_NBR =
     NODE_NBR in MPI mode.  Subject to the availability of adequate RAM,
     expand the number of jobs per node by increasing JOB_NBR until
     overall throughput peaks.

     The main throughput bottleneck in timeseries reshaping mode is I/O.
     Increasing JOB_NBR may reduce throughput once the maximum I/O
     bandwidth of the node is reached, due to contention for I/O
     resources.  Regridding requires math that can relieve some I/O
     contention and allows for some throughput gains with increasing
     JOB_NBR.  One strategy that seems sensible is to set JOB_NBR equal
     to the number of nodes times the number of cores per node, and
     increase or decrease as necessary until throughput peaks.

'-L (--dfl_lvl, --dfl, --deflate)'
     Activate deflation (i.e., lossless compress, see *note Deflation::)
     with the '-L DFL_LVL' short option (or with the same argument to
     the '--dfl_lvl' or '--deflate' long options).  Specify deflation
     level DFL_LVL on a scale from no deflation (DFL_LVL = 0, the
     default) to maximum deflation (DFL_LVL = 9).

'-l (--lnk_flg, --link_flag)'
'--no_amwg_link (--no_amwg_link, --no_amwg_links, --no_amwg, --no_AMWG_link, --no_AMWG_links)'
'--amwg_link (--amwg_link, --amwg_links, --AMWG_link, --AMWG_links)'
     These options turn-on or turn-off the linking of E3SM/ACME-climo to
     AMWG-climo filenames.  AMWG omits the YYYYMM components of climo
     filenames, resulting in shorter names.  By default 'ncclimo'
     symbolically links the full E3SM/ACME filename (which is always)
     created to a file with the shorter (AMWG) name whose creation is
     optional.  AMWG diagnostics scripts can produce plots directly from
     the linked AMWG filenames.  The '-l' (and '--lnk_flg' and
     '--link_flag' long-option synonmyms) are true options that require
     an argument of either 'Yes' or 'No'.  The remaining synonyms are
     switches that take no arguments.  The '--amwg_link' switch and its
     synonyms cause the creation of symbolic links with AMWG filenames.
     The '--no_amwg_link' switch and its synonyms prevent the creation
     of symbolic links with AMWG filenames.  If you do not need AMWG
     filenames, turn-off linking to reduce file proliferation in the
     output directories.

'-m MDL_NM (--mdl_nm, --mdl, --model_name, --model)'
     Model name (as embedded in monthly input filenames).  Default is
     'cam'.  Other options are 'clm2', 'ocn', 'ice', 'cism', 'cice',
     'pop'.

'-n NCO_OPT (nco_opt, nco, nco_options)'
     Specifies a string of options to pass-through unaltered to 'ncks'.
     NCO_OPT defaults to '--no_tmp_fl'.  Note that 'ncclimo' passes its
     NCO_OPT to 'ncremap'.  This can cause unexpected results, so use
     the front-end options to 'ncclimo' when possible, rather than
     attempting to subvert them with NCO_OPT.

'-O DRC_RGR (--drc_rgr, --rgr_drc, --dir_rgr, --regrid)'
     Directory to hold regridded climo files.  Regridded climos are
     placed in DRC_OUT unless a separate directory for them is specified
     with '-O' (NB: capital "O").

'--no_cll_msr (--no_cll_msr, --no_cll, --no_cell_measures, --no_area)'
     This switch (which takes no argument) controls whether 'ncclimo'
     and 'ncremap' add measures variables to the extraction list along
     with the primary variable and other associated variables.  See
     *note CF Conventions:: for a detailed description.

'--no_frm_trm (--no_frm_trm, --no_frm, --no_formula_terms)'
     This switch (which takes no argument) controls whether 'ncclimo'
     and 'ncremap' add formula variables to the extraction list along
     with the primary variable and other associated variables.  See
     *note CF Conventions:: for a detailed description.

'--glb_avg (--glb_avg, --global_average)'
     As of NCO version 4.9.1 (released December, 2019), this switch
     (which takes no argument) tells the splitter to output horizontally
     spatially averaged timeseries files instead of raw, native-grid
     timeseries.  This switch only has effect in timeseries splitting
     mode.  This is useful, for example, to quickly diagnose the
     behavior of ongoing model simulations prior to a full-blown
     analysis.  Thus the spatial mean files will be in the same location
     and have the same name as the native grid timeseries would have
     been and had, respectively.  Note that this switch does not alter
     the capability of also outputting the full regridded timeseries, if
     requested, at the same time.

'--no_ntv_tms (--no_ntv_tms, --no_ntv, --no_native, --remove_native)'
     This switch (which takes no argument) controls whether the splitter
     retains native grid split files, which it does by default, or
     deletes them.  'ncclimo' can split model output from multi-variable
     native grid files into per-variable timeseries files and regrid
     those onto a so-called analysis grid.  That is the typical format
     in which Model Intercomparison Projects (MIPs) request and
     disseminate contributions.  When the data producer has no use for
     the split timeseries on the native grid, he/she can invoke this
     flag to cause 'ncclimo' to delete the native grid timeseries (not
     the raw native grid datafiles).  This functionality is implemented
     by first creating the native grid timeseries, regridding it, and
     then overwriting the native grid timeseries with the regridded
     timeseries.  Thus the regridded files will be in the same location
     and have the same name as the native grid timeseries would have
     been and had, respectively.

'--no_stg_grd (--no_stg_grd, --no_stg, --no_stagger, --no_staggered_grid)'
     This switch (which takes no argument) controls whether regridded
     output will contain the staggered grid coordinates 'slat', 'slon',
     and 'w_stag' (*note Regridding::).  By default the staggered grid
     is output for all files regridded from a Cap (aka FV) grid, except
     when the regridding is performed as part of splitting (reshaping)
     into timeseries.

'-o DRC_OUT (--drc_out, --out_drc, --dir_out, --output)'
     Directory to hold computed (output) native grid climo files.
     Regridded climos are also placed here unless a separate directory
     for them is specified with '-O' (NB: capital "O").

'-p PAR_TYP (--par_typ, --par_md, --parallel_type, --parallel_mode, --parallel)'
     Specifies the parallelism mode desired.  The options are serial
     mode ('-p srl', '-p serial', or '-p nil'), background mode
     parallelism ('-p bck' or '-p background')), and MPI parallelism
     ('-p mpi' or '-p MPI').  The default is background-mode
     parallelism.  The default PAR_TYP is 'background', which means
     'ncclimo' spawns up to twelve (one for each month) parallel
     processes at a time.  See discussion below under Memory
     Considerations.

'--ppc=PPC_PRC (--ppc, --ppc_prc, --precision, --quantize)'
     Specifies the precision of the Precision-Preserving Compression
     algorithm (*note Precision-Preserving Compression::).  A positive
     integer is interpreted as the Number of Significant Digits for the
     Bit-Grooming algorithm, and is equivalent to specifying '--ppc
     default=PPC_PRC' to a binary operator.  A positive or negative
     integer preceded by a period, e.g., '.-2' is interpreted as the
     number of Decimal Significant Digits for the rounding algorithm and
     is equivalent to specifying '--ppc default=.PPC_PRC' to a binary
     operator.  This option applies one precision algorithm and a
     uniform precision for the entire file.  To specify
     variable-by-variable precision options, pass the desired options as
     a quoted string directly with '-n NCO_OPT', e.g., '-n '--ppc
     FSNT,TREFHT=4 --ppc CLOUD=2''.

'-R RGR_OPT (rgr_opt, regrid_options)'
     Specifies a string of options to pass-through unaltered to 'ncks'.
     RGR_OPT defaults to '-O --no_tmp_fl'.

'-r RGR_MAP (--rgr_map, --regrid_map, --map)'
     Regridding map.  Unless '-r' is specified 'ncclimo' produces only a
     climatology on the native grid of the input datasets.  The RGR_MAP
     specifies how to (quickly) transform the native grid into the
     desired analysis grid.  'ncclimo' will (call 'ncremap' to) apply
     the given map to the native grid climatology and produce a second
     climatology on the analysis grid.  Options intended exclusively for
     the regridder may be passed as arguments to the '-R' switch.  See
     below the discussion on regridding.

'-s SRT_YR (--srt_yr, --yr_srt, --start_year, --year_start, --start)'
     Start year (example: 1980).  Unless the option '-a sdd' is
     specified, the first month used will be December of the year before
     the start year (to allow for contiguous DJF climos).  If '-a sdd'
     is specified, the first month used is January of the specified
     start year.

'--seasons=CSN_LST (--seasons, --csn_lst, --csn)'
     Seasons for 'ncclimo' to compute in monthly climatology generation
     mode.  The list of seasons, CSN_LST, is a comma-separated,
     case-insensitive, unordered subset of the abbreviations for the
     eleven (so far) defined seasons: 'jfm', 'amj', 'jas', 'ond', 'on',
     'fm', 'djf', 'mam', 'jja', 'son', and 'ann'.  By default
     'CSN_LST=mam,jja,son,djf'.  Moreover, 'ncclimo' automatically
     computes the climatological annual mean, 'ANN', is always computed
     when MAM, JJA, SON, and DJF are all requested (which is the
     default).  The ANN computed automatically is the time-weighted
     average of the four seasons, rather than as the time-weighted
     average of the twelve monthly climatologies.  Users who need ANN
     but not DJF, MAM, JJA, and SON should instead explicitly specify
     ANN as a season in CSN_LST.  The ANN computed as a season is the
     time-weighted average of the twelve monthly climatologies, rather
     than the time-weighted average of four seasonal climatologies.
     Specifying the four seasons and ANN in CSN_LST (e.g.,
     'CSN_LST=mam,jja,son,djf,ann') is legal though redundant and
     wasteful.  It cause ANN to be computed twice, first as the average
     of the twelve monthly climatologies, then as the average of the
     four seasons.  The special value 'CSN_LST=none' turns-off
     computation of seasonal (and annual) climatologies.
          ncclimo --seasons=none ...            # Produce only monthly climos
          ncclimo --seasons=mam,jja,son,djf ... # Monthly + MAM,JJA,SON,DJF,ANN
          ncclimo --seasons=jfm,jas,ann ...     # Monthly + JFM,JAS,ANN
          ncclimo --seasons=fm,on ...           # Monthly + FM,ON

'--stdin (--stdin, --inp_std, --std_flg, --redirect, --standard_input)'
     This switch (which takes no argument) explicitly indicates that
     input file lists are provided via 'stdin', i.e., standard input.
     In interactive environments, 'ncclimo' and 'ncremap' can
     automatically (i.e., without any switch) detect whether input is
     provided via 'stdin'.  This switch is never required for jobs run
     in an interactive shell.  However, non-interactive batch jobs (such
     as those submitted to the SLURM and PBS schedulers) make it
     impossible to unambiguously determine whether input has been
     provided via 'stdin'.  Specifically, the '--stdin' switch _must_ be
     used in non-interactive batch jobs on PBS when the input files are
     piped to 'stdin', and on SLURM when the input files are redirected
     from a file to 'stdin'.  Using this switch in any other context
     (e.g., interactive shells) is optional.

     In some other non-interactive environments (e.g., 'crontab',
     'nohup', Azure CI, CWL), 'ncclimo' and 'ncremap' may mistakenly
     expect input to be provided on 'stdin' simply because the
     environment is using 'stdin' for other purposes.  In such cases
     users may disable checking 'stdin' by explicitly invoking the
     '--clm_md' option (this works, as described above, only for
     'ncclimo'), or by invoking the '--no_stdin' flag (described next),
     which works for both 'ncclimo' and 'ncremap'.

'--no_stdin (--no_stdin, --no_inp_std, --no_redirect, --no_standard_input)'
     First introduced in NCO version 4.8.0 (released May, 2019), this
     switch (which takes no argument) disables checking standard input
     (aka 'stdin') for input files.  This is useful because 'ncclimo'
     and 'ncremap' may mistakenly expect input to be provided on 'stdin'
     in environments that use 'stdin' for other purposes.  Some
     non-interactive environments (e.g., 'crontab', 'nohup', Azure CI,
     CWL), use standard input for their own purposes, and thus confuse
     NCO into thinking that you provided the input files names via the
     'stdin' mechanism.  In such cases users may disable the automatic
     checks for standard input by explicitly invoking the '--no_stdin'
     flag.  This switch is never required for jobs run in an interactive
     shell.

'-t THR_NBR (--thr_nbr, --thr, --thread_number, --threads)'
     Specifies the number of threads used per regridding process (*note
     OpenMP Threading::).  The NCO regridder scales well to 8-16
     threads.  However, regridding with the maximum number of threads
     can interfere with climatology generation in parallel climatology
     mode (i.e., when PAR_TYP = 'mpi' or 'bck').  Hence 'ncclimo'
     defaults to THR_NBR=2.

'--tpd_out=TPD_OUT (--tpd_out, --tpd, --timesteps_per_day)'
     The number of timesteps-per-day in output created by 'ncclimo''s
     climatology generator in daily average mode.  The climatology
     output from input files at daily or sub-daily resolution is, by
     default, averaged to daily resolution, i.e., TPD_OUT=1.  If the
     number of timesteps per day in each input file is TPD_IN, then the
     user may select any value of TPD_OUT that is smaller than and
     integrally divides TPD_IN.  For example, an input timeseries with
     TPD_IN=8 (i.e., 3-hourly resolution), can be used to produce
     climatological output at 3, 6, or 12-hourly resolution by setting
     TPD_OUT to 8, 4, or 2, respectively.  This option only takes effect
     in daily-average climatology mode.

'-v VAR_LST (--var_lst, --var, --vars, --variables, --variable_list)'
     Variables to subset or to split.  Same behavior as *note Subsetting
     Files::.  The use of VAR_LST is optional in climate generation
     mode.  We suggest using this feature to test whether an 'ncclimo'
     command, especially one that is lengthy and/or time-consuming,
     works as intended on one or a few variables with, e.g., '-v T,FSNT'
     before generating the full climatology (by omitting this option).
     Invoking this switch was required in the original splitter released
     in version 4.6.5 (March, 2017), and became optional as of version
     4.6.6 (May, 2017).  This option is recommended in timeseries
     reshaping mode to prevent inadvertently copying the results of an
     entire model simulation.  Regular expressions are allowed so, e.g.,
     'PREC.?' extracts the variables 'PRECC,PRECL,PRECSC,PRECSL' if
     present.  Currently in reshaping mode all matches to a regular
     expression are placed in the same output file.  We hope to remove
     this limitation in the future.

'--version (--version, --vrs, --config, --configuration, --cnf)'
     This switch (which takes no argument) causes the operator to print
     its version and configuration.  This includes the copyright notice,
     URLs to the BSD and NCO license, directories from which the NCO
     scripts and binaries are running, and the locations of any separate
     executables that may be used by the script.

'--xcl_var (--xcl_var, --xcl, --exclude, --exclude_variables)'
     This flag (which takes no argument) changes VAR_LST, as set by the
     '--var_lst' option, from an extraction list to an exclusion list so
     that variables in VAR_LST will not be processed, and variables not
     in VAR_LST will be processed.  Thus the option '-v VAR_LST' must
     also be present for this flag to take effect.  Variables explicitly
     specified for exclusion by '--xcl --vars=VAR_LST[,...]' need not be
     present in the input file.

'--ypf_max YPF_MAX (--ypf, --years, --years_per_file)'
     Specifies the maximum number of years-per-file output by
     'ncclimo''s splitting operation.  When 'ncclimo' subsets and splits
     a collection of input files spanning a timerseries, it places each
     subset variable in its own output file.  The maximum length, in
     years, of each output file is YPF_MAX, which defaults to
     YPF_MAX=50.  If an input timeseries spans 237 years and YPF_MAX=50,
     then 'ncclimo' will generate four output files of length 50 years
     and one output file of length 37 years.  Note that invoking this
     option _causes_ 'ncclimo' to enter timeseries reshaping mode.  In
     fact, one _must_ use '--ypf' to turn-on splitter mode when the
     input files are specified by using the '-i drc_in' method.
     Otherwise it would be ambiguous whether to generate a climatology
     from or to split the input files.

Timeseries Reshaping mode, aka Splitting
----------------------------------------

This section of the 'ncclimo' documentation applies only to resphaping
mode, whereas all subsequent sections apply to climatology generation
mode.  As mentioned above, 'ncclimo' automatically switches to
timeseries reshaping mode if it receives a list of files through
'stdin', or, alternatively, placed as positional arguments (after the
last command-line option), or if neither of these is done and no CASEID
is specified, in which case it assumes all '*.nc' files in DRC_IN
constitute the input file list.  These examples invoke reshaping mode in
the three possible ways:
     # Pipe list to stdin
     cd $drc_in;ls *mdl*000[1-9]*.nc | ncclimo -v T,Q,RH -s 1 -e 9 -o $drc_out
     # Redirect list from file to stdin
     cd $drc_in;ls *mdl*000[1-9]*.nc > foo;ncclimo -v T,Q,RH -s 1 -e 9 -o $drc_out < foo
     # List as positional arguments
     ncclimo -v T,Q,RH -s 1 -e 9 -o $drc_out $drc_in/*mdl*000[1-9]*.nc
     # Glob directory
     ncclimo -v T,Q,RH -s 1 -e 9 -i $drc_in -o $drc_out
   Assuming each input file is a monthly average comprising the
variables T, Q, and RH, then the output will be files
'T_000101_000912.nc', 'Q_000101_000912.nc', and 'RH_000101_000912.nc'.
'ncclimo' _reshapes_ the input so that the outputs are continuous
timeseries of each variable taken from all input files.  When necessary,
the output is split into segments each containing no more than YPF_MAX
(default 50) years of input, i.e., 'T_000101_005012.nc',
'T_005101_009912.nc', 'T_010001_014912.nc', etc.

MPAS-O/I considerations
-----------------------

MPAS ocean and ice models currently have their own (non-CESM'ish) naming
convention that guarantees output files have the same names for all
simulations.  By default 'ncclimo' analyzes the "timeSeriesStatsMonthly"
analysis member output (tell us if you want options for other analysis
members).  'ncclimo' recognizes input files as being MPAS-style when
invoked with '-m mpaso' or '-m mpascice' like this:
     ncclimo -m mpaso    -s 1980 -e 1983 -i $drc_in -o $drc_out # MPAS-O
     ncclimo -m mpascice -s 1980 -e 1983 -i $drc_in -o $drc_out # MPAS-I

   MPAS climos are unaware of missing values until/unless input files
are "fixed".  We recommend that simulation producers annotate all
floating point variables with the appropriate '_FillValue' prior to
invoking 'ncclimo'.  Run something like this once in the history-file
directory:
     for fl in `ls hist.*` ; do
       ncatted -O -t -a _FillValue,,o,d,-9.99999979021476795361e+33 ${fl}
     done
   If/when MPAS-O/I generates the '_FillValue' attributes itself, this
step can and should be skipped.  All other 'ncclimo' features like
regridding (below) are invoked identically for MPAS as for CAM/CLM users
although under-the-hood 'ncclimo' does do some special pre-processing
(dimension permutation, metadata annotation) for MPAS.  A five-year
oEC60to30 MPAS-O climo with regridding to T62 takes less than 10 minutes
on the machine 'rhea'.

Annual climos
-------------

Not all model or observed history files are created as monthly means.
To create a climatological annual mean from a series of annual mean
inputs, select 'ncclimo''s annual climatology mode with the '-C ann'
option:
     ncclimo -C ann -m cism -h h -c caseid -s 1851 -e 1900 -i drc_in -o drc_out
   The options '-m mdl_nm' and '-h hst_nm' (that default to 'cam' and
'h0', respectively) tell 'ncclimo' how to construct the input filenames.
The above formula names the files 'caseid.cism.h.1851-01-01-00000.nc',
'caseid.cism.h.1852-01-01-00000.nc', and so on.  Annual climatology mode
produces a single output file (or two if regridding is selected), and in
all other respects behaves the same as monthly climatology mode.

Regridding Climos and Other Files
---------------------------------

'ncclimo' will (optionally) regrid during climatology generation and
produce climatology files on both native and analysis grids.  This
regridding is virtually free, because it is performed on idle
nodes/cores after monthly climatologies have been computed and while
seasonal climatologies are being computed.  This load-balancing can save
half-an-hour on ne120 datasets.  To regrid, simply pass the desired
mapfile name with '-r map.nc', e.g., '-r
maps/map_ne120np4_to_fv257x512_aave.20150901.nc'.  Although this should
not be necessary for normal use, you may pass any options specific to
regridding with '-R opt1 opt2'.

   Specifying '-O DRC_RGR' (NB: uppercase 'O') causes 'ncclimo' to place
the regridded files in the directory DRC_RGR.  These files have the same
names as the native grid climos from which they were derived.  There is
no namespace conflict because they are in separate directories.  These
files also have symbolic links to their AMWG filenames.  If '-O DRC_RGR'
is not specified, 'ncclimo' places all regridded files in the native
grid climo output directory, DRC_OUT, specified by '-o DRC_OUT' (NB:
lowercase 'o').  To avoid namespace conflicts when both climos are
stored in the same directory, the names of regridded files are suffixed
by the destination geometry string obtained from the mapfile, e.g.,
'*_climo_fv257x512_bilin.nc'.  These files also have symbolic links to
their AMWG filenames.
     ncclimo -c amip_xpt -s 1980 -e 1983 -i drc_in -o drc_out
     ncclimo -c amip_xpt -s 1980 -e 1983 -i drc_in -o drc_out -r map_fl
     ncclimo -c amip_xpt -s 1980 -e 1983 -i drc_in -o drc_out -r map_fl -O drc_rgr
The above commands perform a climatology without regridding, then with
regridding (all climos stored in DRC_OUT), then with regridding and
storing regridded files separately.  Paths specified by DRC_IN, DRC_OUT,
and DRC_RGR may be relative or absolute.  An alternative to regridding
during climatology generation is to regrid afterwards with 'ncremap',
which has more special features built-in for regridding.  To use
'ncremap' to regrid a climatology in DRC_OUT and place the results in
DRC_RGR, use something like
     ncremap -I drc_out -m map.nc -O drc_rgr
     ls drc_out/*climo* | ncremap -m map.nc -O drc_rgr
   See *note ncremap netCDF Remapper:: for more details (including
MPAS!).

Extended Climatologies
----------------------

'ncclimo' supports two methods for generating extended climatologies:
Binary and Incremental.  Both methods lengthen a climatology without
requiring access to all the raw monthly data spanning the time period.
The binary method combines, with appropriate weighting, two previously
computed climatologies into a single climatology.  No raw monthly data
are employed.  The incremental method computes a climatology from raw
monthly data and (with appropriate weighting) combines that with a
previously computed climatology that ends the month prior to raw data.
The incremental method was introduced in NCO version 4.6.1 (released
August, 2016), and the binary method was introduced in NCO version 4.6.3
(released December, 2016).

   Both methods, binary and incremental, compute the so-called "extended
climo" as a weighted mean of two shorter climatologies, called the
"previous" and "current" climos.  The incremental method uses the
original monthly input to compute the curent climo, which must
immediately follow in time the previous climo which has been
pre-computed.  The binary method use pre-computed climos for both the
previous and current climos, and these climos need not be sequential nor
chronological.  Both previous and current climos for both binary and
incremental methods may be of any length (in years); their weights will
be automatically adjusted in computing the extended climo.

   The use of pre-computed climos permits ongoing simulations (or
lengthy observations) to be analyzed in shorter segments combined
piecemeal, instead of requiring all raw, native-grid data to be
simultaneously accessible.  Without extended climatology capability,
generating a one-hundred year climatology requires that one-hundred
years of monthly data be available on disk.  Disk-space requirements for
large datasets may make this untenable.  Extended climo methods permits
a one-hundred year climo to be generated as the weighted mean of, say,
the current ten year climatology (weighted at 10%) combined with the
pre-computed climatology of the previous 90-years (weighted at 90%).
The 90-year climo could itself have been generated incrementally or
binary-wise, and so on.  Climatologies occupy at most 17/(12N) the
amount of space of N years of monthly data, so the extended methods
vastly reduce disk-space requirements.

   Incremental mode is selected by specifying '-S', the start year of
the pre-computed, previous climo.  The argument to '-S') is the previous
climo start year.  That, together with the current climo end year,
determines the extended climo range.  'ncclimo' assumes that the
previous climo ends the month before the current climo begins.  In
incremental mode, 'ncclimo' first generates the current climatology from
the current monthly input files then weights that current climo with the
previous climo to produce the extended climo.

   Binary mode is selected by specifying both '-S' and '-E', the end
year of the pre-computed, previous climo.  In binary mode, the previous
and current climatologies can be of any length, and from any
time-period, even overlapping.  Most users will run extended clmos the
same way they run regular climos in terms of parallelism and regridding,
although that is not required.  Both climos must treat Decembers same
way (or else previous climo files will not be found), and if subsetting
(i.e., '-v var_lst') is performed, then the subset must remain the same,
and if nicknames (i.e., '-f fml_nm') are employed, then the nickname
must remain the same.

   As of 20161129, the 'climatology_bounds' attributes of extended
climos are incorrect.  This is a work in progress...

   Options:
'-E YR_END_PRV (--yr_end_prv, --prv_yr_end, --previous_end)'
     The ending year of the previous climo.  This argument is required
     to trigger binary climatologies, and should not be used for
     incremental climatologies.

'-S YR_SRT_PRV (--yr_srt_prv, --prv_yr_srt, --previous_start)'
     The starting year of the previous climo.  This argument is required
     to trigger incremental climatologies, and is also mandatory for
     binary climatologies.

'-X DRC_XTN (--drc_xtn, --xtn_drc, --extended)'
     Directory in which the extended native grid climo files will be
     stored for an extended climatology.  Default value is DRC_PRV.
     Unless a separate directory is specified (with '-Y') for the
     extended climo on the analysis grid, it will be stored in DRC_XTN,
     too.

'-x DRC_PRV (--drc_prv, --prv_drc, --previous)'
     Directory in which the previous native grid climo files reside for
     an incremental climatology.  Default value is DRC_OUT.  Unless a
     separate directory is specified (with '-y') for the previous climo
     on the analysis grid, it is assumed to reside in DRC_PRV, too.

'-Y DRC_RGR_XTN (--drc_rgr_xtn, --drc_xtn_rgr, --extended_regridded, --regridded_extended)'
     Directory in which the extended analysis grid climo files will be
     stored in an incremental climatology.  Default value is DRC_XTN.

'-y DRC_RGR_PRV (--drc_rgr_prv, --drc_prv_rgr, --regridded_previous, --previous_regridded)'
     Directory in which the previous climo on the analysis grid resides
     in an incremental climatology.  Default value is DRC_PRV.

   Incremental method climatologies can be as simple as providing a
start year for the previous climo, e.g.,
     ncclimo -v FSNT,AODVIS -c caseid -s 1980 -e 1981 -i raw -o clm -r map.nc
     ncclimo -v FSNT,AODVIS -c caseid -s 1982 -e 1983 -i raw -o clm -r map.nc -S 1980
   By default 'ncclimo' stores all native and analysis grid climos in
one directory so the above "just works".  There are no namespace clashes
because all climos are for distinct years, and regridded files have a
suffix based on their grid resolution.  However, there can be only one
set of AMWG filename links due to AMWG filename convention.  Thus AMWG
filename links, if any, point to the latest extended climo in a given
directory.

   Many researchers segregate (with '-O DRC_RGR') native-grid from
analysis-grid climos.  Incrementally generated climos must be consistent
in this regard.  In other words, all climos contributing to an extended
climo must have their native-grid and analysis-grid files in the same
(per-climo) directory, or all climos must segregate their native from
their analysis grid files.  Do not segregate the grids in one climo, and
combine them in another.  Such climos cannot be incrementally
aggregated.  Thus incrementing climos can require from zero to four
additional options that specify all the previous and extended
climatologies for both native and analysis grids.  The example below
constructs the current climo in CRR, then combines the weighted average
of that with the previous climo in PRV, and places the resulting
extended climatology in XTN.  Here the native and analysis climos are
combined in one directory per climo:
     ncclimo -v FSNT,AODVIS -c caseid -s 1980 -e 1981 -i raw -o prv -r map.nc
     ncclimo -v FSNT,AODVIS -c caseid -s 1982 -e 1983 -i raw -o clm -r map.nc \
             -S 1980 -x prv -X xtn
   If the native and analysis grid climo directories are segregated,
then those directories must be specified, too:
     ncclimo -v FSNT,AODVIS -c caseid -s 1980 -e 1981 -i raw -o prv -O rgr_prv -r map.nc
     ncclimo -v FSNT,AODVIS -c caseid -s 1982 -e 1983 -i raw -o clm -O rgr -r map.nc \
             -S 1980 -x prv -X xtn -y rgr_prv -Y rgr_xtn

   'ncclimo' does not know whether a pre-computed climo is on a native
grid or an analysis grid, i.e., whether it has been regridded.  In
binary mode, 'ncclimo' may be pointed to two pre-computed native grid
climatologies, or to two pre-computed analysis grid climatologies.  In
other words, it is not necessary to maintain native grid climatologies
for use in creating extended climatologies.  It is sufficient to
generate climatologies on the analysis grid, and feed them to 'ncclimo'
in binary mode, without a mapping file:
     ncclimo -c caseid -S 1980 -E 1981 -x prv -s 1980 -e 1981 -i crr -o clm

Coupled Runs
------------

'ncclimo' works on all E3SM/ACME and CESM models.  It can simultaneously
generate climatologies for a coupled run, where climatologies mean both
native and regridded monthly, seasonal, and annual averages as per
E3SM/ACME specifications (which mandate the inclusion of certain helpful
metadata and provenance information).  Here are template commands for a
recent simulation:
     caseid=20160121.A_B2000ATMMOD.ne30_oEC.titan.a00
     drc_in=/scratch/simulations/$caseid/run
     drc_out=${DATA}/acme
     map_atm=${DATA}/maps/map_ne30np4_to_fv129x256_aave.20150901.nc
     map_lnd=$map_atm
     map_ocn=${DATA}/maps/map_oEC60to30_to_t62_bilin.20160301.nc
     map_ice=$map_ocn
     ncclimo -p mpi -c $caseid -m cam  -s 2 -e 5 -i $drc_in -r $map_atm -o $drc_out/atm
     ncclimo        -c $caseid -m clm2 -s 2 -e 5 -i $drc_in -r $map_lnd -o $drc_out/lnd
     ncclimo -p mpi -m mpaso           -s 2 -e 5 -i $drc_in -r $map_ocn -o $drc_out/ocn 
     ncclimo        -m mpascice        -s 2 -e 5 -i $drc_in -r $map_ice -o $drc_out/ice
   Atmosphere and ocean model output is typically larger than land and
ice model output.  These commands recognize that by using different
parallelization strategies that may ('rhea' standard queue) or may not
('cooley', or 'rhea''s 'bigmem' queue) be required, depending on the
fatness of the analysis nodes, as explained below.

Memory Considerations
---------------------

It is important to employ the optimal 'ncclimo' parallelization strategy
for your computer hardware resources.  Select from the three available
choices with the '-p PAR_TYP' switch.  The options are serial mode ('-p
srl', '-p serial', or '-p nil'), background mode parallelism ('-p bck',
or '-p background'), and MPI parallelism ('-p mpi' or '-p MPI').  The
default is background-mode parallelism.  This is appropriate for lower
resolution (e.g., ne30L30) simulations on most nodes at high-performance
computer centers.  Use (or at least start with) serial mode on personal
laptops/workstations.  Serial mode requires twelve times less RAM than
the parallel modes, and is much less likely to deadlock or cause OOM
(out-of-memory) conditions on your personal computer.  If the available
RAM (plus swap) is < 12*4*'sizeof('monthly input file')', then try
serial mode first (12 is the optimal number of parallel processes for
monthly climos, the computational overhead is a factor of four).  CAM-SE
ne30L30 output is about 1 GB/month so each month requires about 4 GB of
RAM.  CAM-SE ne30L72 output (with LINOZ) is about 10 GB/month so each
month requires about 40 GB RAM.  CAM-SE ne120 output is about
12 GB/month so each month requires about 48 GB RAM.  The computer does
not actually use all this memory at one time, and many kernels compress
RAM usage to below what top reports, so the actual physical usage is
hard to pin-down, but may be a factor of 2.5-3.0 (rather than a factor
of four) times the size of the input file.  For instance, my 16 GB 2014
MacBookPro successfully runs an ne30L30 climatology (that requests 48 GB
RAM) in background mode.  However the laptop is slow and unresponsive
for other uses until it finishes (in 6-8 minutes) the climos.
Experiment and choose the parallelization option that performs best.

   Serial-mode, as its name implies, uses one core at a time for climos,
and proceeds sequentially from months to seasons to annual
climatologies.  Serial mode means that climos are performed serially,
while regridding still employs OpenMP threading (up to 16 cores) on
platforms that support it.  By design each month and each season is
independent of the others, so all months can be computed in parallel,
then each season can be computed in parallel (using monthly
climatologies), from which annual average is computed.  Background
parallelization mode exploits this parallelism and executes the climos
in parallel as background processes on a single node, so that twelve
cores are simultaneously employed for monthly climatologies, four for
seasonal, and one for annual.  The optional regridding will employ, by
default, up to two cores per process.  The MPI parallelism mode executes
the climatologies on different nodes so that up to (optimally) twelve
nodes compute monthly climos.  The full memory of each node is available
for each individual climo.  The optional regridding employs, by default,
up to eight cores per node in MPI-mode.  MPI-mode or serial-mode must be
used to process ne30L72 and ne120L30 climos on all but the fattest DOE
nodes.  An ne120L30 climo in background mode on 'rhea' (i.e., on one
128 GB compute node) fails due to OOM.  (Unfortunately OOM errors do not
produce useful return codes so if your climo processes die without
printing useful information, the cause may be OOM).  However the same
climo in background-mode succeeds when executed on a single big-memory
(1 TB) node on 'rhea' (use '-lpartition=gpu', as shown below).  Or
MPI-mode can be used for any climatology.  The same ne120L30 climo will
also finish blazingly fast in background mode on 'cooley' (i.e., on one
384 GB compute node), so MPI-mode is unnecessary on 'cooley'.  In
general, the fatter the memory, the better the performance.

Single, Dedicated Nodes at LCFs
-------------------------------

The basic approach above (running the script from a standard terminal
window) that works well for small cases can be unpleasantly slow on
login nodes of LCFs and for longer or higher resolution (e.g., ne120)
climatologies.  As a baseline, generating a climatology of 5 years of
ne30 (~1x1 degree) CAM-SE output with 'ncclimo' takes 1-2 minutes on
'rhea' (at a time with little contention), and 6-8 minutes on a 2014
MacBook Pro.  To make things a bit faster at LCFs, request a dedicated
node (this only makes sense on supercomputers or clusters with
job-schedulers).  On 'rhea' or 'titan', which use the PBS scheduler, do
this with
     # Standard node (128 GB), PBS scheduler
     qsub -I -A CLI115 -V -l nodes=1 -l walltime=00:10:00 -N ncclimo
     # Bigmem node (1 TB), PBS scheduler
     qsub -I -A CLI115 -V -l nodes=1 -l walltime=00:10:00 -lpartition=gpu -N ncclimo
   The equivalent requests on 'cooley' or 'mira' (Cobalt scheduler) and
'cori' or 'titan' (SLURM scheduler) are:
     # Cooley node (384 GB) with Cobalt
     qsub -I -A HiRes_EarthSys --nodecount=1 --time=00:10:00 --jobname=ncclimo
     # Cori node (128 GB) with SLURM
     salloc  -A acme --nodes=1 --partition=debug --time=00:10:00 --job-name=ncclimo
Flags used and their meanings:
'-I'
     Submit in interactive mode.  This returns a new terminal shell
     rather than running a program.
'--time'
     How long to keep this dedicated node for.  Unless you kill the
     shell created by the 'qsub' command, the shell will exist for this
     amount of time, then die suddenly.  In the above examples,
     10 minutes is requested.
'-l nodes=1'
     PBS syntax (e.g., on 'rhea') for nodes.
'--nodecount 1'
     Cobalt syntax (e.g., on 'cooley') for nodes.
'--nodes=1'
     SLURM syntax (e.g., on 'cori' or 'edison') for nodes.  These
     scheduler-dependent variations request a quantity of nodes.
     Request 1 node for Serial or Background-mode, and up to 12 nodes
     for MPI-mode parallelism.  In all cases 'ncclimo' will use multiple
     cores per node if available.
'-V'
     Export existing environmental variables into the new interactive
     shell.  This may not actually be needed.
'-q name'
     Queue name.  This is needed for locations like 'edison' that have
     multiple queues with no default queue.
'-A'
     Name of account to charge for time used.
   Acquiring a dedicated node is useful for any workflow, not just
creating climos.  This command returns a prompt once nodes are assigned
(the prompt is returned in your home directory so you may then have to
'cd' to the location you meant to run from).  Then run your code with
the basic 'ncclimo' invocation.  The is faster because the node is
exclusively dedicated to 'ncclimo'.  Again, ne30L30 climos only require
< 2 minutes, so the 10 minutes requested in the example is excessive and
conservative.  Tune it with experience.

12 node MPI-mode Jobs
---------------------

The above parallel approaches will fail when a single node lacks enough
RAM (plus swap) to store all twelve monthly input files, plus extra RAM
for computations.  One should employ MPI multinode parallelism '-p mpi'
on nodes with less RAM than 12*3*'sizeof('input')'.  The longest an
ne120 climo will take is less than half an hour (~25 minutes on 'edison'
or 'rhea'), so the simplest method to run MPI jobs is to request
12-interactive nodes using the above commands (though remember to add
'-p mpi'), then execute the script at the command line.

   It is also possible, and sometimes preferable, to request
non-interactive compute nodes in a batch queue.  Executing an MPI-mode
climo (on machines with job scheduling and, optimally, 12 nodes) in a
batch queue can be done in two commands.  First, write an executable
file which calls the 'ncclimo' script with appropriate arguments.  We do
this below by echoing to a file, 'ncclimo.pbs'.
     echo "ncclimo -p mpi -c $caseid -s 1 -e 20 -i $drc_in -o $drc_out" > ncclimo.pbs
   The only new argument here is '-p mpi' that tells 'ncclimo' to use
MPI parallelism.  Then execute this command file with a 12 node
non-interactive job:
     qsub -A CLI115 -V -l nodes=12 -l walltime=00:30:00 -j oe -m e -N ncclimo \
          -o ncclimo.out ncclimo.pbs
   This script adds new flags: '-j oe' (combine output and error streams
into standard error), '-m e' (send email to the job submitter when the
job ends), '-o ncclimo.out' (write all output to 'ncclimo.out').  The
above commands are meant for PBS schedulers like on 'rhea'.  Equivalent
commands for 'cooley'/'mira' (Cobalt) and 'cori'/'edison' (SLURM) are
     # Cooley (Cobalt scheduler)
     /bin/rm -f ncclimo.err ncclimo.out
     echo '#!/bin/bash' > ncclimo.cobalt
     echo "ncclimo -p mpi -c $caseid -s 1 -e 20 -i $drc_in -o $drc_out" >> ncclimo.cobalt
     chmod a+x ncclimo.cobalt
     qsub -A HiRes_EarthSys --nodecount=12 --time=00:30:00 --jobname ncclimo \
          --error ncclimo.err --output ncclimo.out --notify zender@uci.edu ncclimo.cobalt
     
     # Cori/Edison (SLURM scheduler)
     echo "ncclimo -p mpi -c $caseid -s 1 -e 20 -i $drc_in -o $drc_out -r $map_fl" \
           > ncclimo.pbs
     chmod a+x ncclimo.slurm
     sbatch -A acme --nodes=12 --time=03:00:00 --partition=regular --job-name=ncclimo \
            --mail-type=END --error=ncclimo.err --output=ncclimo.out ncclimo.slurm
   Notice that Cobalt and SLURM require the introductory
shebang-interpreter line ('#!/bin/bash') which PBS does not need.  Set
only the scheduler batch queue parameters mentioned above.  In MPI-mode,
'ncclimo' determines the appropriate number of tasks-per-node based on
the number of nodes available and script internals (like load-balancing
for regridding).  Hence do not set a tasks-per-node parameter with
scheduler configuration parameters as this could cause conflicts.

What does 'ncclimo' do?
-----------------------

For monthly climatologies (e.g., JAN), 'ncclimo' passes the list of all
relevant January monthly files to NCO's 'ncra' command, which averages
each variable in these monthly files over their time-dimension (if it
exists) or copies the value from the first month unchanged (if no
time-axis exists).  Seasonal climos are then created by taking the
average of the monthly climo files using 'ncra'.  To account for
differing numbers of days per month, the 'ncra' '-w' flag is used,
followed by the number of days in the relevant months.  For example, the
MAM climo is computed with 'ncra -w 31,30,31 MAR_climo.nc APR_climo.nc
MAY_climo.nc MAM_climo.nc' (details about file names and other
optimization flags have been stripped here to make the concept easier to
follow).  The annual (ANN) climo is then computed as a weighted average
of the seasonal climos.

Assumptions, Approximations, and Algorithms (AAA) Employed:
-----------------------------------------------------------

A climatology embodies many algorithmic choices, and regridding from the
native to the analysis grid involves still more choices.  A separate
method should reproduce the 'ncclimo' and NCO answers to round-off
precision if it implements the same algorithmic choices.  For example,
'ncclimo' agrees to round-off with AMWG diagnostics when making the same
(sometimes questionable) choices.  The most important choices have to do
with converting single- to double-precision (SP and DP, respectively),
treatment of missing values, and generation/application of regridding
weights.  For concreteness and clarity we describe the algorithmic
choices made in processing a CAM-SE monthly output into a climatological
annual mean (ANN) and then regridding that.  Other climatologies (e.g.,
daily to monthly, or annual-to-climatological) involve similar choices.

   E3SM/ACME (and CESM) computes fields in DP and outputs history (not
restart) files as monthly means in SP.  The NCO climatology generator
('ncclimo') processes these data in four stages.  Stage N accesses input
only from stage N-1, never from stage N-2 or earlier.  Thus the
(on-disk) files from stage N determine the highest precision achievable
by stage N+1.  The general principal is to perform math (addition,
weighting, normalization) in DP and output results to disk in the same
precision in which they were input from disk (usually SP).  In Stage 1,
NCO ingests Stage 0 monthly means (raw CAM-SE output), converts SP input
to DP, performs the average across all years, then converts the answer
from DP to SP for storage on-disk as the climatological monthly mean.
In Stage 2, NCO ingests Stage 1 climatological monthly means, converts
SP input to DP, performs the average across all months in the season
(e.g., DJF), then converts the answer from DP to SP for storage on-disk
as the climatological seasonal mean.  In Stage 3, NCO ingests Stage 2
climatological seasonal means, converts SP input to DP, performs the
average across all four seasons (DJF, MAM, JJA, SON), then converts the
answer from DP to SP for storage on-disk as the climatological annual
mean.

   Stage 2 weights each input month by its number of days (e.g., 31 for
January), and Stage 3 weights each input season by its number of days
(e.g., 92 for MAM).  E3SM/ACME runs CAM-SE with a 365-day calendar, so
these weights are independent of year and never change.  The treatment
of missing values in Stages 1-3 is limited by the lack of missing value
tallies provided by Stage 0 (model) output.  Stage 0 records a value as
missing if it is missing for the entire month, and present if the value
is valid for one or more timesteps.  Stage 0 does not record the missing
value tally (number of valid timesteps) for each spatial point.  Thus a
point with a single valid timestep during a month is weighted the same
in Stages 1-4 as a point with 100% valid timesteps during the month.
The absence of tallies inexorably degrades the accuracy of subsequent
statistics by an amount that varies in time and space.  On the positive
side, it reduces the output size (by a factor of two) and complexity of
analyzing fields that contain missing values.  Due to the ambiguous
nature of missing values, it is debatable whether they merit efforts to
treat them more exactly.

   The vast majority of fields undergo three promotion/demotion cycles
between CAM-SE and ANN.  No promotion/demotion cycles occur for history
fields that CAM-SE outputs in DP rather than SP, nor for fields without
a time dimension.  Typically these fields are grid coordinates (e.g.,
longitude, latitude) or model constants (e.g., CO2 mixing ratio).  NCO
never performs any arithmetic on grid coordinates or non-time-varying
input, regardless of whether they are SP or DP.  Instead, NCO copies
these fields directly from the first input file.  Stage 4 uses a mapfile
to regrid climos from the native to the desired analysis grid.
E3SM/ACME currently uses mapfiles generated by 'ESMF_RegridWeightGen'
(ERWG) and by TempestRemap.

   The algorithmic choices, approximations, and commands used to
generate mapfiles from input gridfiles are separate issues.  We mention
only some of these issues here for brevity.  Input gridfiles used by
E3SM/ACME until ~20150901, and by CESM (then and currently, at least for
Gaussian grids) contained flaws that effectively reduced their
precision, especially at regional scales, and especially for Gaussian
grids.  E3SM/ACME (and CESM) mapfiles continue to approximate grids as
connected by great circles, whereas most analysis grids (and some
models) use great circles for longitude and small circles for latitude.
The great circle assumption may be removed in the future.  Constraints
imposed by ERWG during weight-generation ensure that global integrals of
fields undergoing conservative regridding are exactly conserved.

   Application of weights from the mapfile to regrid the native data to
the analysis grid is straightforward.  Grid fields (e.g., latitude,
longitude, area) are not regridded.  Instead they are copied (and area
is reconstructed if absent) directly from the mapfile.  NCO ingests all
other native grid (source) fields, converts SP to DP, and accumulates
destination gridcell values as the sum of the DP weight (from the sparse
matrix in the mapfile) times the (usually SP-promoted-to-DP) source
values.  Fields without missing values are then stored to disk in their
original precision.  Fields with missing values are treated (by default)
with what NCO calls the "conservative" algorithm.  This algorithm uses
all valid data from the source grid on the destination grid once and
only once.  Destination cells receive the weighted valid values of the
source cells.  This is conservative because the global integrals of the
source and destination fields are equal.  See *note ncremap netCDF
Remapper:: for more description of the conservative and of the optional
("renormalized") algorithm.

EXAMPLES

   How to create a climo from a collection of monthly non-CESM'ish
files?  This is a two-step procedure: First be sure the names are
arranged with a YYYYMM-format date preceding the suffix (usually '.nc').
Then give _any_ monthly input filename to 'ncclimo'.  Consider the
MERRA2 collection, for example.  As retrieved from NASA, MERRA2 files
have names like 'svc_MERRA2_300.tavgM_2d_aer_Nx.200903.nc4'.  While the
sub-string '200903' is easy to recognize as a month in YYYYMM format,
other parts (specifically the '300' code) of the filename also change
with date.  We can use Bash regular expressions to extract dates and
create symbolic links to simpler filenames with regularly patterned
YYYYMM strings like 'merra2_200903.nc4':
     for fl in `ls *.nc4` ; do
     # Convert svc_MERRA2_300.tavgM_2d_aer_Nx.YYYYMM.nc4 to merra2_YYYYMM.nc4
         sfx_out=`expr match "${fl}" '.*_Nx.\(.*.nc4\)'`
         fl_out="merra2_${sfx_out}"
         ln -s ${fl} ${fl_out}
     done
   Then call 'ncclimo' with 'merra2_200903.nc4' as CASEID:
     ncclimo -c merra2_200903.nc4 -s 1980 -e 2016 -i $drc_in -o $drc_out

   In the default monthly climo generation mode, 'ncclimo' expects each
input file to contain one single record that is the monthly average of
all fields.  Another example of of wrangling observed datasets into a
CESMish format is ECMWF Integrated Forecasting System (IFS) output that
contains twelve months per file, rather than the one month per file that
'ncclimo' expects.
     for yr in {1979..2016}; do
     # Convert ifs_YYYY01-YYYY12.nc to ifs_YYYYMM.nc
         yyyy=`printf "%04d" $yr`
         for mth in {1..12}; do
             mm=`printf "%02d" $mth`
             ncks -O -F -d time,${mth} ifs_${yyyy}01-${yyyy}12.nc ifs_${yyyy}${mm}.nc
         done
     done
   Then call 'ncclimo' with 'ifs_197901.nc' as CASEID:
     ncclimo -c ifs_197901.nc -s 1979 -e 2016 -i $drc_in -o $drc_out
   ncclimo does not recognize all combinations imaginable of records per
file and files per year.  However, support can be added for the most
prevalent combinations so that ncclimo, rather than the user, does any
necessary data wrangling.  Contact us if there is a common input data
format you would like supported as a custom option.

   Often one wishes to create a climatology of a single variable.  The
'-f FML_NM' option to 'ncclimo' makes this easy.  Consider a series of
single-variable climos for the fields 'FSNT', and 'FLNT'
     ncclimo -v FSNT -f FSNT -c amip_xpt -s 1980 -e 1983 -i drc_in -o drc_out
     ncclimo -v FLNT -f FLNT -c amip_xpt -s 1980 -e 1983 -i drc_in -o drc_out
   These climos use the '-f' option and so their output files will have
no namespace conflicts.  Moreover, the climatologies can be generated in
parallel.

   ---------- Footnotes ----------

   (1) This means that newer (including user-modified) versions of
'ncclimo' work fine without re-compiling NCO.  Re-compiling is only
necessary to take advantage of new features or fixes in the NCO
binaries, not to improve 'ncclimo'.  One may download and give
executable permissions to the latest source at
<https://github.com/nco/nco/tree/master/data/ncclimo> without
re-installing the rest of NCO.

   (2) At least one known environment (the E3SM-Unified Anaconda
environment at NERSC) prevents users from spawning scores of processes
and may report OpenBLAS/pthread or 'RLIMIT_NPROC'-related errors.  A
solution seems to be executing 'ulimit -u unlimited'


File: nco.info,  Node: ncecat netCDF Ensemble Concatenator,  Next: nces netCDF Ensemble Statistics,  Prev: ncclimo netCDF Climatology Generator,  Up: Reference Manual

4.5 'ncecat' netCDF Ensemble Concatenator
=========================================

SYNTAX
     ncecat [-3] [-4] [-5] [-6] [-7] [-A] [-C] [-c]
     [--cnk_byt SZ_BYT] [--cnk_csh SZ_BYT] [--cnk_dmn NM,SZ_LMN]
     [--cnk_map MAP] [--cnk_min SZ_BYT] [--cnk_plc PLC] [--cnk_scl SZ_LMN]
     [-D DBG] [-d DIM,[MIN][,[MAX][,[STRIDE]]] [-F] [--fl_fmt FL_FMT]
     [-G GPE_DSC] [-g GRP[,...]] [--gag] [--glb ...]
     [-h] [--hdf] [--hdr_pad NBR] [--hpss]
     [-L DFL_LVL] [-l PATH] [-M] [--md5_digest] [--mrd] [-n LOOP]
     [--no_cll_msr] [--no_frm_trm] [--no_tmp_fl]
     [-O] [-o OUTPUT-FILE] [-p PATH] [--ppc ...] [-R] [-r] [--ram_all]
     [-t THR_NBR] [-u ULM_NM] [--unn] [-v VAR[,...]] [-X ...] [-x]
     [INPUT-FILES] [OUTPUT-FILE]

DESCRIPTION

   'ncecat' aggregates an arbitrary number of input files into a single
output file using using one of two methods.  "Record AGgregation" (RAG),
the traditional method employed on (flat) netCDF3 files and still the
default method, stores INPUT-FILES as consecutive records in the
OUTPUT-FILE.  "Group AGgregation" (GAG) stores INPUT-FILES as top-level
groups in the netCDF4 OUTPUT-FILE.  Record Aggregation (RAG) makes
numerous assumptions about the structure of input files whereas Group
Aggregation (GAG) makes none.  Both methods are described in detail
below.  Since 'ncecat' aggregates all the contents of the input files,
it can easily produce large output files so it is often helpful to
invoke subsetting simultaneously (*note Subsetting Files::).

   RAG makes each variable (except coordinate variables) in each input
file into a single record of the same variable in the output file.
Coordinate variables are not concatenated, they are instead simply
copied from the first input file to the OUTPUT-FILE.  All INPUT-FILES
must contain all extracted variables (or else there would be "gaps" in
the output file).

   A new record dimension is the glue which binds together the input
file data.  The new record dimension is defined in the root group of the
output file so it is visible to all sub-groups.  Its name is, by
default, "record".  This default name can be overridden with the '-u
ULM_NM' short option (or the '--ulm_nm' or 'rcd_nm' long options).

   Each extracted variable must be constant in size and rank across all
INPUT-FILES.  The only exception is that 'ncecat' allows files to differ
in the record dimension size if the requested record hyperslab (*note
Hyperslabs::) resolves to the same size for all files.  This allows
easier gluing/averaging of unequal length timeseries from simulation
ensembles (e.g., the CMIP archive).

   Classic (i.e., all netCDF3 and 'NETCDF4_CLASSIC') output files can
contain only one record dimension.  'ncecat' makes room for the new glue
record dimension by changing the pre-existing record dimension, if any,
in the input files into a fixed dimension in the output file.  netCDF4
output files may contain any number of record dimensions, so 'ncecat'
need not and does not alter the record dimensions, if any, of the input
files as it copies them to the output file.

   "Group AGgregation" (GAG) stores INPUT-FILES as top-level groups in
the OUTPUT-FILE.  No assumption is made about the size or shape or type
of a given object (variable or dimension or group) in the input file.
The entire contents of the extracted portion of each input file is
placed in its own top-level group in OUTPUT-FILE, which is automatically
made as a netCDF4-format file.

   GAG has two methods to specify group names for the OUTPUT-FILE.  The
'-G' option, or its long-option equivalent '--gpe', takes as argument a
group path editing description GPE_DSC of where to place the results.
Each input file needs a distinct output group name to avoid namespace
conflicts in the OUTPUT-FILE.  Hence 'ncecat' automatically creates
unique output group names based on either the input filenames or the
GPE_DSC arguments.  When the user provides GPE_DSC (i.e., with '-G'),
then the output groups are formed by enumerating sequential two-digit
numeric suffixes starting with zero, and appending them to the specified
group path (*note Group Path Editing::).  When GPE_DSC is not provided
(i.e., user requests GAG with '--gag' instead of '-G'), then 'ncecat'
forms the output groups by stripping the input file name of any
type-suffix (e.g., '.nc'), and all but the final component of the full
filename.
     ncecat --gag 85.nc 86.nc 87.nc 8587.nc # Output groups 85, 86, 87
     ncecat -G 85_ a.nc b.nc c.nc 8589.nc # Output groups 85_00, 85_01, 85_02
     ncecat -G 85/ a.nc b.nc c.nc 8589.nc # Output groups 85/00, 85/01, 85/02

   With both RAG and GAG the OUTPUT-FILE size is the sum of the sizes of
the extracted variables in the input files.  *Note Statistics vs.
Concatenation::, for a description of the distinctions between the
various statistics tools and concatenators.  As a multi-file operator,
'ncecat' will read the list of INPUT-FILES from 'stdin' if they are not
specified as positional arguments on the command line (*note Large
Numbers of Files::).

   Suppress global metadata copying.  By default NCO's multi-file
operators copy the global metadata from the first input file into
OUTPUT-FILE.  This helps to preserve the provenance of the output data.
However, the use of metadata is burgeoning and sometimes one encounters
files with excessive amounts of extraneous metadata.  Extracting small
bits of data from such files leads to output files which are much larger
than necessary due to the automatically copied metadata.  'ncecat'
supports turning off the default copying of global metadata via the '-M'
switch (or its long option equivalents, '--no_glb_mtd' and
'--suppress_global_metadata').

   Consider five realizations, '85a.nc', '85b.nc', ... '85e.nc' of 1985
predictions from the same climate model.  Then 'ncecat 85?.nc 85_ens.nc'
glues together the individual realizations into the single file,
'85_ens.nc'.  If an input variable was dimensioned ['lat','lon'], it
will by default have dimensions ['record','lat','lon'] in the output
file.  A restriction of 'ncecat' is that the hyperslabs of the processed
variables must be the same from file to file.  Normally this means all
the input files are the same size, and contain data on different
realizations of the same variables.

   Concatenating a variable packed with different scales across multiple
datasets is beyond the capabilities of 'ncecat' (and 'ncrcat', the other
concatenator (*note Concatenation::).  'ncecat' does not unpack data, it
simply _copies_ the data from the INPUT-FILES, and the metadata from the
_first_ INPUT-FILE, to the OUTPUT-FILE.  This means that data compressed
with a packing convention must use the identical packing parameters
(e.g., 'scale_factor' and 'add_offset') for a given variable across
_all_ input files.  Otherwise the concatenated dataset will not unpack
correctly.  The workaround for cases where the packing parameters differ
across INPUT-FILES requires three steps: First, unpack the data using
'ncpdq'.  Second, concatenate the unpacked data using 'ncecat', Third,
re-pack the result with 'ncpdq'.

EXAMPLES

   Consider a model experiment which generated five realizations of one
year of data, say 1985.  You can imagine that the experimenter slightly
perturbs the initial conditions of the problem before generating each
new solution.  Assume each file contains all twelve months (a seasonal
cycle) of data and we want to produce a single file containing all the
seasonal cycles.  Here the numeric filename suffix denotes the
experiment number (_not_ the month):
     ncecat 85_01.nc 85_02.nc 85_03.nc 85_04.nc 85_05.nc 85.nc
     ncecat 85_0[1-5].nc 85.nc
     ncecat -n 5,2,1 85_01.nc 85.nc
These three commands produce identical answers.  *Note Specifying Input
Files::, for an explanation of the distinctions between these methods.
The output file, '85.nc', is five times the size as a single INPUT-FILE.
It contains 60 months of data.

   One often prefers that the (new) record dimension have a more
descriptive, context-based name than simply "record".  This is easily
accomplished with the '-u ULM_NM' switch.  To add a new record dimension
named "time" to all variables
     ncecat -u time in.nc out.nc
   To glue together multiple files with a new record variable named
"realization"
     ncecat -u realization 85_0[1-5].nc 85.nc
Users are more likely to understand the data processing history when
such descriptive coordinates are used.

   Consider a file with an existing record dimension named 'time'.  and
suppose the user wishes to convert 'time' from a record dimension to a
non-record dimension.  This may be useful, for example, when the user
has another use for the record variable.  The simplest method is to use
'ncks --fix_rec_dmn', and another possibility is to use 'ncecat'
followed by 'ncwa':
     ncecat in.nc out.nc # Convert time to non-record dimension
     ncwa -a record in.nc out.nc # Remove new degenerate record dimension
The second step removes the degenerate record dimension.  See *note
ncpdq netCDF Permute Dimensions Quickly:: and *note ncks netCDF Kitchen
Sink:: for other methods of of changing variable dimensionality,
including the record dimension.


File: nco.info,  Node: nces netCDF Ensemble Statistics,  Next: ncflint netCDF File Interpolator,  Prev: ncecat netCDF Ensemble Concatenator,  Up: Reference Manual

4.6 'nces' netCDF Ensemble Statistics
=====================================

SYNTAX
     nces [-3] [-4] [-5] [-6] [-7] [-A] [-C] [-c]
     [--cnk_byt SZ_BYT] [--cnk_csh SZ_BYT] [--cnk_dmn NM,SZ_LMN]
     [--cnk_map MAP] [--cnk_min SZ_BYT] [--cnk_plc PLC] [--cnk_scl SZ_LMN]
     [-D DBG] [-d DIM,[MIN][,[MAX][,[STRIDE]]] [-F]
     [-G GPE_DSC] [-g GRP[,...]] [--glb ...]
     [-h] [--hdf] [--hdr_pad NBR] [--hpss]
     [-L DFL_LVL] [-l PATH] [-n LOOP]
     [--no_cll_msr] [--no_frm_trm] [--no_tmp_fl] [--nsm_fl|grp] [--nsm_sfx sfx]
     [-O] [-o OUTPUT-FILE] [-p PATH] [--ppc ...] [-R] [-r] [--ram_all] [--rth_dbl|flt]
     [-t THR_NBR] [--unn] [-v VAR[,...]] [-X ...] [-x] [-y OP_TYP]
     [INPUT-FILES] [OUTPUT-FILE]

DESCRIPTION

   'nces' performs gridpoint statistics (including, but not limited to,
averages) on variables across an arbitrary number (an "ensemble") of
INPUT-FILES and/or of input groups within each file.  Each file (or
group) receives an equal weight.  'nces' was formerly (until NCO version
4.3.9, released December, 2013) known as 'ncea' (netCDF Ensemble
Averager)(1).  For example, 'nces' will average a set of files or
groups, weighting each file or group evenly.  This is distinct from
'ncra', which performs statistics only over the record dimension(s)
(e.g., TIME), and weights each record in each record dimension evenly.

   The file or group is the logical unit of organization for the results
of many scientific studies.  Often one wishes to generate a file or
group which is the statistical product (e.g., average) of many separate
files or groups.  This may be to reduce statistical noise by combining
the results of a large number of experiments, or it may simply be a step
in a procedure whose goal is to compute anomalies from a mean state.  In
any case, when one desires to generate a file whose statistical
properties are equally influenced by all the inputs, then 'nces' is the
operator to use.

   Variables in the OUTPUT-FILE are the same size as the variable
hyperslab in each input file or group, and each input file or group must
be the same size after hyperslabbing (2) 'nces' does allow files to
differ in the record dimension size if the requested record hyperslab
(*note Hyperslabs::) resolves to the same size for all files.  'nces'
recomputes the record dimension hyperslab limits for each input file so
that coordinate limits may be used to select equal length timeseries
from unequal length files.  This simplifies analysis of unequal length
timeseries from simulation ensembles (e.g., the CMIP3 IPCC AR4 archive).

   'nces' works in one of two modes, file ensembles or group ensembles.
File ensembles are the default (equivalent to the old 'ncea') and may
also be explicitly specified by the '--nsm_fl' or '--ensemble_file'
switches.  To perform statistics on ensembles of groups, a newer
feature, use '--nsm_grp' or '--ensemble_group'.  Members of a group
ensemble are groups that share the same structure, parent group, and
nesting level.  Members must be "leaf groups", i.e., not contain any
sub-groups.  Their contents usually have different values because they
are realizations of replicated experiments.  In group ensemble mode
'nces' computes the statistics across the ensemble, which may span
multiple input files.  Files may contain members of multiple, distinct
ensembles.  However, all ensembles must have at least one member in the
first input file.  Group ensembles behave as an unlimited dimension of
datasets: they may contain an arbitrary and extensible number of
realizations in each file, and may be composed from multiple files.

   Output statistics in group ensemble mode are stored in the parent
group by default.  If the ensemble members are '/cesm/cesm_01' and
'/cesm/cesm_02', then the computed statistic will be in '/cesm' in the
output file.  The '--nsm_sfx' option instructs nces to instead store
output in a new child group of the parent created by attaching the
suffix to the parent group's name, e.g., '--nsm_sfx='_avg'' would store
results in the output group '/cesm/cesm_avg':
     nces --nsm_grp                  mdl1.nc mdl2.nc mdl3.nc out.nc
     nces --nsm_grp --nsm_sfx='_avg' mdl1.nc mdl2.nc mdl3.nc out.nc

   *Note Statistics vs. Concatenation::, for a description of the
distinctions between the statistics tools and concatenators.  As a
multi-file operator, 'nces' will read the list of INPUT-FILES from
'stdin' if they are not specified as positional arguments on the command
line (*note Large Numbers of Files::).

   Like 'ncra' and 'ncwa', 'nces' treats coordinate variables as a
special case.  Coordinate variables are assumed to be the same in all
ensemble members, so 'nces' simply copies the coordinate variables that
appear in ensemble members directly to the output file.  This has the
same effect as averaging the coordinate variable across the ensemble,
yet does not incur the time- or precision- penalties of actually
averaging them.  'ncra' and 'ncwa' allow coordinate variables to be
processed only by the linear average operation, regardless of the
arithmetic operation type performed on the non-coordinate variables
(*note Operation Types::).  Thus it can be said that the three operators
('ncra', 'ncwa', and 'nces') all average coordinate variables (even
though 'nces' simply copies them).  All other requested arithmetic
operations (e.g., maximization, square-root, RMS) are applied only to
non-coordinate variables.  In these cases the linear average of the
coordinate variable will be returned.

EXAMPLES

   Consider a model experiment which generated five realizations of one
year of data, say 1985.  Imagine that the experimenter slightly perturbs
the initial conditions of the problem before generating each new
solution.  Assume each file contains all twelve months (a seasonal
cycle) of data and we want to produce a single file containing the
ensemble average (mean) seasonal cycle.  Here the numeric filename
suffix denotes the realization number (_not_ the month):
     nces 85_01.nc 85_02.nc 85_03.nc 85_04.nc 85_05.nc 85.nc
     nces 85_0[1-5].nc 85.nc
     nces -n 5,2,1 85_01.nc 85.nc
These three commands produce identical answers.  *Note Specifying Input
Files::, for an explanation of the distinctions between these methods.
The output file, '85.nc', is the same size as the inputs files.  It
contains 12 months of data (which might or might not be stored in the
record dimension, depending on the input files), but each value in the
output file is the average of the five values in the input files.

   In the previous example, the user could have obtained the ensemble
average values in a particular spatio-temporal region by adding a
hyperslab argument to the command, e.g.,
     nces -d time,0,2 -d lat,-23.5,23.5 85_??.nc 85.nc
In this case the output file would contain only three slices of data in
the TIME dimension.  These three slices are the average of the first
three slices from the input files.  Additionally, only data inside the
tropics is included.

   As of NCO version 4.3.9 (released December, 2013) 'nces' also works
with groups (rather than files) as the fundamental unit of the ensemble.
Consider two ensembles, '/ecmwf' and '/cesm' stored across three input
files 'mdl1.nc', 'mdl2.nc', and 'mdl3.nc'.  Ensemble members would be
leaf groups with names like '/ecmwf/01', '/ecmwf/02' etc.  and
'/cesm/01', '/cesm/02', etc.  These commands average both ensembles:
     nces --nsm_grp mdl1.nc mdl2.nc mdl3.nc out.nc
     nces --nsm_grp --nsm_sfx='_min' --op_typ=min -n 3,1,1 mdl1.nc out.nc
     nces --nsm_grp -g cesm -v tas -d time,0,3 -n 3,1,1 mdl1.nc out.nc
   The first command stores averages in the output groups '/cesm' and
'/ecmwf', while the second stores minima in the output groups
'/cesm/cesm_min' and '/ecmwf/ecmwf_min': The third command demonstrates
that sub-setting and hyperslabbing work as expected.  Note that each
input file may contain different numbers of members of each ensemble, as
long as all distinct ensembles contain at least one member in the first
file.

   ---------- Footnotes ----------

   (1) The old ncea command was deprecated in NCO version 4.3.9,
released December, 2013.  NCO will attempt to maintain
back-compatibility and work as expected with invocations of 'ncea' for
as long as possible.  Please replace 'ncea' by 'nces' in all future
work.

   (2) As of NCO version 4.4.2 (released February, 2014) 'nces' allows
hyperslabs in all dimensions so long as the hyperslabs resolve to the
same size.  The fixed (i.e., non-record) dimensions should be the same
size in all ensemble members both before and after hyperslabbing,
although the hyperslabs may (and usually do) change the size of the
dimensions from the input to the output files.  Prior to this, 'nces'
was only guaranteed to work on hyperslabs in the record dimension that
resolved to the same size.


File: nco.info,  Node: ncflint netCDF File Interpolator,  Next: ncks netCDF Kitchen Sink,  Prev: nces netCDF Ensemble Statistics,  Up: Reference Manual

4.7 'ncflint' netCDF File Interpolator
======================================

SYNTAX
     ncflint [-3] [-4] [-5] [-6] [-7] [-A] [-C] [-c]
     [--cnk_byt SZ_BYT] [--cnk_csh SZ_BYT] [--cnk_dmn NM,SZ_LMN]
     [--cnk_map MAP] [--cnk_min SZ_BYT] [--cnk_plc PLC] [--cnk_scl SZ_LMN]
     [-D DBG] [-d DIM,[MIN][,[MAX][,[STRIDE]]] [--fl_fmt FL_FMT]
     [-F] [--fix_rec_crd] [-G GPE_DSC] [-g GRP[,...]] [--glb ...]
     [-h] [--hdr_pad NBR] [--hpss]
     [-i VAR,VAL3] [-L DFL_LVL] [-l PATH] [-N]
     [--no_cll_msr] [--no_frm_trm] [--no_tmp_fl]
     [-O] [-o FILE_3] [-p PATH] [--ppc ...] [-R] [-r] [--ram_all]
     [-t THR_NBR] [--unn] [-v VAR[,...]] [-w WGT1[,WGT2]] [-X ...] [-x]
     FILE_1 FILE_2 [FILE_3]

DESCRIPTION

   'ncflint' creates an output file that is a linear combination of the
input files.  This linear combination is a weighted average, a
normalized weighted average, or an interpolation of the input files.
Coordinate variables are not acted upon in any case, they are simply
copied from FILE_1.

   There are two conceptually distinct methods of using 'ncflint'.  The
first method is to specify the weight each input file contributes to the
output file.  In this method, the value VAL3 of a variable in the output
file FILE_3 is determined from its values VAL1 and VAL2 in the two input
files according to VAL3 = WGT1*VAL1 + WGT2*VAL2 .  Here at least WGT1,
and, optionally, WGT2, are specified on the command line with the '-w'
(or '--weight' or '--wgt_var') switch.  If only WGT1 is specified then
WGT2 is automatically computed as WGT2 = 1 - WGT1.  Note that weights
larger than 1 are allowed.  Thus it is possible to specify WGT1 = 2 and
WGT2 = -3.  One can use this functionality to multiply all values in a
given file by a constant.

   As of NCO version 4.6.1 (July, 2016), the '-N' switch (or long-option
equivalents '--nrm' or '--normalize') implements a variation of this
method.  This switch instructs 'ncflint' to internally normalize the two
supplied (or one supplied and one inferred) weights so that WGT1 =
WGT1/(WGT1 + WGT2 and WGT2 = WGT2/(WGT1 + WGT2 and .  This allows the
user to input integral weights, say, and to delegate the chore of
normalizing them to 'ncflint'.  Be careful that '-N' means what you
think, since the same switch means something quite different in 'ncwa'.

   The second method of using 'ncflint' is to specify the interpolation
option with '-i' (or with the '--ntp' or '--interpolate' long options).
This is the inverse of the first method in the following sense: When the
user specifies the weights directly, 'ncflint' has no work to do besides
multiplying the input values by their respective weights and adding
together the results to produce the output values.  It makes sense to
use this when the weights are known _a priori_.

   Another class of problems has the "arrival value" (i.e., VAL3) of a
particular variable VAR known _a priori_.  In this case, the implied
weights can always be inferred by examining the values of VAR in the
input files.  This results in one equation in two unknowns, WGT1 and
WGT2: VAL3 = WGT1*VAL1 + WGT2*VAL2 .  Unique determination of the
weights requires imposing the additional constraint of normalization on
the weights: WGT1 + WGT2 = 1.  Thus, to use the interpolation option,
the user specifies VAR and VAL3 with the '-i' option.  'ncflint' then
computes WGT1 and WGT2, and uses these weights on all variables to
generate the output file.  Although VAR may have any number of
dimensions in the input files, it must represent a single, scalar value.
Thus any dimensions associated with VAR must be "degenerate", i.e., of
size one.

   If neither '-i' nor '-w' is specified on the command line, 'ncflint'
defaults to weighting each input file equally in the output file.  This
is equivalent to specifying '-w 0.5' or '-w 0.5,0.5'.  Attempting to
specify both '-i' and '-w' methods in the same command is an error.

   'ncflint' does not interpolate variables of type 'NC_CHAR' and
'NC_STRING'.  This behavior is hardcoded.

   By default 'ncflint' interpolates or multiplies record coordinate
variables (e.g., time is often stored as a record coordinate) not other
coordinate variables (e.g., latitude and longitude).  This is because
'ncflint' is often used to time-interpolate between existing files, but
is rarely used to spatially interpolate.  Sometimes however, users wish
to multiply entire files by a constant that does not multiply any
coordinate variables.  The '--fix_rec_crd' switch was implemented for
this purpose in NCO version 4.2.6 (March, 2013).  It prevents 'ncflint'
from multiplying or interpolating any coordinate variables, including
record coordinate variables.

   Depending on your intuition, 'ncflint' may treat missing values
unexpectedly.  Consider a point where the value in one input file, say
VAL1, equals the missing value MSS_VAL_1 and, at the same point, the
corresponding value in the other input file VAL2 is not misssing (i.e.,
does not equal MSS_VAL_2).  There are three plausible answers, and this
creates ambiguity.

   Option one is to set VAL3 = MSS_VAL_1.  The rationale is that
'ncflint' is, at heart, an interpolator and interpolation involving a
missing value is intrinsically undefined.  'ncflint' currently
implements this behavior since it is the most conservative and least
likely to lead to misinterpretation.

   Option two is to output the weighted valid data point, i.e., VAL3 =
WGT2*VAL2 .  The rationale for this behavior is that interpolation is
really a weighted average of known points, so 'ncflint' should weight
the valid point.

   Option three is to return the _unweighted_ valid point, i.e., VAL3 =
VAL2.  This behavior would appeal to those who use 'ncflint' to estimate
data using the closest available data.  When a point is not bracketed by
valid data on both sides, it is better to return the known datum than no
datum at all.

   The current implementation uses the first approach, Option one.  If
you have strong opinions on this matter, let us know, since we are
willing to implement the other approaches as options if there is enough
interest.

EXAMPLES

   Although it has other uses, the interpolation feature was designed to
interpolate FILE_3 to a time between existing files.  Consider input
files '85.nc' and '87.nc' containing variables describing the state of a
physical system at times 'time' = 85 and 'time' = 87.  Assume each file
contains its timestamp in the scalar variable 'time'.  Then, to linearly
interpolate to a file '86.nc' which describes the state of the system at
time at 'time' = 86, we would use
     ncflint -i time,86 85.nc 87.nc 86.nc

   Say you have observational data covering January and April 1985 in
two files named '85_01.nc' and '85_04.nc', respectively.  Then you can
estimate the values for February and March by interpolating the existing
data as follows.  Combine '85_01.nc' and '85_04.nc' in a 2:1 ratio to
make '85_02.nc':
     ncflint -w 0.667 85_01.nc 85_04.nc 85_02.nc
     ncflint -w 0.667,0.333 85_01.nc 85_04.nc 85_02.nc

   Multiply '85.nc' by 3 and by -2 and add them together to make
'tst.nc':
     ncflint -w 3,-2 85.nc 85.nc tst.nc
This is an example of a null operation, so 'tst.nc' should be identical
(within machine precision) to '85.nc'.

   Multiply all the variables except the coordinate variables in the
file 'emissions.nc' by by 0.8:
     ncflint --fix_rec_crd -w 0.8,0.0 emissions.nc emissions.nc scaled_emissions.nc
The use of '--fix_rec_crd' ensures, e.g., that the 'time' coordinate, if
any, is not scaled (i.e., multiplied).

   Add '85.nc' to '86.nc' to obtain '85p86.nc', then subtract '86.nc'
from '85.nc' to obtain '85m86.nc'
     ncflint -w 1,1 85.nc 86.nc 85p86.nc
     ncflint -w 1,-1 85.nc 86.nc 85m86.nc
     ncdiff 85.nc 86.nc 85m86.nc
Thus 'ncflint' can be used to mimic some 'ncbo' operations.  However
this is not a good idea in practice because 'ncflint' does not broadcast
(*note ncbo netCDF Binary Operator::) conforming variables during
arithmetic.  Thus the final two commands would produce identical results
except that 'ncflint' would fail if any variables needed to be
broadcast.

   Rescale the dimensional units of the surface pressure 'prs_sfc' from
Pascals to hectopascals (millibars)
     ncflint -C -v prs_sfc -w 0.01,0.0 in.nc in.nc out.nc
     ncatted -a units,prs_sfc,o,c,millibar out.nc


File: nco.info,  Node: ncks netCDF Kitchen Sink,  Next: ncpdq netCDF Permute Dimensions Quickly,  Prev: ncflint netCDF File Interpolator,  Up: Reference Manual

4.8 'ncks' netCDF Kitchen Sink
==============================

SYNTAX
     ncks [-3] [-4] [-5] [-6] [-7] [-A] [-a] [--area_wgt]
     [-b FL_BNR] [-C] [-c] [--cdl] [--chk_map] [--chk_nan]
     [--cnk_byt SZ_BYT] [--cnk_csh SZ_BYT] [--cnk_dmn NM,SZ_LMN]
     [--cnk_map MAP] [--cnk_min SZ_BYT] [--cnk_plc PLC] [--cnk_scl SZ_LMN]
     [-D DBG] [-d DIM,[MIN][,[MAX][,[STRIDE]]]
     [-F] [--fix_rec_dmn DIM] [--fl_fmt FL_FMT] [--fmt_val FORMAT]
     [-G GPE_DSC] [-g GRP[,...]] [--glb ...] [--grp_xtr_var_xcl]
     [-H] [-h] [--hdn] [--hdr_pad NBR] [--hpss] [--jsn] [--jsn_fmt LVL]
     [-L DFL_LVL] [-l PATH]
     [-M] [-m] [--map MAP-FILE] [--md5] [--mk_rec_dmn DIM]
     [--no_blank] [--no_cll_msr] [--no_frm_trm] [--no_tmp_fl]
     [-O] [-o OUTPUT-FILE] [-P] [-p PATH] [--ppc ...] [--prn_fl PRINT-FILE]
     [-Q] [-q] [-R] [-r] [--rad] [--ram_all] [--rgr ...] [--rnr=wgt]
     [-s FORMAT] [-u] [--unn] [-V] [-v VAR[,...]] [--vrt VRT-FILE]
     [-X ...] [-x] [--xml] INPUT-FILE [[OUTPUT-FILE]]

DESCRIPTION

   The nickname "kitchen sink" is a catch-all because 'ncks' combines
most features of 'ncdump' and 'nccopy' with extra features to extract,
hyperslab, multi-slab, sub-set, and translate into one versatile
utility.  'ncks' extracts (a subset of the) data from INPUT-FILE,
regrids it according to MAP-FILE if specified, then writes in netCDF
format to OUTPUT-FILE, and optionally writes it in flat binary format to
'fl_bnr', and optionally prints it to screen.

   'ncks' prints netCDF input data in ASCII, CDL, JSON, or NcML/XML text
formats to 'stdout', like (an extended version of) 'ncdump'.  By default
'ncks' prints CDL format.  Option '-s' (or long options '--sng_fmt' and
'--string') permits the user to format data using C-style format
strings, while option '--cdl' outputs CDL, option '--jsn' (or 'json')
outputs JSON, option '--trd' (or 'traditional') outputs "traditional"
format, and option '--xml' (or 'ncml') outputs NcML.  The "traditional"
tabular format is intended to be easy to search for the data you want,
one datum per screen line, with all dimension subscripts and coordinate
values (if any) preceding the datum.  'ncks' exposes many flexible
controls over printed output, including CDL, JSON, and NcML.

   Options '-a', '--cdl', '-F', '--fmt_val', '-H', '--hdn', '--jsn',
'-M', '-m', '-P', '--prn_fl', '-Q', '-q', '-s', '--trd', '-u', '-V', and
'--xml' (and their long option counterparts) control the presence of
data and metadata and their formatted location and appearance when
printed.

   'ncks' extracts (and optionally creates a new netCDF file comprised
of) only selected variables from the input file (similar to the old
'ncextr' specification).  Only variables and coordinates may be
specifically included or excluded--all global attributes and any
attribute associated with an extracted variable are copied to the screen
and/or output netCDF file.  Options '-c', '-C', '-v', and '-x' (and
their long option synonyms) control which variables are extracted.

   'ncks' extracts hyperslabs from the specified variables ('ncks'
implements the original 'nccut' specification).  Option '-d' controls
the hyperslab specification.  Input dimensions that are not associated
with any output variable do not appear in the output netCDF. This
feature removes superfluous dimensions from netCDF files.

   'ncks' will append variables and attributes from the INPUT-FILE to
OUTPUT-FILE if OUTPUT-FILE is a pre-existing netCDF file whose relevant
dimensions conform to dimension sizes of INPUT-FILE.  The append
features of 'ncks' are intended to provide a rudimentary means of adding
data from one netCDF file to another, conforming, netCDF file.  If
naming conflicts exist between the two files, data in OUTPUT-FILE is
usually overwritten by the corresponding data from INPUT-FILE.  Thus,
when appending, the user should backup OUTPUT-FILE in case valuable data
are inadvertantly overwritten.

   If OUTPUT-FILE exists, the user will be queried whether to
"overwrite", "append", or "exit" the 'ncks' call completely.  Choosing
"overwrite" destroys the existing OUTPUT-FILE and create an entirely new
one from the output of the 'ncks' call.  Append has differing effects
depending on the uniqueness of the variables and attributes output by
'ncks': If a variable or attribute extracted from INPUT-FILE does not
have a name conflict with the members of OUTPUT-FILE then it will be
added to OUTPUT-FILE without overwriting any of the existing contents of
OUTPUT-FILE.  In this case the relevant dimensions must agree (conform)
between the two files; new dimensions are created in OUTPUT-FILE as
required.  When a name conflict occurs, a global attribute from
INPUT-FILE will overwrite the corresponding global attribute from
OUTPUT-FILE.  If the name conflict occurs for a non-record variable,
then the dimensions and type of the variable (and of its coordinate
dimensions, if any) must agree (conform) in both files.  Then the
variable values (and any coordinate dimension values) from INPUT-FILE
will overwrite the corresponding variable values (and coordinate
dimension values, if any) in OUTPUT-FILE (1).

   Since there can only be one record dimension in a file, the record
dimension must have the same name (though not necessarily the same size)
in both files if a record dimension variable is to be appended.  If the
record dimensions are of differing sizes, the record dimension of
OUTPUT-FILE will become the greater of the two record dimension sizes,
the record variable from INPUT-FILE will overwrite any counterpart in
OUTPUT-FILE and fill values will be written to any gaps left in the rest
of the record variables (I think).  In all cases variable attributes in
OUTPUT-FILE are superseded by attributes of the same name from
INPUT-FILE, and left alone if there is no name conflict.

   Some users may wish to avoid interactive 'ncks' queries about whether
to overwrite existing data.  For example, batch scripts will fail if
'ncks' does not receive responses to its queries.  Options '-O' and '-A'
are available to force overwriting existing files, and appending
existing variables, respectively.

* Menu:

* Filters for ncks::

Options specific to 'ncks'
--------------------------

The following summarizes features unique to 'ncks'.  Features common to
many operators are described in *note Shared features::.

'-a'
     Switches '-a', '--abc', and '--alphabetize' _turn-off_ the default
     alphbetization of extracted fields in 'ncks' only.  These switches
     are misleadingly named and were deprecated in 'ncks' as of NCO
     version 4.7.1 (December, 2017).

     This is the default behavior so these switches are no-ops included
     only for completeness.  By default, NCO extracts, prints, and
     writes specified output variables to disk in alphabetical order.
     This tends to make long output lists easier to search for
     particular variables.  Again, no option is necessary to write
     output in alphabetical order.  Until NCO version 4.7.1 (December,
     2017), 'ncks' used the '-a', '--abc', or '--alphabetize' switches
     to _turn-off_ the default alphabetization.  These names were
     counter-intuitive and needlessly confusing.  As of NCO version
     4.7.1, 'ncks' uses the new switches '--no_abc', '--no-abc',
     '--no_alphabetize', or '--no-alphabetize', all of which are
     equivalent.  The '--abc' and '--alphabetize' switches are now
     no-ops, i.e., they write the output in the unsorted order of the
     input.  The '-a' switch is now completely deprecated in favor of
     the clearer long option switches.

'-b 'file''
     Activate native machine binary output writing to binary file
     'file'.  Also '--fl_bnr' and '--binary-file'.  Writing packed
     variables in binary format is not supported.  Metadata is never
     output to the binary file.  Examine the netCDF output file to see
     the variables in the binary file.  Use the '-C' switch, if
     necessary, to avoid wanting unwanted coordinates to the binary
     file:
          % ncks -O -v one_dmn_rec_var -b bnr.dat -p ~/nco/data in.nc out.nc
          % ls -l bnr.dat | cut -d ' ' -f 5 # 200 B contains time and one_dmn_rec_var
          200
          % ls -l bnr.dat
          % ncks -C -O -v one_dmn_rec_var -b bnr.dat -p ~/nco/data in.nc out.nc
          % ls -l bnr.dat | cut -d ' ' -f # 40 B contains one_dmn_rec_var only
          40

'--cal'
     As of NCO version 4.6.5 (March, 2017), 'ncks' can print
     human-legible calendar strings corresponding to time values with
     UDUnits-compatible date units of the form time-since-basetime,
     e.g., 'days since 2000-01-01' and a CF calendar attribute, if any.
     Enact this with the '--calendar' (also '--cln', '--prn_lgb', and
     '--datestamp') option when printing in any mode.  Invoking this
     option when DBG_LVL >= 1 in CDL mode prints both the value and the
     calendar string (one in comments):
          zender@aerosol:~$ ncks -D 1 --cal -v tm_365 ~/nco/data/in.nc
          ...
            variables:
              double tm_365 ;
                tm_365:units = "days since 2013-01-01" ; // char
                tm_365:calendar = "365_day" ; // char
          
            data:
              tm_365 = "2013-03-01"; // double value: 59
          ...
          zender@aerosol:~$ ncks -D 1 -v tm_365 ~/nco/data/in.nc
          ...
              tm_365 = 59; // calendar format: "2013-03-01"
          ...
     This option is similar to the 'ncdump' '-t' option.  As of NCO
     version 4.6.8 (August, 2017), 'ncks' CDL printing supports
     finer-grained control of date formats with the '--dt_fmt=DT_FMT'
     (or '--date_format') option.  The DT_FMT is an enumerated integer
     from 0-3.  Values DT_FMT=0 or 1 correspond to the short format for
     dates that are the default.  The value DT_FMT=2 requests the
     "regular" format for dates, DT_FMT=3 requests the full ISO-8601
     format with the "T" separator:
          ncks -H -m -v time_bnds -C --dt_fmt=value ~/nco/data/in.nc
          # Value:    Output:
          # 0,1       1964-03-13 09:08:16        # Default, short format
          # 2         1964-03-13 09:08:16.000000 # Regular format
          # 3         1964-03-13T09:08:16.000000 # ISO8601 'T' format
     Note that '--dt_fmt' automatically implies '--cal' makes that
     options superfluous.

'--chk_map'
     As of NCO version 4.9.0 (December, 2019), 'ncks' can evaluate the
     quality of regridding weights.  This option works with map-files
     (not grid-files) in ESMF/CMIP6-compliant format (i.e., a sparse
     matrix variable named 'S' and coordinates '[xy][ab]_[cv]'.  When
     invoked with the additional '--area_wgt' option, the evaluation
     statistics are area-weighted and thus exactly represent the
     global-mean/min/max/mebs/rms/sdn biases expected when regridding a
     globally uniform field.  This tool makes it easier to objectively
     assess weight-generation algorithms, and will hopefully assist in
     their improvement.  Thanks to Mark Taylor of Saturday Night Live
     (SNL) and Paul Ullrich of UC Davis for this suggestion and early
     prototypes.
          $ ncks --chk_map map.nc            # Unweighted statistics
          $ ncks --chk_map --area_wgt map.nc # Area-weighted statistics

     The analysis computes 'frac_b' as row-sums of the weight array, and
     compares this to the stored values of 'frac_b'.  'frac_b' in the
     file may differ from the row-sum for example if the map-file
     generator artificially limits the stored 'frac_b' value for any
     cell to 1.0 for those row-sums that exceed 1.0.  This precautions
     is necessary because there are semi-valid reasons a map-generator
     might do this.  Computing row-sums and comparing to stored 'frac_b'
     catches any such discrepancies.  The analysis sounds an alarm when
     discrepancies exceed a tolerance (5.0e-16).

     Note that 'frac_b' min/max are generally not perfect (1.0) for
     global grids, and reporting the detectable biases is a prime
     feature of the checker.  When invoked with area-weighting as 'ncks
     --area_wgt --chk_map', the global weighted mean of 'frac_b' will
     alter the unweighted statistics (min/max will not change).

'--chk_nan'
     As of NCO version 4.8.0 (May, 2019), 'ncks' can locate 'NaN' of
     NaNf in double- and single-precision floating-point variables,
     respectively.  If a NaN is encountered, NCO prints its location and
     then exits with an error code.  Thanks to Matthew Thompson of NASA
     for this suggestion.
          $ ncks --chk_nan ~/nco/data/in.nc

'--fix_rec_dmn'
     Change record dimension DIM in the input file into a fixed
     dimension in the output file.  Also '--no_rec_dmn'.  Before NCO
     version 4.2.5 (January, 2013), the syntax for '--fix_rec_dmn' did
     not permit or require the specification of the dimension name DIM.
     This is because the feature only worked on netCDF3 files, which
     support only one record dimension, so specifying its name was not
     necessary.  netCDF4 files allow an arbitrary number of record
     dimensions, so the user must specify which record dimension to fix.
     The decision was made that starting with NCO version 4.2.5
     (January, 2013), it is always required to specify the dimension
     name to fix regardless of the netCDF file type.  This keeps the
     code simple, and is symmetric with the syntax for '--mk_rec_dmn',
     described next.

     As of NCO version 4.4.0 (January, 2014), the argument 'all' may be
     given to '--fix_rec_dmn' to convert _all_ record dimensions to
     fixed dimensions in the output file.  Previously, '--fix_rec_dmn'
     only allowed one option, the name of a single record dimension to
     be fixed.  Now it is simple to simultaneously fix all record
     dimensions.  This is useful (and nearly mandatory) when flattening
     netCDF4 files that have multiple record dimensions per group into
     netCDF3 files (which are limited to at most one record dimension)
     (*note Group Path Editing::).

'--hdn'
     As of NCO version 4.4.0 (January, 2014), the '--hdn' or '--hidden'
     options print hidden (aka special) attributes.  This is equivalent
     to 'ncdump -s'.  Hidden attributes include: '_Format',
     '_DeflateLevel', '_Shuffle', '_Storage', '_ChunkSizes',
     '_Endianness', '_Fletcher32', and '_NOFILL'.  Previously 'ncks'
     ignored all these attributes in CDL/XML modes.  Now it prints these
     attributes as appropriate in all modes.  As of NCO version 4.4.6
     (September, 2014), '--hdn' also prints the extended file format
     (i.e., the format of the file or server supplying the data) as
     '_SOURCE_FORMAT'.  As of NCO version 4.6.1 (August, 2016), '--hdn'
     also prints the hidden attributes '_NCProperties', '_IsNetcdf4',
     and '_SuperblockVersion' for netCDF4 files so long as NCO is linked
     against netCDF library version 4.4.1 or later.  Users are referred
     to the Unidata netCDF Documentation
     (http://www.unidata.ucar.edu/software/netcdf/docs), or the man
     pages for 'ncgen' or 'ncdump', for detailed descriptions of the
     meanings of these hidden attributes.

'--cdl'
     As of NCO version 4.3.3 (July, 2013), 'ncks' can print extracted
     data and metadata to screen (i.e., 'stdout') as valid CDL (network
     Common data form Description Language).  CDL is the human-readable
     "lingua franca" of netCDF ingested by 'ncgen' and excreted by
     'ncdump'.  As of NCO version 4.6.9 (September, 2017), 'ncks' prints
     CDL by default, and the "traditional" mode must be explicitly
     selected with '--trd'.  Compare 'ncks' "traditional" with CDL
     printing:
          zender@roulee:~$ ncks --trd -v one ~/nco/data/in.nc
          one: type NC_FLOAT, 0 dimensions, 1 attribute, chunked? no, compressed? no, packed? no
          one size (RAM) = 1*sizeof(NC_FLOAT) = 1*4 = 4 bytes
          one attribute 0: long_name, size = 3 NC_CHAR, value = one
          
          one = 1 
          
          zender@roulee:~$ ncks --cdl -v one ~/nco/data/in.nc
          netcdf in {
          
            variables:
              float one ;
              one:long_name = "one" ;
          
            data:
              one = 1 ;
          
          } // group /
     Users should note the NCO's CDL mode outputs successively more
     verbose additional diagnostic information in CDL comments as the
     level of debugging increases from zero to two.  For example
     printing the above with '-D 2' yields
          zender@roulee:~$ ncks -D 2 --cdl -v one ~/nco/data/in.nc
          netcdf in {
            // ncgen -k classic -b -o in.nc in.cdl
          
            variables:
              float one ; // RAM size = 1*sizeof(NC_FLOAT) = 1*4 = 4 bytes, ID = 147
                one:long_name = "one" ; // char
          
            data:
              one = 1 ; 
          
          } // group /

     'ncgen' converts CDL-mode output into a netCDF file:
          ncks -v one ~/nco/data/in.nc > ~/in.cdl
          ncgen -k netCDF-4 -b -o ~/in.nc ~/in.cdl
          ncks -v one ~/in.nc
     The HDF version of 'ncgen', often named 'hncgen', 'h4_ncgen', or
     'ncgen-hdf', converts netCDF3 CDL into an HDF file:
          /usr/hdf4/bin/ncgen -b -o ~/in.hdf ~/in.cdl # HDF ncgen (local builds)
          /usr/bin/hncgen     -b -o ~/in.hdf ~/in.cdl # Same as HDF ncgen (RPM packages?)
          /usr/bin/h4_ncgen   -b -o ~/in.hdf ~/in.cdl # Same as HDF ncgen (Anaconda)
          /usr/bin/ncgen-hdf  -b -o ~/in.hdf ~/in.cdl # Same as HDF ncgen (Debian packages?)
          hdp dumpsds ~/in.hdf                        # ncdump/h5dump-equivalent for HDF4
          h4_ncdump dumpsds ~/in.hdf                  # ncdump/h5dump-equivalent for HDF4
     Note that HDF4 does not support netCDF-style groups, so the above
     commands fail when the input file contains groups.  Only netCDF4
     and HDF5 support groups.  In our experience the HDF 'ncgen'
     command, by whatever name installed, is not robust and fails on
     many valid netCDF3 CDL constructs.

'--mk_rec_dmn DIM'
     Change existing dimension DIM to a record dimension in the output
     file.  This is the most straightforward way of changing a dimension
     to a/the record dimension, and works fine in most cases.  See *note
     ncecat netCDF Ensemble Concatenator:: and *note ncpdq netCDF
     Permute Dimensions Quickly:: for other methods of changing variable
     dimensionality, including the record dimension.

'-H'
     Toggle (turn-on or turn-off) default behavior of printing data (not
     metadata) to screen or copying data to disk.  Also activated using
     '--print' or '--prn'.  By default 'ncks' prints all metadata but no
     data to screen when no netCDF OUTPUT-FILE is specified.  And if
     OUTPUT-FILE is specified, 'ncks' copies all metadata and all data
     to it.  In other words, the printing/copying default is
     context-sensitive, and '-H' toggles the default behavior.  Hence,
     use '-H' to turn-off copying data (not metadata) to an output file.
     (It is occasionally useful to write all metadata to a file, so that
     the file has allocated the required disk space to hold the data,
     yet to withold writing the data itself).  And use '-H' to turn-on
     printing data (not metadata) to screen.  Unless otherwise specified
     (with '-s'), each element of the data hyperslab prints on a
     separate line containing the names, indices, and, values, if any,
     of all of the variables dimensions.  The dimension and variable
     indices refer to the location of the corresponding data element
     with respect to the variable as stored on disk (i.e., not the
     hyperslab).
          % ncks --trd -C -v three_dmn_var in.nc
          lat[0]=-90 lev[0]=100 lon[0]=0 three_dmn_var[0]=0
          lat[0]=-90 lev[0]=100 lon[1]=90 three_dmn_var[1]=1
          lat[0]=-90 lev[0]=100 lon[2]=180 three_dmn_var[2]=2
          ...
          lat[1]=90 lev[2]=1000 lon[1]=90 three_dmn_var[21]=21
          lat[1]=90 lev[2]=1000 lon[2]=180 three_dmn_var[22]=22
          lat[1]=90 lev[2]=1000 lon[3]=270 three_dmn_var[23]=23
     Printing the same variable with the '-F' option shows the same
     variable indexed with Fortran conventions
          % ncks -F -C -v three_dmn_var in.nc
          lon(1)=0 lev(1)=100 lat(1)=-90 three_dmn_var(1)=0
          lon(2)=90 lev(1)=100 lat(1)=-90 three_dmn_var(2)=1
          lon(3)=180 lev(1)=100 lat(1)=-90 three_dmn_var(3)=2
          ...
     Printing a hyperslab does not affect the variable or dimension
     indices since these indices are relative to the full variable (as
     stored in the input file), and the input file has not changed.
     However, if the hyperslab is saved to an output file and those
     values are printed, the indices will change:
          % ncks --trd -H -d lat,90.0 -d lev,1000.0 -v three_dmn_var in.nc out.nc
          ...
          lat[1]=90 lev[2]=1000 lon[0]=0 three_dmn_var[20]=20
          lat[1]=90 lev[2]=1000 lon[1]=90 three_dmn_var[21]=21
          lat[1]=90 lev[2]=1000 lon[2]=180 three_dmn_var[22]=22
          lat[1]=90 lev[2]=1000 lon[3]=270 three_dmn_var[23]=23
          % ncks --trd -C -v three_dmn_var out.nc
          lat[0]=90 lev[0]=1000 lon[0]=0 three_dmn_var[0]=20
          lat[0]=90 lev[0]=1000 lon[1]=90 three_dmn_var[1]=21
          lat[0]=90 lev[0]=1000 lon[2]=180 three_dmn_var[2]=22
          lat[0]=90 lev[0]=1000 lon[3]=270 three_dmn_var[3]=23

'--jsn, --json'
     As of NCO version 4.6.2 (November, 2016), 'ncks' can print
     extracted metadata and data to screen (i.e., 'stdout') as JSON,
     JavaScript Object Notation, defined here (http://www.json.org).
     'ncks' supports JSON output more completely, flexibly, and robustly
     than any other tool to our knowledge.  With 'ncks' one can
     translate entire netCDF3 and netCDF4 files into JSON, including
     metadata and data, using all NCO's subsetting and hyperslabbing
     capabilities.  NCO uses a JSON format we developed ourselves,
     during a year of discussion among interested researchers.  Some
     refer to this format as NCO-JSON, to disambiguate it from other
     JSON formats for netCDF data.  Other projects have since adopted,
     and some can generate, NCO-JSON.  Projects that support NCO-JSON
     include ERDDAP
     (<https://coastwatch.pfeg.noaa.gov/erddap/index.html>, choose
     output filetype '.ncoJson' from this table
     (https://coastwatch.pfeg.noaa.gov/erddap/griddap/documentation.html#fileType))
     and CF-JSON (<http://cf-json.org>).

     Behold JSON output in default mode:
          zender@aerosol:~$ ncks --jsn -v one ~/nco/data/in.nc
          {
            "variables": {
              "one": {
                "type": "float",
                "attributes": {
                  "long_name": "one"
                },
                "data": 1.0
              }
            }
          }
     NCO converts to (using commonsense rules) and prints all NC_TYPEs
     as one of three atomic types distinguishable as JSON values:
     'float', 'string', and 'int' (2).  Floating-point types ('NC_FLOAT'
     and 'NC_DOUBLE') are printed with a decimal point and at least one
     signficant digit following the decimal point, e.g., '1.0' rather
     than '1.' or '1'.  Integer types (e.g., 'NC_INT', 'NC_UINT64') are
     printed with no decimal point.  String types ('NC_CHAR' and
     'NC_STRING') are enclosed in double-quotes.

     The JSON specification allows many possible output formats for
     netCDF files.  NCO developers implemented a working prototype in
     Octoboer, 2016 and, after discussing options with interested
     parties here
     (https://sourceforge.net/p/nco/discussion/9829/thread/8c4d7e72),
     finalized the emitted JSON syntax a few weeks later.  The resulting
     JSON backend supports three levels of pedanticness, ordered from
     more concise, flexible, and human-readable to more verbose,
     restrictive, and 1-to-1 reproducible.  JSON-specific switches
     access these modes and other features.  Each JSON configuration
     option automatically triggers JSON printing, so that specifying
     '--json' in addition to a JSON configuration option is redundant
     and unnecessary.

     Request a specific format level with the pedantic level argument to
     the '--jsn_fmt LVL' option.  As of NCO version 4.6.3 (December,
     2016), the option formerly known as '--jsn_att_fmt' was renamed
     simply '--jsn_fmt'.  The more general name reflects the fact that
     the option controls all JSON formatting, not just attribute
     formatting.  As of version 4.6.3, NCO defaults to demarcate inner
     dimensions of variable data with (nested) square brackets rather
     than printing data as an unrolled single dimensional array.  An
     array with C-ordered dimensionality [2,3,4] prints as:
          % ncks --jsn -v three_dmn_var ~/nco/data/in.nc
          ...
          "data": [[[0.0, 1.0, 2.0, 3.0], [4.0, 5.0, 6.0, 7.0], [8.0, 9.0, 10.0,11.0]], [[12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0], [20.0,21.0, 22.0, 23.0]]]
          ...
          % ncks --jsn_fmt=4 -v three_dmn_var ~/nco/data/in.nc
          ...
          "data": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0,22.0, 23.0]
          ...
     One can recover the former behavior (and omit the brackets) by
     adding four to the base pedantic level LVL (as shown above).
     Besides the potential offset of four, LVL may take one of three
     values between 0-2:
        * LVL = 0 is the default mode, and is also explicitly selectable
          with '--jsn_fmt=0'.  All values are output without the
          original NC_TYPE token.  This allows attributes to print as
          JSON name-value pairs, rather than as more complex objects:
               % ncks --jsn_fmt=0 -v att_var ~/nco/data/in_grp.nc
               ...
               "att_var": {
                 "shape": ["time"],
                 "type": "float",
                 "attributes": {
                   "byte_att": [0, 1, 2, 127, -128, -127, -2, -1],
                   "char_att": "Sentence one.\nSentence two.\n",
                   "short_att": 37,
                   "int_att": 73,
                   "long_att": 73,
                   "float_att": [73.0, 72.0, 71.0, 70.010, 69.0010, 68.010, 67.010],
                   "double_att": [73.0, 72.0, 71.0, 70.010, 69.0010, 68.010, 67.0100010]
                 },
                 "data": [10.0, 10.10, 10.20, 10.30, 10.40101, 10.50, 10.60, 10.70, 10.80, 10.990]
               ...

          This least pedantic mode produces the most easily read
          results, and suffices for many (most?)  purposes.  Any
          downstream parser is expected to assign an appropriate type as
          indicated by JSON syntax rules.  Because the original
          attributes' 'NC_TYPE' are not output, a downstream parser may
          not exactly reproduce the input file datatypes.  For example,
          whether the original attribute string was stored as 'NC_CHAR'
          or 'NC_STRING' will be unknown to a downstream parser.
          Distinctions between 'NC_FLOAT' and 'NC_DOUBLE' are similarly
          lost, as are all distinctions among the integer types.

          In our experience, these distinctions are immaterial for
          attributes, which are intended for metadata not for
          large-scale storage.  Type-distinctions can, however,
          significantly impact the size of variable data, responsible
          for nearly all the storage required by datasets.  For
          instance, storing or transferring an 'NC_SHORT' field as
          'NC_DOUBLE' would waste a factor of four in space or
          bandwidth.  This is why NCO _always_ prints the 'NC_TYPE' of
          variable data.  Downstream parsers can (but are not required
          to) take advantage of the variable's 'NC_TYPE' to choose the
          most efficient storage type.

          The Shape member of the variable object is an ordered array of
          dimension names such as '"shape": ["lat","lon"]', similar to
          its use in NcML. Each name corresponds to a previously defined
          Dimension object that, taken together, define the rank, shape,
          and size of the variable.  Variables are assumed to be scalar
          by default.  Hence the Shape member is mandatory for arrays,
          and is always omitted for scalars (by contrast, NcML requires
          an empty shape string to indicate scalars).

        * LVL = 1 is a medium-pedantic level that prints all attributes
          as objects (with explicit types) unless the attribute type
          match the simplest default JSON value types.  In other words,
          attributes of type 'NC_FLOAT', 'NC_CHAR', 'NC_SHORT', and
          'NC_BYTE' are printed as objects with an explicit type so that
          parsers do not use the default type.  Attributes of type
          'NC_DOUBLE', 'NC_STRING', and 'NC_INT' are printed as JSON
          arrays, as in the LVL = 0 above:
               % ncks --jsn_fmt=1 -v att_var ~/nco/data/in.nc
               ...
               "att_var": {
                 "shape": ["time"],
                 "type": "float",
                 "attributes": {
                   "byte_att": { "type": "byte", "data": [0, 1, 2, 127, -128, -127, -2, -1]},
                   "char_att": "Sentence one.\nSentence two.\n",
                   "short_att": { "type": "short", "data": 37},
                   "int_att": 73,
                   "long_att": 73,
                   "float_att": [73.0, 72.0, 71.0, 70.010, 69.0010, 68.010, 67.010],
                   "double_att": { "type": "double", "data": [73.0, 72.0, 71.0, 70.010, 69.0010, 68.010, 67.0100010]}
                 },
                 "data": [10.0, 10.10, 10.20, 10.30, 10.40101, 10.50, 10.60, 10.70, 10.80, 10.990]
               ...
          The attributes of type 'NC_BYTE', 'NC_SHORT', and 'NC_DOUBLE'
          are printed as JSON objects that comprise an 'NC_TYPE' and a
          value list, because their values could conceivably not be
          representable, or would waste space, if interpreted as
          'NC_INT' or 'NC_FLOAT', respectively.  All other attributes
          may be naturally mapped to the type indicated by the JSON
          syntax of the value, where numbers are assumed to correspond
          to 'NC_FLOAT' for floating-point, 'NC_INT' for integers, and
          'NC_CHAR' or 'NC_STRING' for strings.  This minimal increase
          in verbosity allows a downstream parser to re-construct the
          original dataset with nearly identical attributes types to the
          original.

        * LVL = 2 is the most pedantic mode, and should be used when
          preserving all input types (e.g., to ensure exact
          reproducibility of the input file) is important.  This mode
          always prints attributes as JSON objects with a type value so
          that any downstream parser can (though it need not) guarantee
          exact reproduction of the original dataset:
               % ncks --jsn_fmt=2 -v att_var ~/nco/data/in.nc
               ...
               "att_var": {
                 "shape": ["time"],
                 "type": "float",
                 "attributes": {
                   "byte_att": { "type": "byte", "data": [0, 1, 2, 127, -128, -127, -2, -1]},
                   "char_att": { "type": "char", "data": "Sentence one.\nSentence two.\n"},
                   "short_att": { "type": "short", "data": 37},
                   "int_att": { "type": "int", "data": 73},
                   "long_att": { "type": "int", "data": 73},
                   "float_att": { "type": "float", "data": [73.0, 72.0, 71.0, 70.010, 69.0010, 68.010, 67.010]},
                   "double_att": { "type": "double", "data": [73.0, 72.0, 71.0, 70.010, 69.0010, 68.010, 67.0100010]}
                 },
                 "data": [10.0, 10.10, 10.20, 10.30, 10.40101, 10.50, 10.60, 10.70, 10.80, 10.990]
               ...

     That ncks produces correct translations of for all supported
     datatypes may be verified by a JSON syntax checker command like
     'jsonlint'.  Please let us know how to improve JSON features for
     your application.

     There

'-M'
     Turn-on printing to screen or turn-off copying global and group
     metadata.  This includes file summary information and global and
     group attributes.  Also '--Mtd' and '--Metadata'.  By default
     'ncks' prints global metadata to screen if no netCDF output file
     and no variable extraction list is specified (with '-v').  Use '-M'
     to print global metadata to screen if a netCDF output is specified,
     or if a variable extraction list is specified (with '-v').  Use
     '-M' to turn-off copying of global and group metadata when copying,
     subsetting, or appending to an output file.

     The various combinations of printing switches can be confusing.  In
     an attempt to anticipate what most users want to do, 'ncks' uses
     context-sensitive defaults for printing.  Our goal is to minimize
     the use of switches required to accomplish the common operations.
     We assume that users creating a new file or overwriting (e.g., with
     '-O') an existing file usually wish to copy all global and
     variable-specific attributes to the new file.  In contrast, we
     assume that users appending (e.g., with '-A' an explicit variable
     list from one file to another usually wish to copy only the
     variable-specific attributes to the output file.  The switches
     '-H', '-M', and '-m' switches are implemented as toggles which
     reverse the default behavior.  The most confusing aspect of this is
     that '-M' inhibits copying global metadata in overwrite mode and
     causes copying of global metadata in append mode.
          ncks                 in.nc        # Print  VAs and GAs
          ncks          -v one in.nc        # Print  VAs not GAs
          ncks    -M    -v one in.nc        # Print  GAs only
          ncks       -m -v one in.nc        # Print  VAs only
          ncks    -M -m -v one in.nc        # Print  VAs and GAs
          ncks -O              in.nc out.nc # Copy   VAs and GAs
          ncks -O       -v one in.nc out.nc # Copy   VAs and GAs
          ncks -O -M    -v one in.nc out.nc # Copy   VAs not GAs
          ncks -O    -m -v one in.nc out.nc # Copy   GAs not VAs
          ncks -O -M -m -v one in.nc out.nc # Copy   only data (no atts)
          ncks -A              in.nc out.nc # Append VAs and GAs
          ncks -A       -v one in.nc out.nc # Append VAs not GAs
          ncks -A -M    -v one in.nc out.nc # Append VAs and GAs
          ncks -A    -m -v one in.nc out.nc # Append only data (no atts)
          ncks -A -M -m -v one in.nc out.nc # Append GAs not VAs
     where 'VAs' and 'GAs' denote variable and group/global attributes,
     respectively.

'-m'
     Turn-on printing to screen or turn-off copying variable metadata.
     Using '-m' will print variable metadata to screen (similar to
     'ncdump -h').  This displays all metadata pertaining to each
     variable, one variable at a time.  This includes information on the
     storage properties of the variable, such as whether it employs
     chunking, compression, or packing.  Also activated using '--mtd'
     and '--metadata'.  The 'ncks' default behavior is to print variable
     metadata to screen if no netCDF output file is specified.  Use '-m'
     to print variable metadata to screen if a netCDF output is
     specified.  Also use '-m' to turn-off copying of variable metadata
     to an output file.

'--no_blank'
     Print numeric representation of missing values.  As of NCO version
     4.2.2 (October, 2012), NCO prints missing values as blanks (i.e.,
     the underscore character '_') by default.  To enable the old
     behavior of printing the numeric representation of missing values
     (e.g., '1.0e36'), use the '--no_blank' switch.  Also activated
     using '--noblank' or '--no-blank'.

'-P'
     Print data, metadata, and units to screen.  The '-P' switch is a
     convenience abbreviation for '-C -H -M -m -u'.  Also activated
     using '--print' or '--prn'.  This set of switches is useful for
     exploring file contents.

'--prn_fl 'print-file''
     Activate printing formatted output to file 'print-file'.  Also
     '--print_file', '--fl_prn', and '--file_print'.  One can achieve
     the same result by redirecting stdout to a named file.  However, it
     is slightly faster to print formatted output directly to a file
     than to stdout:
          ncks --fl_prn=foo.txt --jsn in.nc

'-Q'
     Print quietly, meaning omit dimension names, indices, and
     coordinate values when printing arrays.  Variable (not dimension)
     indices are printed.  Variable names appear flush left in the
     output:
          zender@roulee:~$ ncks --trd -Q -v three_dmn_rec_var -C -H ~/nco/data/in.nc              
          three_dmn_rec_var[0]=1 
          ...
     This helps locate specific variables in lists with many variables
     and different dimensions.  See also the '-V' option, which omits
     all names and indices and prints only variable values.

'-q'
     Quench (turn-off) all printing to screen.  This overrides the
     setting of all print-related switches, equivalent to '-H -M -m'
     when in single-file printing mode.  When invoked with '-R' (*note
     Retaining Retrieved Files::), 'ncks' automatically sets '-q'.  This
     allows 'ncks' to retrieve remote files without automatically trying
     to print them.  Also '--quench'.

'--rad'
     Retain all dimensions.  When invoked with '--rad' (Retain All
     Dimensions), 'ncks' copies each dimension in the input file to the
     output file, regardless of whether the dimension is utilized by any
     variables.  Normally 'ncks' discards "orphan dimensions", i.e.,
     dimensions not referenced by any variables.  This switch allows
     users to keep non-referenced dimensions in the workflow.  When
     invoked in printing mode, causes orphaned dimensions to be printed
     (they are not printed by default).  Also '--retain_all_dimensions',
     '--orphan_dimensions', and '--rph_dmn'.

'-s FORMAT'
     String format for text output.  Accepts C language escape sequences
     and 'printf()' formats.  Also '--string' and '--sng_fmt'.  This
     option is only intended for use with traditional (TRD) printing,
     and thus automatically invokes the '--trd' switch.

'--fmt_val FORMAT'
     Supply a 'printf()'-style format for printed output, i.e., in CDL,
     JSON, TRD, or XML modes.  Also '--val_fmt' and '--value_format'.
     One use for this option is to reduce the printed precision of
     floating point values:
          # Default printing of original double precision values
          # 0.0,0.1,0.12,0.123,0.1234,0.12345,0.123456,0.1234567,0.12345678,0.123456789
          % ncks -C -v ppc_dbl ~/nco/data/in.nc
          ...
          ppc_dbl = 0, 0.1, 0.12, 0.123, 0.1234, 0.12345, 0.123456, 0.1234567, 0.12345678, 0.123456789 ;
          ...
          # Restrict printing to three digits after the decimal
          % ncks --fmt_val=%.3f -C -v ppc_dbl ~/nco/data/in.nc
          ...
          ppc_dbl = 0., 0.1, 0.12, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123 ;
          ...
     The supplied FORMAT only applies to floating point variable values
     ('NC_FLOAT' or 'NC_DOUBLE'), and not to other types or to
     attributes.  For reference, the default 'printf()' FORMAT for CDL,
     JSON, TRD, and XML modes is '%#.7gf', '%#.7g', '%g', and '%#.7g',
     respectively, for single-precision data, and, for double-precision
     data is '%#.15g', '%#.15g', '%.12g', and '%#.15g', respectively.
     NCO introduced this feature in version 4.7.3 (March, 2018).  We
     would appreciate your feedback on whether and how to extend this
     feature to make it more useful.

'--secret'
     Print summary of 'ncks' hidden features.  These hidden or secret
     features are used mainly by developers.  They are not supported for
     general use and may change at any time.  This demonstrates
     conclusively that I cannot keep a secret.  Also '--ssh' and
     '--scr'.

'--trd, --traditional'
     From 1995-2017 'ncks' dumped the ASCII text representation of
     netCDF files in what we now call "traditional" mode.  Much of this
     manual contains output printed in traditional mode, which places
     one value per line, with complete dimensional information.
     Traditional-mode metadata output includes lower-level information,
     such as RAM usage and internal variable IDs, than CDL.  While this
     is useful for some developers and user, CDL has, over the years,
     become more useful than traditional mode for most users.  As of NCO
     version 4.6.9 (September, 2017) CDL became the default printing
     mode.  Traditional printing mode is accessed via the '--trd'
     option.

'-u'
     Toggle the printing of a variable's 'units' attribute, if any, with
     its values.  Also '--units'.

'-V'
     Print variable values only.  Do not print variable and dimension
     names, indices, and coordinate values when printing arrays.
          zender@roulee:~$ ncks --trd -V -v three_dmn_rec_var -C -H ~/nco/data/in.nc
          1
          ...
     See also the '-Q' option, which prints variable names and indices,
     but not dimension names, indices, or coordinate values when
     printing arrays.  Using '-V' is the same as specifying '-Q
     --no_nm_prn'.

'--xml, --ncml'
     As of NCO version 4.3.3 (July, 2013), 'ncks' can print extracted
     data and metadata to screen (i.e., 'stdout') as XML in NcML, the
     netCDF Markup Language.  'ncks' supports XML more completely than
     'ncdump -x'.  With 'ncks' one can translate entire netCDF3 and
     netCDF4 files into NcML, including metadata and data, using all
     NCO's subsetting and hyperslabbing capabilities.  Compare 'ncks'
     "traditional" with XML printing:
          zender@roulee:~$ ncks --trd -v one ~/nco/data/in.nc
          one: type NC_FLOAT, 0 dimensions, 1 attribute, chunked? no, compressed? no, packed? no
          one size (RAM) = 1*sizeof(NC_FLOAT) = 1*4 = 4 bytes
          one attribute 0: long_name, size = 3 NC_CHAR, value = one
          
          one = 1 
          
          zender@roulee:~$ ncks --xml -v one ~/nco/data/in.nc
          <?xml version="1.0" encoding="UTF-8"?>
          <netcdf xmlns="http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2" location="/home/zender/nco/data/in.nc">
            <variable name="one" type="float" shape="">
              <attribute name="long_name" separator="*" value="one" />
              <values>1.</values>
            </variable>
          </netcdf>
     XML-mode prints variable metadata and, as of NCO version 4.3.7
     (October, 2013), variable data and, as of NCO version 4.4.0
     (January, 2014), hidden attributes.  That ncks produces correct
     NcML translations of CDM files for all supported datatypes is
     verified by comparison to output from Unidata's 'toolsUI' Java
     program.  Please let us know how to improve XML/NcML features.

     'ncks' provides additional options to configure NcML output:
     '--xml_no_location', '--xml_spr_chr', and '--xml_spr_nmr'.  Every
     NcML configuration option automatically triggers NcML printing, so
     that specifying '--xml' in addition to a configuration option is
     redundant and unnecessary.  The '--xml_no_location' switch prevents
     output of the NcML 'location' element.  By default the location
     element is printed with a value equal to the location of the input
     dataset, e.g., 'location="/home/zender/in.nc"'.  The
     '--xml_spr_chr' and '--xml_spr_nmr' options customize the strings
     used as NcML separators for attributes and variables of
     character-type and numeric-type, respectively.  Their default
     separators are '*' and "' '" (a space):
          zender@roulee:~$ ncks --xml -d time,0,3 -v two_dmn_rec_var_sng in.nc
          ...
             <values separator="*">abc*bcd*cde*def</values>
           ...
           zender@roulee:~$ ncks --xml_spr_chr=', ' -v two_dmn_rec_var_sng in.nc
          ...
          <values separator=", ">abc, bcd, cde, def, efg, fgh, ghi, hij, jkl, klm</values>
          ...
          zender@roulee:~$ ncks --xml -v one_dmn_rec_var in.nc
          ...
          <values>1 2 3 4 5 6 7 8 9 10</values>
          ...
          zender@roulee:~$ ncks --xml_spr_nmr=', ' -v one_dmn_rec_var in.nc
          ...
          <values separator=", ">1, 2, 3, 4, 5, 6, 7, 8, 9, 10</values>
          ...
     Separator elements for strings are a thorny issue.  One must be
     sure that the separator element is not mistaken as a portion of the
     string.  NCO attempts to produce valid NcML and supplies the
     '--xml_spr_chr' option to work around any difficulties.  NCO
     performs precautionary checks with 'strstr(VAL,SPR)' to identify
     presence of the separator string (SPR) in data (VAL) and, when it
     detects a match, automatically switches to a backup separator
     string ('*|*').  However limitations of 'strstr()' may lead to
     false negatives when the separator string occurs in data beyond the
     first string in multi-dimensional 'NC_CHAR' arrays.  Hence, results
     may be ambiguous to NcML parsers.  If problems arise, use
     '--xml_spr_chr' to specify a multi-character separator that does
     not appear in the string array and that does not include an NcML
     formatting characters (e.g., commas, angles, quotes).

* Menu:

* Filters for ncks::

   ---------- Footnotes ----------

   (1) Those familiar with netCDF mechanics might wish to know what is
happening here: 'ncks' does not attempt to redefine the variable in
OUTPUT-FILE to match its definition in INPUT-FILE, 'ncks' merely copies
the values of the variable and its coordinate dimensions, if any, from
INPUT-FILE to OUTPUT-FILE.

   (2) The JSON boolean atomic type is not (yet) supported as there is
no obvious netCDF-equivalent to this type.


File: nco.info,  Node: Filters for ncks,  Prev: ncks netCDF Kitchen Sink,  Up: ncks netCDF Kitchen Sink

4.8.1 Filters for 'ncks'
------------------------

We encourage the use of standard UNIX pipes and filters to narrow the
verbose output of 'ncks' into more precise targets.  For example, to
obtain an uncluttered listing of the variables in a file try
     ncks --trd -m in.nc | grep -E ': type' | cut -f 1 -d ' ' | sed 's/://' | sort
   A Bash user could alias the previous filter to the shell command
'ncvarlst' as shown below.  More complex examples could involve command
line arguments.  For example, a user may frequently be interested in
obtaining the value of an attribute, e.g., for textual file examination
or for passing to another shell command.  Say the attribute is
'purpose', the variable is 'z', and the file is 'in.nc'.  In this
example, 'ncks --trd -m -v z' is too verbose so a robust 'grep' and
'cut' filter is desirable, such as
     ncks --trd -M -m in.nc | grep -E -i "^z attribute [0-9]+: purpose" | cut -f 11- -d ' ' | sort
   The filters are clearly too complex to remember on-the-fly so the
entire procedure could be implemented as a shell command or function
called, say, 'ncattget'
     function ncattget { ncks --trd -M -m ${3} | grep -E -i "^${2} attribute [0-9]+: ${1}" | cut -f 11- -d ' ' | sort ; }
   The shell 'ncattget' is invoked with three arugments that are, in
order, the names of the attribute, variable, and file to examine.
Global attributes are indicated by using a variable name of 'global'.
This definition yields the following results
     % ncattget purpose z in.nc
     Height stored with a monotonically increasing coordinate
     % ncattget Purpose Z in.nc
     Height stored with a monotonically increasing coordinate
     % ncattget history z in.nc
     % ncattget history global in.nc
     History global attribute.
   Note that case sensitivity has been turned off for the variable and
attribute names (and could be turned on by removing the '-i' switch to
'grep').  Furthermore, extended regular expressions may be used for both
the variable and attribute names.  The next two commands illustrate this
by searching for the values of attribute 'purpose' in all variables, and
then for all attributes of the variable 'z':
     % ncattget purpose .+ in.nc
     1-D latitude coordinate referred to by geodesic grid variables
     1-D longitude coordinate referred to by geodesic grid variables
     ...
     % ncattget .+ Z in.nc
     Height
     Height stored with a monotonically increasing coordinate
     meter

   Extended filters are best stored as shell commands if they are used
frequently.  Shell commands may be re-used when they are defined in
shell configuration files.  These files are usually named '.bashrc',
'.cshrc', and '.profile' for the Bash, Csh, and Sh shells, respectively.
     # NB: Untested on Csh, Ksh, Sh, Zsh! Send us feedback!
     # Bash shell (/bin/bash), .bashrc examples
     # ncattget $att_nm $var_nm $fl_nm : What attributes does variable have?
     function ncattget { ncks --trd -M -m ${3} | grep -E -i "^${2} attribute [0-9]+: ${1}" | cut -f 11- -d ' ' | sort ; }
     # ncunits $att_val $fl_nm : Which variables have given units?
     function ncunits { ncks --trd -m ${2} | grep -E -i " attribute [0-9]+: units.+ ${1}" | cut -f 1 -d ' ' | sort ; }
     # ncavg $var_nm $fl_nm : What is mean of variable?
     function ncavg { ncwa -y avg -O -C -v ${1} ${2} ~/foo.nc ; ncks --trd -H -C -v ${1} ~/foo.nc | cut -f 3- -d ' ' ; }
     # ncavg $var_nm $fl_nm : What is mean of variable?
     function ncavg { ncap2 -O -C -v -s "foo=${1}.avg();print(foo)" ${2} ~/foo.nc | cut -f 3- -d ' ' ; }
     # ncdmnlst $fl_nm : What dimensions are in file?
     function ncdmnlst { ncks --cdl -m ${1} | cut -d ':' -f 1 | cut -d '=' -s -f 1 ; }
     # ncdmnsz $dmn_nm $fl_nm : What is dimension size?
     function ncdmnsz { ncks --trd -m -M ${2} | grep -E -i ": ${1}, size =" | cut -f 7 -d ' ' | uniq ; }
     # ncgrplst $fl_nm : What groups are in file?
     function ncgrplst { ncks -m ${1} | grep 'group:' | cut -d ':' -f 2 | cut -d ' ' -f 2 | sort ; }
     # ncvarlst $fl_nm : What variables are in file?
     function ncvarlst { ncks --trd -m ${1} | grep -E ': type' | cut -f 1 -d ' ' | sed 's/://' | sort ; }
     # ncmax $var_nm $fl_nm : What is maximum of variable?
     function ncmax { ncwa -y max -O -C -v ${1} ${2} ~/foo.nc ; ncks --trd -H -C -v ${1} ~/foo.nc | cut -f 3- -d ' ' ; }
     # ncmax $var_nm $fl_nm : What is maximum of variable?
     function ncmax { ncap2 -O -C -v -s "foo=${1}.max();print(foo)" ${2} ~/foo.nc | cut -f 3- -d ' ' ; }
     # ncmdn $var_nm $fl_nm : What is median of variable?
     function ncmdn { ncap2 -O -C -v -s "foo=gsl_stats_median_from_sorted_data(${1}.sort());print(foo)" ${2} ~/foo.nc | cut -f 3- -d ' ' ; }
     # ncmin $var_nm $fl_nm : What is minimum of variable?
     function ncmin { ncap2 -O -C -v -s "foo=${1}.min();print(foo)" ${2} ~/foo.nc | cut -f 3- -d ' ' ; }
     # ncrng $var_nm $fl_nm : What is range of variable?
     function ncrng { ncap2 -O -C -v -s "foo_min=${1}.min();foo_max=${1}.max();print(foo_min,\"%f\");print(\" to \");print(foo_max,\"%f\")" ${2} ~/foo.nc ; }
     # ncmode $var_nm $fl_nm : What is mode of variable?
     function ncmode { ncap2 -O -C -v -s "foo=gsl_stats_median_from_sorted_data(${1}.sort());print(foo)" ${2} ~/foo.nc | cut -f 3- -d ' ' ; }
     # ncrecsz $fl_nm : What is record dimension size?
     function ncrecsz { ncks --trd -M ${1} | grep -E -i "^Record dimension:" | cut -f 8- -d ' ' ; }
     # nctypget $var_nm $fl_nm : What type is variable?
     function nctypget { ncks --trd -m -v ${1} ${2} | grep -E -i "^${1}: type" | cut -f 3 -d ' ' | cut -f 1 -d ',' ; }
     
     # Csh shell (/bin/csh), .cshrc examples (derive others from Bash definitions):
     ncattget() { ncks --trd -M -m -v ${3} | grep -E -i "^${2} attribute [0-9]+: ${1}" | cut -f 11- -d ' ' | sort ; }
     ncdmnsz() { ncks --trd -m -M ${2} | grep -E -i ": ${1}, size =" | cut -f 7 -d ' ' | uniq ; }
     ncvarlst() { ncks --trd -m ${1} | grep -E ': type' | cut -f 1 -d ' ' | sed 's/://' | sort ; }
     ncrecsz() { ncks --trd -M ${1} | grep -E -i "^Record dimension:" | cut -f 8- -d ' ' ; }
     
     # Sh shell (/bin/sh), .profile examples (derive others from Bash definitions):
     ncattget() { ncks --trd -M -m ${3} | grep -E -i "^${2} attribute [0-9]+: ${1}" | cut -f 11- -d ' ' | sort ; }
     ncdmnsz() { ncks --trd -m -M ${2} | grep -E -i ": ${1}, size =" | cut -f 7 -d ' ' | uniq ; }
     ncvarlst() { ncks --trd -m ${1} | grep -E ': type' | cut -f 1 -d ' ' | sed 's/://' | sort ; }
     ncrecsz() { ncks --trd -M ${1} | grep -E -i "^Record dimension:" | cut -f 8- -d ' ' ; }

EXAMPLES

   View all data in netCDF 'in.nc', printed with Fortran indexing
conventions:
     ncks -F in.nc

   Copy the netCDF file 'in.nc' to file 'out.nc'.
     ncks in.nc out.nc
   Now the file 'out.nc' contains all the data from 'in.nc'.  There are,
however, two differences between 'in.nc' and 'out.nc'.  First, the
'history' global attribute (*note History Attribute::) will contain the
command used to create 'out.nc'.  Second, the variables in 'out.nc' will
be defined in alphabetical order.  Of course the internal storage of
variable in a netCDF file should be transparent to the user, but there
are cases when alphabetizing a file is useful (see description of '-a'
switch).

   Copy all global attributes (and no variables) from 'in.nc' to
'out.nc':
     ncks -A -x ~/nco/data/in.nc ~/out.nc
   The '-x' switch tells NCO to use the complement of the extraction
list (*note Subsetting Files::).  Since no extraction list is explicitly
specified (with '-v'), the default is to extract all variables.  The
complement of all variables is no variables.  Without any variables to
extract, the append ('-A') command (*note Appending Variables::) has
only to extract and copy (i.e., append) global attributes to the output
file.

   Copy/append metadata (not data) from variables in one file to
variables in a second file.  When copying/subsetting/appending files (as
opposed to printing them), the copying of data, variable metadata, and
global/group metadata are now turned OFF by '-H', '-m', and '-M',
respectively.  This is the opposite sense in which these switches work
when _printing_ a file.  One can use these switches to easily replace
data or metadata in one file with data or metadata from another:
     # Extract naked (data-only) copies of two variables
     ncks -h -M -m -O -C -v one,three_dmn_rec_var ~/nco/data/in.nc ~/out.nc
     # Change values to be sure original values are not copied in following step
     ncap2 -O -v -s 'one*=2;three_dmn_rec_var*=0' ~/nco/data/in.nc ~/in2.nc
     # Append in2.nc metadata (not data!) to out.nc
     ncks -A -C -H -v one,three_dmn_rec_var ~/in2.nc ~/out.nc
   Variables in 'out.nc' now contain data (not metadata) from 'in.nc'
and metadata (not data) from 'in2.nc'.

   Print variable 'three_dmn_var' from file 'in.nc' with default
notations.  Next print 'three_dmn_var' as an un-annotated text column.
Then print 'three_dmn_var' signed with very high precision.  Finally,
print 'three_dmn_var' as a comma-separated list:
     % ncks --trd -C -v three_dmn_var in.nc
     lat[0]=-90 lev[0]=100 lon[0]=0 three_dmn_var[0]=0
     lat[0]=-90 lev[0]=100 lon[1]=90 three_dmn_var[1]=1
     ...
     lat[1]=90 lev[2]=1000 lon[3]=270 three_dmn_var[23]=23
     % ncks --trd -s '%f\n' -C -v three_dmn_var in.nc
     0.000000
     1.000000
     ...
     23.000000
     % ncks --trd -s '%+16.10f\n' -C -v three_dmn_var in.nc
        +0.0000000000
        +1.0000000000
     ...
       +23.0000000000
     % ncks --trd -s '%f, ' -C -v three_dmn_var in.nc
     0.000000, 1.000000, ..., 23.000000,
Programmers will recognize these as the venerable C language 'printf()'
formatting strings.  The second and third options are useful when
pasting data into text files like reports or papers.  *Note ncatted
netCDF Attribute Editor::, for more details on string formatting and
special characters.

   As of NCO version 4.2.2 (October, 2012), NCO prints missing values as
blanks (i.e., the underscore character '_') by default:
     % ncks --trd -C -H -v mss_val in.nc
     lon[0]=0 mss_val[0]=73
     lon[1]=90 mss_val[1]=_
     lon[2]=180 mss_val[2]=73
     lon[3]=270 mss_val[3]=_
     % ncks -s '%+5.1f, ' -H -C -v mss_val in.nc
     +73.0, _, +73.0, _,
   To print the numeric value of the missing value instead of a blank,
use the '--no_blank' option.

   'ncks' prints in a verbose fashion by default and supplies a number
of switches to pare-down (or even spruce-up) the output.  The interplay
of the '-Q', '-V', and (otherwise undocumented) '--no_nm_prn' switches
yields most desired verbosities:
     % ncks -v three_dmn_rec_var -C -H ~/nco/data/in.nc
     time[0]=1 lat[0]=-90 lon[0]=0 three_dmn_rec_var[0]=1 
     % ncks -Q -v three_dmn_rec_var -C -H ~/nco/data/in.nc              
     three_dmn_rec_var[0]=1 
     % ncks -V -v three_dmn_rec_var -C -H ~/nco/data/in.nc
     1
     % ncks -Q --no_nm_prn -v three_dmn_rec_var -C -H ~/nco/data/in.nc
     1
     % ncks --no_nm_prn -v three_dmn_rec_var -C -H ~/nco/data/in.nc
     1 -90 0 1

   One dimensional arrays of characters stored as netCDF variables are
automatically printed as strings, whether or not they are
NUL-terminated, e.g.,
     ncks -v fl_nm in.nc
The '%c' formatting code is useful for printing multidimensional arrays
of characters representing fixed length strings
     ncks -s '%c' -v fl_nm_arr in.nc
Using the '%s' format code on strings which are not NUL-terminated (and
thus not technically strings) is likely to result in a core dump.

   Create netCDF 'out.nc' containing all variables, and any associated
coordinates, except variable 'time', from netCDF 'in.nc':
     ncks -x -v time in.nc out.nc
   As a special case of this, consider how to remove a variable such as
'time_bounds' that is identified in a CF Convention (*note CF
Conventions::) compliant 'ancillary_variables', 'bounds', 'climatology',
'coordinates', or 'grid_mapping' attribute.  NCO subsetting assumes the
user wants all ancillary variables, axes, bounds and coordinates
associated with all extracted variables (*note Subsetting Coordinate
Variables::).  Hence to exclude a 'ancillary_variables', 'bounds',
'climatology', 'coordinates', or 'grid_mapping' variable while retaining
the "parent" variable (here 'time'), one must use the '-C' switch:
     ncks -C -x -v time_bounds in.nc out.nc
   The '-C' switch tells the operator _NOT_ to necessarily include all
the CF ancillary variables, axes, bounds, and coordinates.  Hence the
output file will contain 'time' and not 'time_bounds'.

   Extract variables 'time' and 'pressure' from netCDF 'in.nc'.  If
'out.nc' does not exist it will be created.  Otherwise the you will be
prompted whether to append to or to overwrite 'out.nc':
     ncks -v time,pressure in.nc out.nc
     ncks -C -v time,pressure in.nc out.nc
The first version of the command creates an 'out.nc' which contains
'time', 'pressure', and any coordinate variables associated with
PRESSURE.  The 'out.nc' from the second version is guaranteed to contain
only two variables 'time' and 'pressure'.

   Create netCDF 'out.nc' containing all variables from file 'in.nc'.
Restrict the dimensions of these variables to a hyperslab.  The
specified hyperslab is: the fifth value in dimension 'time'; the
half-open range LAT > 0. in coordinate 'lat'; the half-open range LON <
330. in coordinate 'lon'; the closed interval 0.3 < BAND < 0.5 in
coordinate 'band'; and cross-section closest to 1000. in coordinate
'lev'.  Note that limits applied to coordinate values are specified with
a decimal point, and limits applied to dimension indices do not have a
decimal point *Note Hyperslabs::.
     ncks -d time,5 -d lat,,0.0 -d lon,330.0, -d band,0.3,0.5
     -d lev,1000.0 in.nc out.nc

   Assume the domain of the monotonically increasing longitude
coordinate 'lon' is 0 < LON < 360.  Here, 'lon' is an example of a
wrapped coordinate.  'ncks' will extract a hyperslab which crosses the
Greenwich meridian simply by specifying the westernmost longitude as MIN
and the easternmost longitude as MAX, as follows:
     ncks -d lon,260.0,45.0 in.nc out.nc
   For more details *Note Wrapped Coordinates::.


File: nco.info,  Node: ncpdq netCDF Permute Dimensions Quickly,  Next: ncra netCDF Record Averager,  Prev: ncks netCDF Kitchen Sink,  Up: Reference Manual

4.9 'ncpdq' netCDF Permute Dimensions Quickly
=============================================

SYNTAX
     ncpdq [-3] [-4] [-5] [-6] [-7] [-A] [-a [-]DIM[,...]] [-C] [-c]
     [--cnk_byt SZ_BYT] [--cnk_csh SZ_BYT] [--cnk_dmn NM,SZ_LMN]
     [--cnk_map MAP] [--cnk_min SZ_BYT] [--cnk_plc PLC] [--cnk_scl SZ_LMN]
     [-D DBG] [-d DIM,[MIN][,[MAX][,[STRIDE]]] [-F] [--fl_fmt FL_FMT]
     [-G GPE_DSC] [-g GRP[,...]] [--glb ...]
     [-h] [--hdf] [--hdr_pad NBR] [--hpss]
     [-L DFL_LVL] [-l PATH] [-M PCK_MAP] [--mrd]
     [--no_cll_msr] [--no_frm_trm] [--no_tmp_fl]
     [-O] [-o OUTPUT-FILE] [-P PCK_PLC] [-p PATH] [--ppc ...]
     [-R] [-r] [--ram_all] [-t THR_NBR] [-U] [--unn] [-v VAR[,...]] [-X ...] [-x]
     INPUT-FILE [OUTPUT-FILE]

DESCRIPTION

   'ncpdq' performs one (not both) of two distinct functions per
invocation: packing or dimension permutation.  Without any options,
'ncpdq' will pack data with default parameters.  The '-a' option tells
'ncpdq' to permute dimensions accordingly, otherwise 'ncpdq' will pack
data as instructed/controlled by the '-M' and '-P' options.  'ncpdq' is
optimized to perform these actions in a parallel fashion with a minimum
of time and memory.  The "pdq" may stand for "Permute Dimensions
Quickly", "Pack Data Quietly", "Pillory Dan Quayle", or other silly
uses.

Packing and Unpacking Functions
-------------------------------

The 'ncpdq' packing (and unpacking) algorithms are described in *note
Methods and functions::, and are also implemented in 'ncap2'.  'ncpdq'
extends the functionality of these algorithms by providing high level
control of the "packing policy" so that users can consistently pack (and
unpack) entire files with one command.  The user specifies the desired
packing policy with the '-P' switch (or its long option equivalents,
'--pck_plc' and '--pack_policy') and its PCK_PLC argument.  Four packing
policies are currently implemented:
"Packing (and Re-Packing) Variables [_default_]"
     Definition: Pack unpacked variables, re-pack packed variables
     Alternate invocation: 'ncpack'
     PCK_PLC key values: 'all_new', 'pck_all_new_att'
"Packing (and not Re-Packing) Variables"
     Definition: Pack unpacked variables, copy packed variables
     Alternate invocation: none
     PCK_PLC key values: 'all_xst', 'pck_all_xst_att'
"Re-Packing Variables"
     Definition: Re-pack packed variables, copy unpacked variables
     Alternate invocation: none
     PCK_PLC key values: 'xst_new', 'pck_xst_new_att'
"Unpacking"
     Definition: Unpack packed variables, copy unpacked variables
     Alternate invocation: 'ncunpack'
     PCK_PLC key values: 'upk', 'unpack', 'pck_upk'
Equivalent key values are fully interchangeable.  Multiple equivalent
options are provided to satisfy disparate needs and tastes of NCO users
working with scripts and from the command line.

   Regardless of the packing policy selected, 'ncpdq' no longer (as of
NCO version 4.0.4 in October, 2010) packs coordinate variables, or the
special variables, weights, and other grid properties described in *note
CF Conventions::.  Prior 'ncpdq' versions treated coordinate variables
and grid properties no differently from other variables.  However,
coordinate variables are one-dimensional, so packing saves little space
on large files, and the resulting files are difficult for humans to
read.  'ncpdq' will, of course, _unpack_ coordinate variables and
weights, for example, in case some other, non-NCO software packed them
in the first place.

   Concurrently, Gaussian and area weights and other grid properties are
often used to derive fields in re-inflated (unpacked) files, so packing
such grid properties causes a considerable loss of precision in
downstream data processing.  If users express strong wishes to pack grid
properties, we will implement new packing policies.  An immediate
workaround for those needing to pack grid properties now, is to use the
'ncap2' packing functions or to rename the grid properties prior to
calling 'ncpdq'.  We welcome your feedback.

   To reduce required memorization of these complex policy switches,
'ncpdq' may also be invoked via a synonym or with switches that imply a
particular policy.  'ncpack' is a synonym for 'ncpdq' and behaves the
same in all respects.  Both 'ncpdq' and 'ncpack' assume a default
packing policy request of 'all_new'.  Hence 'ncpack' may be invoked
without any '-P' switch, unlike 'ncpdq'.  Similarly, 'ncunpack' is a
synonym for 'ncpdq' except that 'ncpack' implicitly assumes a request to
unpack, i.e., '-P pck_upk'.  Finally, the 'ncpdq' '-U' switch (or its
long option equivalents '--unpack') requires no argument.  It simply
requests unpacking.

   Given the menagerie of synonyms, equivalent options, and implied
options, a short list of some equivalent commands is appropriate.  The
following commands are equivalent for packing: 'ncpdq -P all_new',
'ncpdq --pck_plc=all_new', and 'ncpack'.  The following commands are
equivalent for unpacking: 'ncpdq -P upk', 'ncpdq -U', 'ncpdq
--pck_plc=unpack', and 'ncunpack'.  Equivalent commands for other
packing policies, e.g., 'all_xst', follow by analogy.  Note that 'ncpdq'
synonyms are subject to the same constraints and recommendations
discussed in the secion on 'ncbo' synonyms (*note ncbo netCDF Binary
Operator::).  That is, symbolic links must exist from the synonym to
'ncpdq', or else the user must define an 'alias'.

   The 'ncpdq' packing algorithms must know to which type particular
types of input variables are to be packed.  The correspondence between
the input variable type and the output, packed type, is called the
"packing map".  The user specifies the desired packing map with the '-M'
switch (or its long option equivalents, '--pck_map' and '--map') and its
PCK_MAP argument.  Six packing maps are currently implemented:
"Pack Floating Precisions to 'NC_SHORT' [_default_]"
     Definition: Pack floating precision types to 'NC_SHORT'
     Map: Pack ['NC_DOUBLE','NC_FLOAT'] to 'NC_SHORT'
     Types copied instead of packed:
     ['NC_INT64','NC_UINT64','NC_INT','NC_UINT','NC_SHORT','NC_USHORT','NC_CHAR','NC_BYTE','NC_UBYTE']
     PCK_MAP key values: 'flt_sht', 'pck_map_flt_sht'
"Pack Floating Precisions to 'NC_BYTE'"
     Definition: Pack floating precision types to 'NC_BYTE'
     Map: Pack ['NC_DOUBLE','NC_FLOAT'] to 'NC_BYTE'
     Types copied instead of packed:
     ['NC_INT64','NC_UINT64','NC_INT','NC_UINT','NC_SHORT','NC_USHORT','NC_CHAR','NC_BYTE','NC_UBYTE']
     PCK_MAP key values: 'flt_byt', 'pck_map_flt_byt'
"Pack Higher Precisions to 'NC_SHORT'"
     Definition: Pack higher precision types to 'NC_SHORT'
     Map: Pack
     ['NC_DOUBLE','NC_FLOAT','NC_INT64','NC_UINT64','NC_INT','NC_UINT']
     to 'NC_SHORT'
     Types copied instead of packed:
     ['NC_SHORT','NC_USHORT','NC_CHAR','NC_BYTE','NC_UBYTE']
     PCK_MAP key values: 'hgh_sht', 'pck_map_hgh_sht'
"Pack Higher Precisions to 'NC_BYTE'"
     Definition: Pack higher precision types to 'NC_BYTE'
     Map: Pack
     ['NC_DOUBLE','NC_FLOAT','NC_INT64','NC_UINT64','NC_INT','NC_UINT','NC_SHORT','NC_USHORT']
     to 'NC_BYTE'
     Types copied instead of packed: ['NC_CHAR','NC_BYTE','NC_UBYTE']
     PCK_MAP key values: 'hgh_byt', 'pck_map_hgh_byt'
"Pack to Next Lesser Precision"
     Definition: Pack each type to type of next lesser size
     Map: Pack ['NC_DOUBLE','NC_INT64','NC_UINT64'] to 'NC_INT'.  Pack
     ['NC_FLOAT','NC_INT','NC_UINT'] to 'NC_SHORT'.  Pack
     ['NC_SHORT','NC_USHORT'] to 'NC_BYTE'.
     Types copied instead of packed: ['NC_CHAR','NC_BYTE','NC_UBYTE']
     PCK_MAP key values: 'nxt_lsr', 'pck_map_nxt_lsr'
"Pack Doubles to Floats"
     Definition: Demote (via type-conversion, _not packing_)
     double-precision variables to single-precision
     Map: Demote 'NC_DOUBLE' to 'NC_FLOAT'.  Types copied instead of
     packed: All except 'NC_DOUBLE'
     PCK_MAP key values: 'dbl_flt', 'pck_map_dbl_flt', 'dbl_sgl',
     'pck_map_dbl_sgl'
     The 'dbl_flt' map was introduced in NCO version 4.7.7 (September,
     2018).
"Promote Floats to Doubles"
     Definition: Promote (via type-conversion, _not packing_)
     single-precision variables to double-precision
     Map: Promote 'NC_FLOAT' to 'NC_DOUBLE'.  Types copied instead of
     packed: All except 'NC_FLOAT'
     PCK_MAP key values: 'flt_dbl', 'pck_map_flt_dbl', 'sgl_dbl',
     'pck_map_sgl_dbl'
     The 'flt_dbl' map was introduced in NCO version 4.9.1 (December,
     2019).
The default 'all_new' packing policy with the default 'flt_sht' packing
map reduces the typical 'NC_FLOAT'-dominated file size by about 50%.
'flt_byt' packing reduces an 'NC_DOUBLE'-dominated file by about 87%.

   The "packing map" 'pck_map_dbl_flt' does a pure type-conversion (no
packing is involved) from 'NC_DOUBLE' to 'NC_FLOAT'.  The resulting
variables are not packed, they are just single-precision floating point
instead of double-precision floating point.  This operation is
irreversible, and no attributes are created, modified, or deleted for
these variables.  Note that coordinate and coordinate-like variables
will not be demoted as best practices dictate maintaining coordinates in
the highest possible precision.

   The "packing map" 'pck_map_flt_dbl' does a pure type-conversion (no
packing is involved) from 'NC_FLOAT' to 'NC_DOUBLE'.  The resulting
variables are not packed, they are just double-precision floating point
instead of single-precision floating point.  This operation is
irreversible, and no attributes are created, modified, or deleted for
these variables.  All single-precision variables, including coordinates,
are promoted.  Note that this map can double the size of a dataset.

   The netCDF packing algorithm (*note Methods and functions::) is
lossy--once packed, the exact original data cannot be recovered without
a full backup.  Hence users should be aware of some packing caveats:
First, the interaction of packing and data equal to the _FILLVALUE is
complex.  Test the '_FillValue' behavior by performing a pack/unpack
cycle to ensure data that are missing _stay_ missing and data that are
not misssing do not join the Air National Guard and go missing.  This
may lead you to elect a new _FILLVALUE.  Second, 'ncpdq' actually allows
packing into 'NC_CHAR' (with, e.g., 'flt_chr').  However, the intrinsic
conversion of 'signed char' to higher precision types is tricky for
values equal to zero, i.e., for 'NUL'.  Hence packing to 'NC_CHAR' is
not documented or advertised.  Pack into 'NC_BYTE' (with, e.g.,
'flt_byt') instead.

Dimension Permutation
---------------------

'ncpdq' re-shapes variables in INPUT-FILE by re-ordering and/or
reversing dimensions specified in the dimension list.  The dimension
list is a whitespace-free, comma separated list of dimension names,
optionally prefixed by negative signs, that follows the '-a' (or long
options '--arrange', '--permute', '--re-order', or '--rdr') switch.  To
re-order variables by a subset of their dimensions, specify these
dimensions in a comma-separated list following '-a', e.g., '-a lon,lat'.
To reverse a dimension, prefix its name with a negative sign in the
dimension list, e.g., '-a -lat'.  Re-ordering and reversal may be
performed simultaneously, e.g., '-a lon,-lat,time,-lev'.

   Users may specify any permutation of dimensions, including
permutations which change the record dimension identity.  The record
dimension is re-ordered like any other dimension.  This unique 'ncpdq'
capability makes it possible to concatenate files along any dimension.
See *note Concatenation:: for a detailed example.  The record dimension
is always the most slowly varying dimension in a record variable (*note
C and Fortran Index Conventions::).  The specified re-ordering fails if
it requires creating more than one record dimension amongst all the
output variables (1).

   Two special cases of dimension re-ordering and reversal deserve
special mention.  First, it may be desirable to completely reverse the
storage order of a variable.  To do this, include all the variable's
dimensions in the dimension re-order list in their original order, and
prefix each dimension name with the negative sign.  Second, it may
useful to transpose a variable's storage order, e.g., from C to Fortran
data storage order (*note C and Fortran Index Conventions::).  To do
this, include all the variable's dimensions in the dimension re-order
list in reversed order.  Explicit examples of these two techniques
appear below.

EXAMPLES

   Pack and unpack all variables in file 'in.nc' and store the results
in 'out.nc':
     ncpdq in.nc out.nc # Same as ncpack in.nc out.nc
     ncpdq -P all_new -M flt_sht in.nc out.nc # Defaults
     ncpdq -P all_xst in.nc out.nc
     ncpdq -P upk in.nc out.nc # Same as ncunpack in.nc out.nc
     ncpdq -U in.nc out.nc # Same as ncunpack in.nc out.nc
   The first two commands pack any unpacked variable in the input file.
They also unpack and then re-pack every packed variable.  The third
command only packs unpacked variables in the input file.  If a variable
is already packed, the third command copies it unchanged to the output
file.  The fourth and fifth commands unpack any packed variables.  If a
variable is not packed, the third command copies it unchanged.

   The previous examples all utilized the default packing map.  Suppose
you wish to archive all data that are currently unpacked into a form
which only preserves 256 distinct values.  Then you could specify the
packing map PCK_MAP as 'hgh_byt' and the packing policy PCK_PLC as
'all_xst':
     ncpdq -P all_xst -M hgh_byt in.nc out.nc
   Many different packing maps may be used to construct a given file by
performing the packing on subsets of variables (e.g., with '-v') and
using the append feature with '-A' (*note Appending Variables::).

   Users may wish to unpack data packed with the HDF convention, and
then re-pack it with the netCDF convention so that all their datasets
use the same packing convention prior to intercomparison.
     # One-step procedure: For NCO 4.4.0+, netCDF 4.3.1+
     # 1. Convert, unpack, and repack HDF file into netCDF file
     ncpdq --hdf_upk -P xst_new modis.hdf modis.nc # HDF4 files
     ncpdq --hdf_upk -P xst_new modis.h5  modis.nc # HDF5 files

     # One-step procedure: For NCO 4.3.7--4.3.9
     # 1. Convert, unpack, and repack HDF file into netCDF file
     ncpdq --hdf4 --hdf_upk -P xst_new modis.hdf modis.nc # HDF4
     ncpdq        --hdf_upk -P xst_new modis.h5  modis.nc # HDF5

     # Two-step procedure: For NCO 4.3.6 and earlier
     # 1. Convert HDF file to netCDF file
     ncl_convert2nc modis.hdf
     # 2. Unpack using HDF convention and repack using netCDF convention
     ncpdq --hdf_upk -P xst_new modis.nc modis.nc
   NCO now (2) automatically detects HDF4 files.  In this case it
produces an output file 'modis.nc' which preserves the HDF packing used
in the input file.  The 'ncpdq' command first unpacks all packed
variables using the HDF unpacking algorithm (as specified by
'--hdf_upk'), and then repacks those same variables using the netCDF
algorithm (because that is the only algorithm NCO packs with).  As
described above the '--P xst_new' packing policy only repacks variables
that are already packed.  Not-packed variables are copied directly
without loss of precision (3).

   Re-order file 'in.nc' so that the dimension 'lon' always precedes the
dimension 'lat' and store the results in 'out.nc':
     ncpdq -a lon,lat in.nc out.nc
     ncpdq -v three_dmn_var -a lon,lat in.nc out.nc
   The first command re-orders every variable in the input file.  The
second command extracts and re-orders only the variable 'three_dmn_var'.

   Suppose the dimension 'lat' represents latitude and monotonically
increases increases from south to north.  Reversing the 'lat' dimension
means re-ordering the data so that latitude values decrease
monotonically from north to south.  Accomplish this with
     % ncpdq -a -lat in.nc out.nc
     % ncks --trd -C -v lat in.nc
     lat[0]=-90
     lat[1]=90
     % ncks --trd -C -v lat out.nc
     lat[0]=90
     lat[1]=-90
   This operation reversed the latitude dimension of all variables.
Whitespace immediately preceding the negative sign that specifies
dimension reversal may be dangerous.  Quotes and long options can help
protect negative signs that should indicate dimension reversal from
being interpreted by the shell as dashes that indicate new command line
switches.
     ncpdq -a -lat in.nc out.nc # Dangerous? Whitespace before "-lat"
     ncpdq -a '-lat' in.nc out.nc # OK. Quotes protect "-" in "-lat"
     ncpdq -a lon,-lat in.nc out.nc # OK. No whitespace before "-"
     ncpdq --rdr=-lat in.nc out.nc # Preferred. Uses "=" not whitespace

   To create the mathematical transpose of a variable, place all its
dimensions in the dimension re-order list in reversed order.  This
example creates the transpose of 'three_dmn_var':
     % ncpdq -a lon,lev,lat -v three_dmn_var in.nc out.nc
     % ncks --trd -C -v three_dmn_var in.nc
     lat[0]=-90 lev[0]=100 lon[0]=0 three_dmn_var[0]=0
     lat[0]=-90 lev[0]=100 lon[1]=90 three_dmn_var[1]=1
     lat[0]=-90 lev[0]=100 lon[2]=180 three_dmn_var[2]=2
     ...
     lat[1]=90 lev[2]=1000 lon[1]=90 three_dmn_var[21]=21
     lat[1]=90 lev[2]=1000 lon[2]=180 three_dmn_var[22]=22
     lat[1]=90 lev[2]=1000 lon[3]=270 three_dmn_var[23]=23
     % ncks --trd -C -v three_dmn_var out.nc
     lon[0]=0 lev[0]=100 lat[0]=-90 three_dmn_var[0]=0
     lon[0]=0 lev[0]=100 lat[1]=90 three_dmn_var[1]=12
     lon[0]=0 lev[1]=500 lat[0]=-90 three_dmn_var[2]=4
     ...
     lon[3]=270 lev[1]=500 lat[1]=90 three_dmn_var[21]=19
     lon[3]=270 lev[2]=1000 lat[0]=-90 three_dmn_var[22]=11
     lon[3]=270 lev[2]=1000 lat[1]=90 three_dmn_var[23]=23

   To completely reverse the storage order of a variable, include all
its dimensions in the re-order list, each prefixed by a negative sign.
This example reverses the storage order of 'three_dmn_var':
     % ncpdq -a -lat,-lev,-lon -v three_dmn_var in.nc out.nc
     % ncks --trd -C -v three_dmn_var in.nc
     lat[0]=-90 lev[0]=100 lon[0]=0 three_dmn_var[0]=0
     lat[0]=-90 lev[0]=100 lon[1]=90 three_dmn_var[1]=1
     lat[0]=-90 lev[0]=100 lon[2]=180 three_dmn_var[2]=2
     ...
     lat[1]=90 lev[2]=1000 lon[1]=90 three_dmn_var[21]=21
     lat[1]=90 lev[2]=1000 lon[2]=180 three_dmn_var[22]=22
     lat[1]=90 lev[2]=1000 lon[3]=270 three_dmn_var[23]=23
     % ncks --trd -C -v three_dmn_var out.nc
     lat[0]=90 lev[0]=1000 lon[0]=270 three_dmn_var[0]=23
     lat[0]=90 lev[0]=1000 lon[1]=180 three_dmn_var[1]=22
     lat[0]=90 lev[0]=1000 lon[2]=90 three_dmn_var[2]=21
     ...
     lat[1]=-90 lev[2]=100 lon[1]=180 three_dmn_var[21]=2
     lat[1]=-90 lev[2]=100 lon[2]=90 three_dmn_var[22]=1
     lat[1]=-90 lev[2]=100 lon[3]=0 three_dmn_var[23]=0

   Creating a record dimension named, e.g., 'time', in a file which has
no existing record dimension is simple with 'ncecat':
     ncecat -O -u time in.nc out.nc # Create degenerate record dimension named "time"

   Now consider a file with all dimensions, including 'time', fixed
(non-record).  Suppose the user wishes to convert 'time' from a fixed
dimension to a record dimension.  This may be useful, for example, when
the user wishes to append additional time slices to the data.  As of NCO
version 4.0.1 (April, 2010) the preferred method for doing this is with
'ncks':
     ncks -O --mk_rec_dmn time in.nc out.nc # Change "time" to record dimension

   Prior to 4.0.1, the procedure to change an existing fixed dimension
into a record dimension required three separate commands, 'ncecat'
followed by 'ncpdq', and then 'ncwa'.  The recommended method is now to
use 'ncks --fix_rec_dmn', yet it is still instructive to present the
original procedure, as it shows how multiple operators can achieve the
same ends by different means:
     ncecat -O in.nc out.nc # Add degenerate record dimension named "record"
     ncpdq -O -a time,record out.nc out.nc # Switch "record" and "time"
     ncwa -O -a record out.nc out.nc # Remove (degenerate) "record"
The first step creates a degenerate (size equals one) record dimension
named (by default) 'record'.  The second step swaps the ordering of the
dimensions named 'time' and 'record'.  Since 'time' now occupies the
position of the first (least rapidly varying) dimension, it becomes the
record dimension.  The dimension named 'record' is no longer a record
dimension.  The third step averages over this degenerate 'record'
dimension.  Averaging over a degenerate dimension does not alter the
data.  The ordering of other dimensions in the file ('lat', 'lon', etc.)
is immaterial to this procedure.  See *note ncecat netCDF Ensemble
Concatenator:: and *note ncks netCDF Kitchen Sink:: for other methods of
changing variable dimensionality, including the record dimension.

   ---------- Footnotes ----------

   (1) This limitation, imposed by the netCDF storage layer, may be
relaxed in the future with netCDF4.

   (2) Prior to NCO 4.4.0 and netCDF 4.3.1 (January, 2014), NCO requires
the '--hdf4' switch to correctly read HDF4 input files.  For example,
'ncpdq --hdf4 --hdf_upk -P xst_new modis.hdf modis.nc'.  That switch is
now obsolete, though harmless for backwards compatibility.  Prior to
version 4.3.7 (October, 2013), NCO lacked the software necessary to
circumvent netCDF library flaws handling HDF4 files, and thus NCO failed
to convert HDF4 files to netCDF files.  In those cases, use the
'ncl_convert2nc' command distributed with NCL to convert HDF4 files to
netCDF.

   (3) 'ncpdq' does not support packing data using the HDF convention.
Although it is now straightforward to support this, we think it might
sow more confusion than it reaps.  Let us know if you disagree and would
like NCO to support packing data with HDF algorithm.


File: nco.info,  Node: ncra netCDF Record Averager,  Next: ncrcat netCDF Record Concatenator,  Prev: ncpdq netCDF Permute Dimensions Quickly,  Up: Reference Manual

4.10 'ncra' netCDF Record Averager
==================================

SYNTAX
     ncra [-3] [-4] [-5] [-6] [-7] [-A] [-C] [-c] [--cb]
     [--cnk_byt SZ_BYT] [--cnk_csh SZ_BYT] [--cnk_dmn NM,SZ_LMN]
     [--cnk_map MAP] [--cnk_min SZ_BYT] [--cnk_plc PLC] [--cnk_scl SZ_LMN]
     [-D DBG] [-d DIM,[MIN][,[MAX][,[STRIDE][,[SUBCYCLE]]]] [-F] [--fl_fmt FL_FMT]
     [-G GPE_DSC] [-g GRP[,...]] [--glb ...]
     [-h] [--hdf] [--hdr_pad NBR] [--hpss]
     [-L DFL_LVL] [-l PATH] [--mro] [-N] [-n LOOP]
     [--no_cll_msr] [--no_cll_mth] [--no_frm_trm] [--no_tmp_fl]
     [-O] [-o OUTPUT-FILE] [-p PATH] [--ppc ...] [-R] [-r] [--ram_all] [--rec_apn] [--rth_dbl|flt]
     [-t THR_NBR] [--unn] [-v VAR[,...]] [-w wgt] [-X ...] [-x] [-y OP_TYP]
     [INPUT-FILES] [OUTPUT-FILE]

DESCRIPTION

   'ncra' computes statistics (including, though not limited to,
averages) of record variables across an arbitrary number of INPUT-FILES.
The record dimension is, by default, retained as a degenerate (size 1)
dimension in the output variables.  *Note Statistics vs.
Concatenation::, for a description of the distinctions between the
various statistics tools and concatenators.  As a multi-file operator,
'ncra' will read the list of INPUT-FILES from 'stdin' if they are not
specified as positional arguments on the command line (*note Large
Numbers of Files::).

   Input files may vary in size, but each must have a record dimension.
The record coordinate, if any, should be monotonic (or else non-fatal
warnings may be generated).  Hyperslabs of the record dimension which
include more than one file work correctly.  'ncra' supports the STRIDE
argument to the '-d' hyperslab option (*note Hyperslabs::) for the
record dimension only, STRIDE is not supported for non-record
dimensions.  'ncra' _always averages_ coordinate variables (e.g.,
'time') regardless of the arithmetic operation type performed on
non-coordinate variables (*note Operation Types::).

   As of NCO version 4.4.9, released in May, 2015, 'ncra' accepts
user-specified weights with the '-w' (or long-option equivalent '--wgt',
'--wgt_var', or '--weight') switch.  When no weight is specified, 'ncra'
weights each record (e.g., time slice) in the INPUT-FILES equally.
'ncra' does not attempt to see if, say, the 'time' coordinate is
irregularly spaced and thus would require a weighted average in order to
be a true time average.

   Weights specified with '-w wgt' may take one of two forms.  In the
first form, the 'wgt' argument is a comma-separated list of values by
which to weight each _file_.  Thus the number of values must equal the
number of files specified in the input file list, or else the program
will exit.  In the second form, the 'wgt' argument is the name of a
weighting variable present in every input file.  The variable may be a
scalar or a one-dimensional record variable.  Scalar weights are applied
uniformly to the entire file (i.e., a per-file weight).  One-dimensional
weights apply to each corresponding record (i.e., per-record weights),
and are suitable for dynamically changing timesteps.

   By default, any weights specified (whether by value or by variable
name) are normalized to unity by dividing each specified weight by the
sum of all the weights.  This means, for example, that, '-w 0.25,0.75'
is equivalent to '-w 2.0,6.0' since both are equal when normalized.
This behavior simplifies specifying weights based on countable items.
For example, time-weighting monthly averages for March, April, and May
to obtain a spring seasonal average can be done with '-w 31,30,31'
instead of '-w
0.33695652173913043478,0.32608695652173913043,0.33695652173913043478'.

   However, sometimes one wishes to use weights in "dot-product mode",
i.e., multiply by the (non-normalized) weights.  As of NCO
version 4.5.2, released in July, 2015, 'ncra' accepts the '-N' (or
long-option equivalent '--no_nrm_by_wgt') switch that prevents automatic
weight normalization.  When this switch is used, the weights will not be
normalized (unless the user provides them as normalized), and the
numerator of the weighted average will not be divided by the sum of the
weights (which is one for normalized weights).

   Bear these two exceptions in mind when weighting input: First, 'ncra'
only applies weights if the arithmetic operation type is averaging
(*note Operation Types::), i.e., for timeseries mean and for timeseries
mean absolute value.  Second, weights are never applied for
minimization, square-roots, etc.  'ncra' _never weights_ coordinate
variables (e.g., 'time') regardless of the weighting performed on
non-coordinate variables.

   As of NCO version 4.6.0 (May, 2016) 'ncra' can honor the CF
'climatology' and climatological statistics conventions described in
*note CF Conventions::.  This functionality only works when each input
file contains only a single record (timestep).  Currently this is opt-in
with the '--cb' flag (or long-option equivalent '--clm_bnd'), or with
the '--c2b' flag (or its long-option equivalent '--clm2bnd') switches.
Invoking '--cb' causes 'ncra' to:
  1. Add a 'climatology' attribute with value "climatology_bounds" the
     time coordinate, if necessary
  2. Remove the 'bounds' attribute from the time coordinate, if
     necessary
  3. Output a variable named 'climatology_bounds' with values that are
     minima/maxima of the input time coordinate 'bounds' variable.
  4. Omit any input time coordinate 'bounds' attribute and variable
  5. Ensure the 'cell_methods' attribute for all variables is
     appropriate for climatologies within and over years.  Climatologies
     within days will have incorrect units (the switch is currently
     opt-in so that incorrect units are not inadvertently generated).
     Please contact the authors if this functionality is important to
     you (The omission of climatologies within days is mainly a matter
     of trying to keep the switches and interface clean).
   Use the '--c2b' flag (instead of '--cb') to convert the input
'climatology' bounds to a non-climatology 'bounds' in the output.  In
other words, use '--c2b' when averaging sub-sampled climatologies
together to produce a continuous (non-climatologically sub-sampled)
mean.
     # Use --cb to average months into a climatological month
     ncra --cb 2014_01.nc 2015_01.nc 2016_01.nc clm_JAN.nc
     # Use --cb to average climatological months into a climatological season
     ncra --cb clm_DEC.nc clm_JAN.nc clm_FEB.nc clm_DJF.nc
     # Four seasons make a complete year so use --c2b
     ncra --c2b clm_DJF.nc clm_MAM.nc clm_JJA.nc clm_SON.nc clm_ANN.nc
   Currently this functionality only works with climatologies within and
over years (not within or over days).

EXAMPLES

   Average files '85.nc', '86.nc', ... '89.nc' along the record
dimension, and store the results in '8589.nc':
     ncra 85.nc 86.nc 87.nc 88.nc 89.nc 8589.nc
     ncra 8[56789].nc 8589.nc
     ncra -n 5,2,1 85.nc 8589.nc
   These three methods produce identical answers.  *Note Specifying
Input Files::, for an explanation of the distinctions between these
methods.

   Assume the files '85.nc', '86.nc', ... '89.nc' each contain a record
coordinate TIME of length 12 defined such that the third record in
'86.nc' contains data from March 1986, etc.  NCO knows how to hyperslab
the record dimension across files.  Thus, to average data from December,
1985 through February, 1986:
     ncra -d time,11,13 85.nc 86.nc 87.nc 8512_8602.nc
     ncra -F -d time,12,14 85.nc 86.nc 87.nc 8512_8602.nc
The file '87.nc' is superfluous, but does not cause an error.  The '-F'
turns on the Fortran (1-based) indexing convention.  The following uses
the STRIDE option to average all the March temperature data from
multiple input files into a single output file
     ncra -F -d time,3,,12 -v temperature 85.nc 86.nc 87.nc 858687_03.nc
   *Note Stride::, for a description of the STRIDE argument.

   Assume the TIME coordinate is incrementally numbered such that
January, 1985 = 1 and December, 1989 = 60.  Assuming '??' only expands
to the five desired files, the following averages June, 1985-June, 1989:
     ncra -d time,6.,54. ??.nc 8506_8906.nc
     ncra -y max -d time,6.,54. ??.nc 8506_8906.nc
   The second example identifies the maximum instead of averaging.
*Note Operation Types::, for a description of all available statistical
operations.

   'ncra' includes the powerful subcycle and multi-record output
features (*note Subcycle::).  This example uses these features to
compute and output winter (DJF) averages for all winter seasons
beginning with year 1990 and continuing to the end of the input file:
     ncra -O --mro -d time,"1990-12-01",,12,3 in.nc out.nc

   The '-w wgt' option weights input data _per-file_ or _per-timestep_:
     ncra -w 31,31,28 dec.nc jan.nc feb.nc out.nc
     ncra -w delta_t in1.nc in2.nc in3.nc out.nc
   The first example weights the input differently per-file to produce
correctly weighted winter seasonal mean statistics.  The second example
weights the input per-timestep to produce correctly weighted mean
statistics.  The last example


File: nco.info,  Node: ncrcat netCDF Record Concatenator,  Next: ncremap netCDF Remapper,  Prev: ncra netCDF Record Averager,  Up: Reference Manual

4.11 'ncrcat' netCDF Record Concatenator
========================================

SYNTAX
     ncrcat [-3] [-4] [-5] [-6] [-7] [-A] [-C] [-c]
     [--cnk_byt SZ_BYT] [--cnk_csh SZ_BYT] [--cnk_dmn NM,SZ_LMN]
     [--cnk_map MAP] [--cnk_min SZ_BYT] [--cnk_plc PLC] [--cnk_scl SZ_LMN]
     [-D DBG] [-d DIM,[MIN][,[MAX][,[STRIDE][,[SUBCYCLE]]]] [-F] [--fl_fmt FL_FMT]
     [-G GPE_DSC] [-g GRP[,...]] [--glb ...]
     [-h] [--hdr_pad NBR] [--hpss]
     [-L DFL_LVL] [-l PATH] [--md5_digest] [-n LOOP]
     [--no_tmp_fl] [--no_cll_msr] [--no_frm_trm] [--no_tmp_fl]
     [-O] [-o OUTPUT-FILE] [-p PATH] [--ppc ...] [-R] [-r] [--ram_all] [--rec_apn]
     [-t THR_NBR] [--unn] [-v VAR[,...]] [-X ...] [-x]
     [INPUT-FILES] [OUTPUT-FILE]

DESCRIPTION

   'ncrcat' concatenates record variables across an arbitrary number of
INPUT-FILES.  The final record dimension is by default the sum of the
lengths of the record dimensions in the input files.  *Note Statistics
vs. Concatenation::, for a description of the distinctions between the
various statistics tools and concatenators.  As a multi-file operator,
'ncrcat' will read the list of INPUT-FILES from 'stdin' if they are not
specified as positional arguments on the command line (*note Large
Numbers of Files::).

   Input files may vary in size, but each must have a record dimension.
The record coordinate, if any, should be monotonic (or else non-fatal
warnings may be generated).  Hyperslabs along the record dimension that
span more than one file are handled correctly.  'ncra' supports the
STRIDE argument to the '-d' hyperslab option for the record dimension
only, STRIDE is not supported for non-record dimensions.

   Concatenating a variable packed with different scales multiple
datasets is beyond the capabilities of 'ncrcat' (and 'ncecat', the other
concatenator (*note Concatenation::).  'ncrcat' does not unpack data, it
simply _copies_ the data from the INPUT-FILES, and the metadata from the
_first_ INPUT-FILE, to the OUTPUT-FILE.  This means that data compressed
with a packing convention must use the identical packing parameters
(e.g., 'scale_factor' and 'add_offset') for a given variable across
_all_ input files.  Otherwise the concatenated dataset will not unpack
correctly.  The workaround for cases where the packing parameters differ
across INPUT-FILES requires three steps: First, unpack the data using
'ncpdq'.  Second, concatenate the unpacked data using 'ncrcat', Third,
re-pack the result with 'ncpdq'.

   'ncrcat' applies special rules to ARM convention time fields (e.g.,
'time_offset').  See *note ARM Conventions:: for a complete description.

EXAMPLES

   Concatenate files '85.nc', '86.nc', ... '89.nc' along the record
dimension, and store the results in '8589.nc':
     ncrcat 85.nc 86.nc 87.nc 88.nc 89.nc 8589.nc
     ncrcat 8[56789].nc 8589.nc
     ncrcat -n 5,2,1 85.nc 8589.nc
These three methods produce identical answers.  *Note Specifying Input
Files::, for an explanation of the distinctions between these methods.

   Assume the files '85.nc', '86.nc', ... '89.nc' each contain a record
coordinate TIME of length 12 defined such that the third record in
'86.nc' contains data from March 1986, etc.  NCO knows how to hyperslab
the record dimension across files.  Thus, to concatenate data from
December, 1985-February, 1986:
     ncrcat -d time,11,13 85.nc 86.nc 87.nc 8512_8602.nc
     ncrcat -F -d time,12,14 85.nc 86.nc 87.nc 8512_8602.nc
The file '87.nc' is superfluous, but does not cause an error.  When
'ncra' and 'ncrcat' encounter a file which does contain any records that
meet the specified hyperslab criteria, they disregard the file and
proceed to the next file without failing.  The '-F' turns on the Fortran
(1-based) indexing convention.

   The following uses the STRIDE option to concatenate all the March
temperature data from multiple input files into a single output file
     ncrcat -F -d time,3,,12 -v temperature 85.nc 86.nc 87.nc 858687_03.nc
   *Note Stride::, for a description of the STRIDE argument.

   Assume the TIME coordinate is incrementally numbered such that
January, 1985 = 1 and December, 1989 = 60.  Assuming '??' only expands
to the five desired files, the following concatenates June, 1985-June,
1989:
     ncrcat -d time,6.,54. ??.nc 8506_8906.nc


File: nco.info,  Node: ncremap netCDF Remapper,  Next: ncrename netCDF Renamer,  Prev: ncrcat netCDF Record Concatenator,  Up: Reference Manual

4.12 'ncremap' netCDF Remapper
==============================

SYNTAX
     ncremap [-3] [-4] [-5] [-6] [-7] [-a ALG_TYP] [--a2o]
     [-D DBG_LVL] [-d DST_FL] [--d2f] [--dpt] [--dpt_fl=DPT_FL]
     [--dt_sng=DT_SNG] [--esmf_nsp=ESMF_NSP] [--esmf_typ=ESMF_TYP] [--esmf_xpn=ESMF_XPN]
     [--fl_fmt=FL_FMT] [-G GRD_SNG] [-g GRD_DST]
     [-I DRC_IN] [-i INPUT-FILE] [-j JOB_NBR] [-L DFL_LVL]
     [-M] [-m MAP_FL] [--msh_fl=MSH_FL]
     [--msk_dst=MSK_DST] [--msk_out=MSK_OUT] [--msk_src=MSK_SRC] [--mss_val=MSS_VAL]
     [-n NCO_OPT] [--nm_dst=NM_DST] [--nm_src=NM_SRC]
     [--no_cll_msr] [--no_frm_trm] [--no_stg_grd]
     [-O DRC_OUT] [-o OUTPUT-FILE] [-P PRC_TYP] [-p PAR_TYP]
     [--preserve=PRS_STT] [-R RGR_OPT] [--rgn_dst] [--rgn_src] [--rnr_thr=RNR_THR]
     [--rrg_bb_wesn=BB_WESN] [--rrg_dat_glb=DAT_GLB] [--rrg_grd_glb=GRD_GLB]
     [--rrg_grd_rgn=GRD_RGN] [--rrg_rnm_sng=RNM_SNG]
     [-s GRD_SRC] [--sgs_frc=SGS_FRC] [--sgs_msk=SGS_MSK] [--sgs_nrm=SGS_NRM]
     [--skl=SKL-FILE] [--stdin] [-T DRC_TMP] [-t THR_NBR]
     [-U] [-u UNQ_SFX] [--ugrid=UGRID-FILE]
     [-V RGR_VAR] [-v VAR_LST[,...]] [--version] [--vrb=VRB_LVL]
     [--vrt_fl=VRT_FL] [--vrt_ntp=VRT_NTP] [--vrt_xtr=VRT_XTR]
     [-W WGT_OPT] [-w WGT_CMD] [-x XTN_LST[,...]] [--xcl_var]
     [INPUT-FILES] [OUTPUT-FILE]

DESCRIPTION

   'ncremap' remaps the data file(s) in INPUT-FILE, in DRC_IN, or piped
through standard input, to the horizontal grid specified by (in
descending order of precedence) MAP_FL, GRD_DST, or DST_FL and stores
the result in OUTPUT-FILE(s).  If a vertical grid VRT_FL is provided,
'ncremap' will (also) vertically interpolate the input file(s) to that
grid.  When no INPUT-FILE is provided, 'ncremap' operates in "map-only"
mode where it exits after producing an annotated map-file.  'ncremap'
was introduced to NCO in version 4.5.4 (December, 2015).

   'ncremap' is a "super-operator" that orchestrates the regridding
features of several different programs including other NCO operators.
Under the hood NCO applies pre-computed remapping weights or, when
necessary, generates and infers grids, generates remapping weights
itself or calls external programs to generate the weights, and then
applies the weights (i.e., regrids).

   Unlike the rest of NCO, 'ncremap' and 'ncclimo' are shell scripts,
not compiled binaries(1).  As of NCO 4.9.2 (February, 2020), the
'ncclimo' and 'ncremap' scripts export the environment variable
'HDF5_USE_FILE_LOCKING' with a value of 'FALSE'.  This prevents failures
of these operators that can occur with some versions of the underlying
HDF library that attempt to lock files on file systems that cannot or do
not support it.  'ncremap' wraps the underlying regridder ('ncks') and
external executables to produce a friendly interface to regridding.
Without any external dependencies, 'ncremap' applies weights from a
pre-exisiting map-file to a source data file to produce a regridded
dataset.  Source and destination datasets may be on any Swath,
Curvilinear, Rectangular, or Unstructured Data (SCRUD) grid.  'ncremap'
will also use its own algorithms or, when necessary, external programs
ESMF's 'ESMF_RegridWeightGen' (ERWG) or TempestRemap's
'GenerateOverlapMesh'/'GenerateOfflineMap') to generate weights and
mapfiles.  In order to use the weight-generation options, either invoke
an internal NCO weight-generation algorithm (e.g., '--alg_typ=nco'), or
ensure that one or both of the external weight-generation packages is
installed and on your '$PATH'.  The recommended way to obtain ERWG is as
distributed in binary format.  Many (most?)  NCO users already have NCL
on their system(s), and NCL usually comes with ERWG.  Since about June,
2016, the Conda NCO package will also install ERWG (2).  Then be sure
the directory containing the ERWG executable is on your '$PATH' before
using 'ncremap'.  As a fallback, ERWG may also be installed from source:
<https://earthsystemcog.org/projects/esmf/download_last_public>.
'ncremap' can also generate and utilize mapfiles created by
TempestRemap, <https://github.com/ClimateGlobalChange/tempestremap>.
Until about April, 2019, TempestRemap had to be built from source
because there were no binary distributions of it.  As of NCO
version 4.8.0, released in May, 2019, the Conda NCO package
automatically installs the new TempestRemap Conda package so building
from source is not necessary.  Please contact those projects for support
on building and installing their software, which makes 'ncremap' more
functional and user-friendly.  Please ensure you have the latest version
of ERWG or TempestRemap before reporting any related problems to NCO.

   As mentioned above, 'ncremap' orchestrates the regridding features of
several different programs.  'ncremap' runs most quickly when it is
supplied with a pre-computed mapfile.  However, 'ncremap' will also
(call other programs to) compute mapfiles when necessary and when given
sufficient grid information.  Thus it is helpful to understand when
'ncremap' will and will not internally generate a mapfile.  Supplying
input data files and a pre-computed mapfile _without_ any other grid
information causes 'ncremap' to regrid the data files without first
pausing to internally generate a mapfile.  On the other hand, supplying
any grid information (i.e., using any of the '-d', '-G', '-g', or '-s'
switches described below), causes 'ncremap' to internally (re-)generate
the mapfile by combining the supplied and inferred grid information.  A
generated mapfile is given a default name unless a user-specified name
is supplied with '-m MAP_FL'.

Fields not regridded by 'ncremap'
---------------------------------

Most people ultimately use 'ncremap' to regrid data, yet not all data
can or should be regridded in the sense of applying a sparse-matrix of
weights to an input field to produce and output field.  Certain fields
(e.g., the longitude coordinate) specify the grid.  These fields must be
provided in order to compute the weights that are used to regrid.  The
regridded usually copies these fields "as is" directly into regridded
files, where they describe the destination grid, and replace or
supercede the source grid information.  Other fields are extensive grid
properties (e.g., the number of cells adjacent to a given cell) that may
apply only to the source (not the destination) grid, or be too difficult
to re-compute for the destination grid.  'ncremap' contains an internal
database of fields that it will not propagate or regrid.  First are
variables with names identical to the coordinate names found in an
ever-growing collection of publicly available geoscience datasets (CMIP,
NASA, etc.):

   'area', 'gridcell_area', 'gw', 'LAT', 'lat', 'Latitude', 'latitude',
'nav_lat', 'global_latitude0', 'latitude0', 'slat', 'TLAT', 'ULAT',
'XLAT', 'XLAT_M', 'CO_Latitude', 'S1_Latitude', 'lat_bnds',
'lat_vertices', 'latt_bounds', 'latu_bounds', 'latitude_bnds',
'LatitudeCornerpoints', 'bounds_lat', 'LON', 'lon', 'Longitude',
'longitude', 'nav_lon', 'global_longitude0', 'longitude0', 'slon',
'TLON', 'TLONG', 'ULON', 'ULONG', 'XLONG', 'XLONG_M', 'CO_Longitude',
'S1_Longitude', 'lon_bnds', 'lon_vertices', 'lont_bounds',
'lonu_bounds', 'longitude_bnds', 'LongitudeCornerpoints', 'bounds_lon',
and 'w_stag'.

   Files produced by MPAS models may contain these variables that will
not be regridded:

   'angleEdge', 'areaTriangle', 'cellsOnCell', 'cellsOnEdge',
'cellsOnVertex', 'dcEdge', 'dvEdge', 'edgeMask', 'edgesOnCell',
'edgesOnEdge', 'edgesOnVertex', 'indexToCellID', 'indexToEdgeID',
'indexToVertexID', 'kiteAreasOnVertex', 'latCell', 'latEdge',
'latVertex', 'lonCell', 'lonEdge', 'lonVertex', 'maxLevelEdgeTop',
'meshDensity', 'nEdgesOnCell', 'nEdgesOnEdge', 'vertexMask',
'verticesOnCell', 'verticesOnEdge', 'weightsOnEdge', 'xEdge', 'yEdge',
'zEdge', 'xVertex', 'yVertex', and 'zVertex'.

   Most of these fields that 'ncremap' will not regrid are also fields
that NCO size-and-rank-preserving operators will not modify, as
described in *note CF Conventions::.

Options specific to 'ncremap'
-----------------------------

The following summarizes features unique to 'ncremap'.  Features common
to many operators are described in *note Shared features::.

'-a ALG_TYP ('--alg_typ', '--algorithm', '--regrid_algorithm')'
     Specifies the interpolation algorithm for weight-generation for use
     by 'ESMF_RegridWeightGen' (ERWG), NCO, and/or TempestRemap.
     'ncremap' unbundles this algorithm choice from the rest of the
     weight-generator invocation syntax because users more frequently
     change interpolation algorithms than other options (that can be
     changed with '-W WGT_OPT').  'ncremap' can invoke all seven ERWG
     weight generation algorithms, one NCO algorithm, and eight
     TempestRemap algorithms.

     The seven ERWG weight generation algorithms are: 'bilinear'
     (default, acceptable abbreviations are 'bilin', 'blin', 'bln'),
     'conserve' (or 'conservative', 'cns', 'c1', or 'aave'),
     'conserve2nd' (or 'conservative2nd', 'c2', or 'c2nd') (NCO supports
     'conserve2nd' as of version 4.7.4 (April, 2018)), 'nearestdtos' (or
     'nds' or 'dtos' or 'ndtos'), 'neareststod' (or 'nsd' or 'stod' or
     'nstod'), and 'patch' (or 'pch' or 'patc').  See ERWG documentation
     here
     (http://www.earthsystemmodeling.org/esmf_releases/public/ESMF_6_3_0rp1/ESMF_refdoc/node3.html#SECTION03020000000000000000)
     for detailed descriptions of ERWG algorithms.

     'ncremap' implements its own internal weight-generation algorithm
     as of NCO version 4.8.0 (May, 2019).  The first and only NCO-native
     algorithm is a first-order conservative algorithm that is still
     under active development yet accurate enough for everyday use.
     Invoke this with 'nco_con' (or 'nco_cns', 'nco_conservative', or
     simply 'nco').  'ncremap' can invoke eight preconfigured
     TempestRemap weight-generation algorithms, and one generic
     algorithm ('tempest') for which users should provide their own
     options.  As of NCO version 4.7.2 (January, 2018), 'ncremap'
     implemented the six E3SM-recommended TempestRemap mapping
     algorithms between FV and SE flux, state, and other variables.
     'ncremap' originated some (we hope) common-sense names for these
     algorithms ('se2fv_flx', 'se2fv_stt', 'se2fv_alt', 'fv2se_flx',
     'fv2se_stt', and 'fv2se_alt'), and also allows more mathematically
     precise synonyms (shown below).  As of NCO version 4.9.0 (December,
     2019), 'ncremap' added two further boutique mappings ('fv2fv_flx'
     and 'fv2fv_stt').  Finally, the '-a tempest' algorithm can be
     specified with the precise TempestRemap options as arguments to the
     '-W' (or '--wgt_opt') option.  Note that support for the named
     algorithms requires TempestRemap version 2.0.0 or later (some
     option combinations fail with earlier versions).

     Generate and use the recommended weights to remap fluxes from SE to
     FV grids, for example, with
          ncremap -a se2fv_flx --src_grd=se.g --dst_grd=fv.nc -m map.nc
          ncremap -m map.nc in.nc out.nc
     This causes 'ncremap' to automatically invoke TempestRemap with the
     boutique options '--in_type cgll --in_np 4 --out_type fv --mono'
     that are recommended by E3SM for conservative and monotone
     remapping of fluxes.  TempestRemap options have the following
     meanings: 'mono' specifies a monotone remapping, i.e., one that
     does not generate any new extrema in the field variables.a 'cgll'
     indicates the input or output are represented by a continuous
     Galerkin method on Gauss-Lobatto-Legendre nodes.  This is
     appropriate for spectral element datasets.  (TempestRemap also
     supports, although NCO does not invoke, the 'dgll' option for a
     discontinuous Galerkin method on Gauss-Lobatto-Legendre nodes.)  It
     is equivalent to, yet simpler to remember and to invoke than
          ncremap -a tempest --src_grd=se.g --dst_grd=fv.nc -m map.nc \
                  -W '--in_type cgll --in_np 4 --out_type fv --mono'
     Specifying '-a tempest' without additional options in the '-W'
     clause causes TempestRemap to employ defaults.  The default
     configuration requires both input and output grids to be FV, and
     produces a conservative, non-monotonic mapping.  The '-a fv2fv'
     option described below may produce more desirable results than this
     default for many users.  Using '-a tempest' alone without other
     options for spectral element grids will lead to undefined and
     likely unintentional results.  In other words, '-a tempest' is
     intended to be used in conjunction with a '-W' option clause to
     supply your own combination of TempestRemap options that does not
     duplicate one of the boutique option collections that already has
     its own name.

     The full list of supported canonical algorithm names, their
     synonyms, and boutique options passed to 'GenerateOfflineMap' are:
     'se2fv_flx' (synonyms 'mono_se2fv', 'conservative_monotone_se2fv')
          Options: '--in_type cgll --in_np 4 --out_type fv --mono'
     'fv2se_flx' (synonyms 'monotr_fv2se', 'conservative_monotone_fv2se'),
          Options: '--in_type cgll --in_np 4 --out_type fv --mono'.  For
          'fv2se_flx' the weights are generated with options identical
          to 'se2fv_flx', and then the transpose of the resulting weight
          matrix is employed.
     'se2fv_stt' (synonyms 'highorder_se2fv', 'accurate_conservative_nonmonotone_se2fv'),
          Options: '--in_type cgll --in_np 4 --out_type fv --out_double'
     'fv2se_stt' (synonyms 'highorder_fv2se', 'accurate_conservative_nonmonotone_fv2se'),
          Options: '--in_type fv --in_np 2 --out_type cgll --out_np 4
          --volumetric'
     'se2fv_alt' (synonyms 'intbilin_se2fv', 'accurate_monotone_nonconservative_se2fv'),
          Options: '--in_type cgll --in_np 4 --out_type fv --mono3
          --noconserve'
     'fv2se_alt' (synonyms 'mono_fv2se', 'conservative_monotone_fv2se_alt'),
          Options: '--in_type fv --in_np 1 --out_type cgll --out_np 4
          --mono --volumetric'
     'se2se' (synonyms 'cs2cs', 'conservative_monotone_se2se'),
          Options: '--in_type cgll --in_np 4 --out_type cgll --out_np 4
          --mono'
     'fv2fv' (synonyms 'rll2rll'),
          Options: '--in_type fv --in_np 2 --out_type fv'
     'fv2fv_flx' (synonyms 'fv2fv_mono', 'conservative_monotone_fv2fv'),
          Options: '--in_type fv --in_np 1 --out_type fv'
     'fv2fv_stt' (synonyms 'fv2fv_highorder', 'accurate_conservative_nonmonotone_fv2fv'),
          Options: '--in_type fv --in_np 2 --out_type fv'
     Thus these boutique options are specialized for SE grids with
     fourth order resolution (NP = 4).  Full documentation of the
     E3SM-recommended boutique options for TempestRemap is here
     (https://acme-climate.atlassian.net/wiki/spaces/Docs/pages/178848194/Transition+to+TempestRemap+for+Atmosphere+grids)
     (may require E3SM-authorization to view).  Let us know if you would
     like other boutique TempestRemap switch sets added as canonical
     options for 'ncremap'.

'--a2o ('--a2o', '--atm2ocn', '--b2l', '--big2ltl', '--l2s', '--lrg2sml')'
     Use one of these flags (that take no arguments) to cause
     TempestRemap to generate mapping weights from a source grid that
     has more coverage than the destination grid, i.e., the destination
     grid is a subset of the source.  When computing the intersection of
     two meshes, TempestRemap uses an algorithm (in an executable named
     'GenerateOverlapMesh') that expects the mesh with less coverage to
     be the first grid, and the grid with greater coverage to be the
     second, regardless of the mapping direction.  By default, 'ncremap'
     supplies the source grid first and the destination second, but this
     order causes 'GenerateOverlapMesh' (which is agnostic about
     ordering for grids of equal coverage) to fail when the source grid
     covers regions not in the destination grid.  For example, a global
     atmosphere grid has more coverage than a global ocean grid, so that
     remapping from atmosphere-to-ocean would require invoking the
     '--atm2ocn' switch:
          # Use --a2o to generate weights for "big" to "little" remaps:
          ncremap --a2o -a se2fv_flx --src_grd=atm_se_grd.nc \
                        --dst_grd=ocn_fv_grd.nc -m map.nc
          # Otherwise, omit it:
          ncremap       -a fv2se_flx --src_grd=ocn_fv_grd.nc \
                        --dst_grd=atm_se_grd.nc -m map.nc
          ncremap       -a se2fv_flx --src_grd=atm_se_grd.nc \
                        --dst_grd=atm_fv_grd.nc -m map.nc
          # Only necessary when generating, not applying, weights:
          ncremap -m atm2ocn.nc in.nc out.nc
     As shown in the second example above, remapping from global
     ocean-to-atmosphere grids does not require (and should not invoke)
     this switch.  The third example shows that the switch is only
     needed when _generating_ weights, not when applying them.  The
     switch is never needed (and is ignored) when generating weights
     with ERWG (which constructs the intersection mesh with a different
     algorithm than TempestRemap).  Attempting to remap a larger source
     grid to a subset destination grid without using '--a2o' causes
     'GenerateOverlapMesh' to emit an error (and a potential workaround)
     like this:
          ....Nearest target face 130767
          ....ERROR: No overlapping face found
          ....This may be caused by mesh B being a subset of mesh A
          ....Try swapping order of mesh A and B, or override with \
              --allow_no_overlap
          ....EXCEPTION (../src/OverlapMesh.cpp, Line 1738) Exiting
     The '--a2o' switch and its synonyms are available in NCO version
     4.7.3 (March, 2018) and later.  Currently (January, 2018) 'ncremap'
     has no means of transmitting options (like '--allow_no_overlap' to
     'GenerateOverlapMesh'.  Please let us know if this capability is
     important to you.

'--version ('--version', '--vrs', '--config', '--configuration', '--cnf')'
     This switch (which takes no argument) causes the operator to print
     its version and configuration.  This includes the copyright notice,
     URLs to the BSD and NCO license, directories from which the NCO
     scripts and binaries are running, and the locations of any separate
     executables that may be used by the script.

'--d2f ('--d2f', '--d2s', '--dbl_flt', '--dbl_sgl', '--double_float')'
     This switch (which takes no argument) demotes all double precision
     non-coordinate variables to single precision.  Internally 'ncremap'
     invokes 'ncpdq' to apply the 'dbl_flt' packing map to an
     intermediate version of the input file before regridding it.  This
     switch has no effect on files that are not regridded.  To demote
     the precision in such files, use 'ncpdq' to apply the 'dbl_flt'
     packing map to the file directly.  Files without any double
     precision fields will be unaltered.

'-D DBG_LVL ('--dbg_lvl', '--dbg', '--debug', '--debug_level')'
     Specifies a debugging level similar to the rest of NCO.  If DBG_LVL
     = 1, 'ncremap' prints more extensive diagnostics of its behavior.
     If DBG_LVL = 2, 'ncremap' prints the commands it would execute at
     any higher or lower debugging level, but does not execute these
     commands.  If DBG_LVL > 2, 'ncremap' prints the diagnostic
     information, executes all commands, and passes-through the
     debugging level to the regridder ('ncks') for additional
     diagnostics.

''--devnull=DVN_FLG' ('--devnull', '--dev_nll', '--dvn_flg')'
     The DVN_FLG controls whether 'ncremap' suppresses regridder output
     or sends it to '/dev/null'.  The default value of DVN_FLG is "Yes",
     so that 'ncremap' prints little output to the terminal.  Set
     DVN_FLG to "No" to allow the internal regridder executables (mainly
     'ncks') to send their output to the terminal.

'--dpt ('--dpt', '--add_dpt', '--depth', '--add_depth')'
'--dpt_fl=DPT_FL ('--dpt_fl', '--depth_file', '--mpas_fl', '--mpas_depth')'
     The '--dpt' switch (which takes no argument) and the
     '--dpt_fl=DPT_FL' option which automatically sets the switch and
     also takes a filename argument, both control the addition of a
     depth coordinate to MPAS ocean datasets.  Depth is the vertical
     distance below sea surface and, like pressure in the atmosphere, is
     an important vertical coordinate whose explicit values are often
     omitted from datasets yet may be computed from other variables
     (gridbox thickness, pressure difference) and grid information.
     Moreover, users are often more interested in the approximate depth,
     aka reference depth, of a given ocean layer independent of its
     horizontal position.  To invoke either of these options first
     obtain and place the 'add_depth.py' command on the executable path
     (i.e., '$PATH'), and use 'ncremap --config' to verify that it is
     found.  These options tell 'ncremap' to invoke 'add_depth.py' which
     uses the 'refBottomDepth' variable in the current data file or, if
     specified, the DPT_FL, to create and add a depth coordinate to the
     current file (before regridding).

     As of NCO version 4.7.9 (February, 2019), the depth coordinate is
     an approximate, one-dimensional, globally uniform coordinate that
     neglects horizontal variations in depth that can occur near strong
     bathymetry or under ice shelves.  Like its atmospheric counterpart
     in many models, the 'lev' pressure-coordinate, 'depth' is useful
     for plotting purposes and global studies.  It would not be
     difficult to modify these options to add other depth information
     based on the 3D cell-thickness field to ocean files (please ask
     Charlie if interested in this).

'-d DST_FL ('--dst_fl', '--destination_file', '--tpl', 'tpl_fl', '--template_file', '--template')'
     Specifies a data file to serve as a template for inferring the
     destination grid.  Currently DST_FL must be a data file (not a
     gridfile, SCRIP or otherwise) from which NCO can infer the
     destination grid.  The more coordinate and boundary information and
     metadata the better NCO will do at inferring the grid.  If DST_FL
     has cell boundaries then NCO will use those.  If DST_FL has only
     cell-center coordinates (and no edges), then NCO will guess-at (for
     rectangular grids) or interpolate (for curvilinear grids) the
     edges.  Unstructured grids must supply cell boundary information,
     as it cannot be interpolated or guessed-at.  NCO only reads
     coordinate and grid data and metadata from DST_FL.  DST_FL is not
     modified, and may have read-only permissions.

'--dt_sng=DT_SNG ('--dt_sng', '--date_string')'
     Specifies the date-string use in the full name of map-files created
     in MWF mode.  Map-file names include, by convention, a string to
     indicate the approximate date (and thus algorithm versions
     employed) of weight generation.  'ncremap' uses the DT_SNG argument
     to encode the date into output map-file names of this format:
     'map_NM_SRC_to_NM_DST_ALG_TYP.DT_SNG.nc'.  MWF mode defaults DT_SNG
     to the current date in 'YYYYMMDD'-format.

'--esmf_typ=ESMF_TYP ('--esmf_typ', '--esmf_mth', '--esmf_extrap_type', '--esmf_extrap_method')'
     Specifies the extrapolation method used to compute unmapped
     destination point values with the ERWG weight generator.  Valid
     values, their synonyms, and their meanings are 'neareststod'
     (synonyms 'stod' and 'nsd') which uses the nearest valid source
     value, 'nearestidavg' (synonyms 'idavg' and 'id') which uses an
     inverse distance-weighted (with an exponent of ESMF_XPN) average of
     the nearest ESMF_NSP valid source values, and 'none' (synonyms
     'nil' and 'nowaydude') which forbids extrapolation.  Default is
     ESMF_TYP = 'none'.  The arguments to options '--esmf_xpn=ESMF_XPN'
     (which defaults to 2.0) and '--esmf_nsp=ESMF_NSP' (which defaults
     to 8) set the parameters that control the extrapolation
     'nearestidavg' algorithm.  For more information on ERWG
     extrapolation, see documentation here
     (http://www.earthsystemmodeling.org/esmf_releases/last_built/ESMF_refdoc/node3.html#SECTION03022300000000000000).
     NCO supports this feature as of version 4.7.4 (April, 2018).

'--esmf_nsp=ESMF_NSP ('--esmf_nsp', '--esmf_pnt_src_nbr', '--esmf_extrap_num_src_pnts')'
     Specifies the number of source points to use in extrapolating
     unmapped destination point values with the ERWG weight generator.
     This option is only useful in conjunction with explicitly requested
     extrapolation types ESMF_TYP = 'neareststod' and ESMF_TYP =
     'nearestidavg'.  Default is ESMF_NSP = 8.  For more information on
     ERWG extrapolation, see documentation here
     (http://www.earthsystemmodeling.org/esmf_releases/last_built/ESMF_refdoc/node3.html#SECTION03022300000000000000).
     NCO supports this feature as of version 4.7.4 (April, 2018).

'--esmf_xpn=ESMF_XPN ('--esmf_xpn', '--esmf_pnt_src_nbr', '--esmf_extrap_num_src_pnts')'
     Specifies the number of source points to use in extrapolating
     unmapped destination point values with the ERWG weight generator.
     This option is only useful in conjunction with explicitly requested
     extrapolation types ESMF_TYP = 'neareststod' and ESMF_TYP =
     'nearestidavg'.  Default is ESMF_XPN = 2.0.  For more information
     on ERWG extrapolation, see documentation here
     (http://www.earthsystemmodeling.org/esmf_releases/last_built/ESMF_refdoc/node3.html#SECTION03022300000000000000).
     NCO supports this feature as of version 4.7.4 (April, 2018).

'-g GRD_DST ('--grd_dst', '--grid_dest', '--dest_grid', '--destination_grid')'
     Specifies the destination gridfile.  An existing gridfile may be in
     any format accepted by the weight generator.  NCO will use ERWG or
     TempestRemap to combine GRD_DST with a source gridfile (either
     inferred from INPUT-FILE, supplied with '-s GRD_SRC', or generated
     from '-G GRD_SNG') to produce remapping weights.  When GRD_DST is
     used as input, it is not modified, and may have read-only
     permissions.  When GRD_DST is inferred from INPUT-FILE or created
     from GRD_SNG, it will be generated in SCRIP format.

     As of NCO version 4.6.8 (August, 2017), 'ncremap' supports most of
     the file format options that the rest of NCO has long supported
     (*note File Formats and Conversion::).  This includes short flags
     (e.g., '-4') and key-value options (e.g., '--fl_fmt=netcdf4')
     though not long-flags without values (e.g., '--netcdf4').  However,
     'ncremap' can only apply the full suite of file format options to
     files that it creates, i.e., regridded files.  The weight
     generators (ERWG and TempestRemap) are limited in the file formats
     that they read and write.  Currently (August, 2017), ERWG supports
     'CLASSIC', '64BIT_OFFSET', and 'NETCDF4', while TempestRemap
     supports only 'CLASSIC'.  These can of course be converted to other
     formats using 'ncks' (*note File Formats and Conversion::).
     However, map-files _produced_ in other non-'CLASSIC' formats can
     remap significantly larger grids than 'CLASSIC'-format map-files.

'-G GRD_SNG ('--grd_sng', '--grid_generation', '--grid_gen', '--grid_string')'
     Specifies, with together with other options, a source gridfile to
     create(3).  'ncremap' creates the gridfile in SCRIP format by
     default, and then, should the requisite options for regridding be
     present, combines that with the destination grid (either inferred
     from INPUT-FILE or supplied with '-g GRD_DST' and generates mapping
     weights.  Manual grid-file generation is not frequently used since
     'ncremap' can infer many grids directly from the INPUT-FILE, and
     few users wish to keep track of SCRIP grids when they can be easily
     regenerated as intermediate files.  This option also allows one to
     visually tune a grid by rapidly generating candidates and
     inspecting the results.

     If a desired grid-file is unavailable, and no dataset on that grid
     is available (so inferral cannot be used), then one must manually
     create a new grid.  Users create new grids for many reasons
     including dataset intercomparisons, regional studies, and
     fine-tuned graphics.  NCO and 'ncremap' support manual generation
     of the most common rectangular grids as SCRIP-format grid-files.
     Create a grid by supplying 'ncremap' with a grid-file name and
     "grid-formula" (GRD_SNG) that contains, at a minimum, the
     grid-resolution.  The grid-formula is a hash-separated string of
     name-value pairs each representing a grid parameter.  All
     parameters except grid resolution have reasonable defaults, so a
     grid-formula can be as simple as 'latlon=180,360':
          ncremap -g grd.nc -G latlon=180,360
     The SCRIP-format grid-file 'grd.nc' is a valid source or
     destination grid for 'ncremap' and other regridders.

     Grid-file generation documentation in the NCO Users Guide at
     <http://nco.sf.net/nco.html#grid> describes all the grid parameters
     and contains many examples.  Note that the examples in this section
     use grid generation API for 'ncremap' version 4.7.6 (August, 2018)
     and later.  Earlier versions can use the 'ncks' API explained at
     *note Grid Generation:: in the Users Guide.

     The most useful grid parameters (besides resolution) are latitude
     type (LAT_TYP), longitude type (LON_TYP), title (TTL), and, for
     regional grids, the SNWE bounding box (SNWE).  The three supported
     varieties of global rectangular grids are Uniform/equiangular
     (LAT_TYP='uni'), Cap/FV (LAT_TYP='cap'), and Gaussian
     (LAT_TYP='gss').  The four supported varieties of longitude types
     are the first (westernmost) gridcell centered at Greenwich
     (LON_TYP='grn_ctr'), western edge at Greenwish ('grn_wst'), or at
     the Dateline (LON_TYP='180_ctr' and LON_TYP='180_wst',
     respectively).  Grids are global, uniform, and have their first
     longitude centered at Greenwich by default.  The grid-formula for
     this is 'lat_typ=uni#lon_typ=grn_ctr'.  Some examples (remember,
     this API requires NCO 4.7.6+):
          ncremap -g grd.nc -G latlon=180,360                 # 1x1 Uniform grid
          ncremap -g grd.nc -G latlon=180,360#lat_drc=n2s     # 1x1 Uniform grid, N->S not S->N
          ncremap -g grd.nc -G latlon=180,360#lon_typ=grn_wst # 1x1 Uniform grid, Greenwich-west edge
          ncremap -g grd.nc -G latlon=129,256#lat_typ=cap     # 1.4x1.4  FV grid
          ncremap -g grd.nc -G latlon=94,192#lat_typ=gss      # T62 Gaussian grid
          ncremap -g grd.nc -G latlon=94,192#lat_typ=gss#lat_drc=n2s # NCEP2 T62 Gaussian grid 
     Regional grids are a powerful tool in regional process analyses,
     and can be much smaller in size than global datasets.  Regional
     grids are always uniform.  Specify the rectangular bounding box,
     i.e., the outside edges of the region, in SNWE order:
          ncremap -g grd.nc -G ttl="Equi-Angular 1x1 Greenland grid"#latlon=30,90#snwe=55.0,85.0,-90.0,0.0

'-I IN_DRC ('--in_drc', '--drc_in', '--dir_in', '--in_dir', 'input')'
     Specifies the input directory, i.e., the directory which contains
     the input file(s).  If IN_FL is also specified, then the input
     filepath is constructed by appending a slash and the filename to
     the directory: 'IN_DRC/IN_FL'.  Specifying IN_DRC without IN_FL
     causes 'ncremap' to attempt to remap every file in IN_DRC that ends
     with one of these suffixes: '.nc', '.nc3', '.nc4', '.nc5', '.nc6',
     '.nc7', '.cdf', '.hdf', '.he5', or '.h5'.  When multiple files are
     regridded, each output file takes the name of the corresponding
     input file.  There is no namespace conflict because the input and
     output files are in separate directories.  Note that 'ncremap' can
     instead accept a list of input files through standard input (e.g.,
     'ls *.nc | ncremap ...') or as positional command-line arguments
     (e.g., 'ncremap in1.nc in2.nc ...').

'-i IN_FL ('--in_fl', '--in_file', '--input_file')'
     Specifies the file containing data on the source grid to be
     remapped to the destination grid.  When provided with the optional
     MAP_FL, 'ncremap' only reads data from IN_FL in order to regrid it.
     Without the optional MAP_FL or SRC_GRD, 'ncremap' will try to infer
     the source grid from IN_FL, and so must read coordinate and
     metatdata information from IN_FL.  In this case the more coordinate
     and boundary information and metadata, the better NCO will do at
     inferring the source grid.  If IN_FL has cell boundaries then NCO
     will use those.  If IN_FL has only cell-center coordinates (and no
     edges), then NCO will guess (for rectangular grids) or interpolate
     (for curvilinear grids) the edges.  Unstructured grids must supply
     cell boundary information, as it cannot be interpolated or
     guessed-at.  IN_FL is not modified, and may have read-only
     permissions.  Note that 'ncremap' can instead accept input file
     name(s) through standard input (e.g., 'ls *.nc | ncremap ...') or
     as positional command-line arguments (e.g., 'ncremap in1.nc in2.nc
     ...').  When one or three-or-more positional arguments are given,
     they are all interpreted as input filename(s).  Two positional
     arguments are interpreted as a single INPUT-FILE and its
     corresponding OUTPUT-FILE.

'-j JOB_NBR ('--job_nbr', '--job_number', '--jobs')'
     Specifies the number of simultaneous regridding processes to spawn
     during parallel execution for both Background and MPI modes.  In
     both parallel modes 'ncremap' spawns processes in batches of
     JOB_NBR jobs, then waits for those processes to complete.  Once a
     batch finishes, 'ncremap' spawns the next batch.  In Background
     mode, all jobs are spawned to the local node.  In MPI mode, all
     jobs are spawned in round-robin fashion to all available nodes
     until JOB_NBR jobs are running.

     If regridding consumes so much RAM (e.g., because variables are
     large and/or the number of threads is large) that a single node can
     perform only one regridding job at a time, then a reasonable value
     for JOB_NBR is the number of nodes, NODE_NBR.  Often, however,
     nodes can regrid multiple files simultaneously.  It can be more
     efficient to spawn multiple jobs per node than to increase the
     threading per job because I/O contention for write access to a
     single file prevents threading from scaling indefinitely.

     By default JOB_NBR = 2 in Background mode, and JOB_NBR = NODE_NBR
     in MPI mode.  This helps prevent users from overloading nodes with
     too many jobs.  Subject to the availability of adequate RAM, expand
     the number of jobs per node by increasing JOB_NBR until, ideally,
     each core on the node is used.  Remember that processes and
     threading are multiplicative in core use.  Four jobs each with four
     threads each consumes sixteen cores.

     As an example, consider regridding 100 files with a single map.
     Say you have a five-node cluster, and each node has 16 cores and
     can simultaneously regrid two files using eight threads each.  (One
     needs to test a bit to optimize these parameters.)  Then an optimal
     (in terms of wallclock time) invocation would request five nodes
     with 10 simultaneous jobs of eight threads.  On many batch systems
     this would involve a scheduler command like this 'qsub -l nodes=5
     ...' followed by 'ncremap -p mpi -j 10 -t 8 ...'.  This job will
     likely complete between five and ten-times faster than a serial
     invocation of 'ncremap'.  The uncertainty range is due to
     unforeseeable, system-dependent load and I/O charateristics.  Nodes
     that can simultaneously write to more than one file fare better
     with multiple jobs per node.  Nodes with only one I/O channel to
     disk may be better exploited by utilizing more threads per process.

'-M ('--mlt_map', '--multimap', '--no_multimap', '--nomultimap')'
     'ncremap' assumes that every input file is on a unique grid unless
     a source gridfile is specified (with '-s GRD_SRC') or
     multiple-mapfile generation is explicitly turned-off (with '-M').
     The '-M' switch is a toggle, it requires and accepts no argument.
     Toggling '-M' tells 'ncremap' to generate at most one mapfile
     regardless of the number of input files.  If '-M' is not toggled
     (and neither '-m MAP_FL' nor '-s GRD_SRC' is invoked) then
     'ncremap' will generate a new mapfile for each input file.
     Generating new mapfiles for each input file is necessary for
     processing batches of data on different grids (e.g., swath-like
     data), and slow, tedious, and unnecessary when batch processing
     data on the same grids.

'-m MAP_FL ('--map_fl', '--map', '--map_file', '--rgr_map', '--regrid_map')'
     Specifies a mapfile (i.e., weight-file) to remap the source to
     destination grid.  If MAP_FL is specified in conjunction with any
     of the '-d', '-G', '-g', or '-s' switches, then 'ncremap' will name
     the internally generated mapfile MAP_FL.  Otherwise (i.e., if none
     of the source-grid switches are used), 'ncremap' assumes that
     MAP_FL is a pre-computed mapfile.  In that case, the MAP_FL must be
     in SCRIP format, although it may have been produced by any
     application (usually ERWG or TempestRemap).  If MAP_FL has only
     cell-center coordinates (and no edges), then NCO will guess-at or
     interpolate the edges.  If MAP_FL has cell boundaries then NCO will
     use those.  A pre-computed MAP_FL is not modified, and may have
     read-only permissions.  The user will be prompted to confirm if a
     newly generated map-file named MAP_FL would overwrite an existing
     file.  'ncremap' adds provenance information to any newly generated
     map-file whose name was specified with '-m MAP_FL'.  This
     provenance includes a 'history' attribute that contains the command
     invoking 'ncremap', and the map-generating command invoked by
     'ncremap'.

'-m MSH_FL ('--msh_fl', '--msh', '--mesh', '--mesh_file')'
     Specifies a meshfile (aka intersection mesh, aka overlap mesh) that
     stores the grid formed by the intersection of the source and
     destination grids.  If not specified then 'ncremap' will name any
     internally generated meshfile with a temporary name and delete the
     file prior to exiting.  NCO and TempestRemap support archiving the
     meshfile, and ERWG does not.  NCO stores the meshfile in SCRIP
     format, while TempestRemap stores it in Exodus format (with a '.g'
     suffix).  'ncremap' adds provenance information to any newly
     generated mesh-file whose name was specified with
     '--msh_fl=MSH_FL'.  This provenance includes a 'history' attribute
     that contains the command invoking 'ncremap', and the
     map-generating command invoked by 'ncremap'.

'--msk_dst=MSK_DST ('--msk_dst', '--dst_msk', '--mask_destination', '--mask_dst')'
     Specifies a template variable to use for the integer mask of the
     destination grid when inferring grid files and/or creating
     map-files (i.e., generating weights).  Any variable on the same
     horizontal grid as a data file can serve as a mask template for
     that grid.  The mask will be one (i.e., gridcells will participate
     in regridding) where MSK_DST has valid values in the data file from
     which NCO infers the destination grid.  The mask will be zero
     (i.e., gridcells will not participate in regridding) where MSK_NM
     has a missing value.  A typical example of this option would be to
     use Sea-surface Temperature (SST) as a template variable for an
     ocean mask because SST is often defined only over ocean, and
     missing values might denote locations to which regridded quantities
     should never be placed.  The special value MSK_DST = 'none'
     prevents the regridder from inferring and treating any variable
     (even one named, e.g., 'mask') in a source file as a mask variable.
     This guarantees that all points in the inferred destination grid
     will be unmasked.  MSK_DST, MSK_OUT, and MSK_SRC are related yet
     distinct: MSK_DST is the mask template variable in the destination
     file (whose grid will be inferred), MSK_OUT is the name to give the
     destination mask in any regridded file, and MSK_SRC is the mask
     template variable in the source file (whose grid will be inferred).
     MSK_SRC and MSK_DST only affect inferred grid files for the source
     and destination grids, respectively, whereas MSK_OUT only affects
     regridded files.

'--msk_out=MSK_OUT ('--msk_out', '--out_msk', '--mask_destination', '--mask_out')'
     Use of this option tells 'ncremap' to include a variable named
     MSK_OUT in any regridded file.  The variable MSK_OUT will contain
     the integer-valued regridding mask on the destination grid.  The
     mask will be one (i.e., fields may have valid values in this
     gridcell) or zero (i.e., fields will have missing values in this
     gridcell).  By default, 'ncremap' does not output the destination
     mask to the regridded file.  This option changes that default
     behavior and causes 'ncremap' to ingest the default destination
     mask variable contained in the MAP-FILE.  ERWG generates
     SCRIP-format map-files that contain the destination mask in the
     variable named 'mask_b'.  SCRIP generates map-files that contain
     the destination mask in the variable named 'dst_grid_imask'.  The
     'msk_out' option works with map-files that adhere to either of
     these conventions.  Tempest generates map-files that do not
     typically contain the destination mask, and so the 'msk_out' option
     has no effect on files that Tempest regrids.  MSK_DST, MSK_OUT, and
     MSK_SRC are related yet distinct: MSK_DST is the mask template
     variable in the destination file (whose grid will be inferred),
     MSK_OUT is the name to give the destination mask in any regridded
     file, and MSK_SRC is the mask template variable in the source file
     (whose grid will be inferred).  MSK_SRC and MSK_DST only affect
     inferred grid files for the source and destination grids,
     respectively, whereas MSK_OUT only affects regridded files.

'--msk_src=MSK_SRC ('--msk_src', '--src_msk', '--mask_source', '--mask_src')'
     Specifies a template variable to use for the integer mask of the
     source grid when inferring grid files and/or creating map-files
     (i.e., generating weights).  Any variable on the same horizontal
     grid as a data file can serve as a mask template for that grid.
     The mask will be one (i.e., gridcells will participate in
     regridding) where MSK_SRC has valid values in the data file from
     which NCO infers the source grid.  The mask will be zero (i.e.,
     gridcells will not participate in regridding) where MSK_NM has a
     missing value.  A typical example of this option would be to use
     Sea-surface Temperature (SST) as a template variable for an ocean
     mask because SST is often defined only over ocean, and missing
     values might denote locations from which regridded quantities
     should emanate.  The special value MSK_SRC = 'none' prevents the
     regridder from inferring and treating any variable (even one named,
     e.g., 'mask') in a source file as a mask variable.  This guarantees
     that all points in the inferred source grid will be unmasked.
     MSK_DST, MSK_OUT, and MSK_SRC are related yet distinct: MSK_DST is
     the mask template variable in the destination file (whose grid will
     be inferred), MSK_OUT is the name to give the destination mask in
     any regridded file, and MSK_SRC is the mask template variable in
     the source file (whose grid will be inferred).  MSK_SRC and MSK_DST
     only affect inferred grid files for the source and destination
     grids, respectively, whereas MSK_OUT only affects regridded files.

'--mss_val=MSS_VAL ('--mss_val', '--fll_val', '--missing_value', '--fill_value')'
     Specifies the numeric value that indicates missing data when
     processing MPAS datasets, i.e., when '-P mpas' is invoked.  The
     default missing value is '-9.99999979021476795361e+33' which is
     correct for the MPAS ocean and sea-ice models.  Currently (January,
     2018) the MPAS land-ice model uses '-1.0e36' for missing values.
     Hence this option is usually invoked as '--mss_val=-1.0e36' to
     facilitate processing of MPAS land-ice datasets.

'-n NCO_OPT ('--nco_opt', '--nco_options', '--nco')'
     Specifies a string of options to pass-through unaltered to 'ncks'.
     NCO_OPT defaults to '-O --no_tmp_fl'.

'--nm_dst=NM_DST ('--nm_dst', '--name_dst', '--name_short_destination', '--nm_sht_dst')'
     Specifies the short name for the destination grid to use in the
     full name of map-files created in MWF mode.  Map-file names
     include, by convention, shortened versions of both the source and
     destination grids.  'ncremap' uses the NM_DST argument to encode
     the destination grid name into the output map-file name of this
     format: 'map_NM_SRC_to_NM_DST_ALG_TYP.DT_SNG.nc'.  MWF mode
     requires this argument, there is no default.

'--nm_src=NM_SRC ('--nm_src', '--name_src', '--name_short_source', '--nm_sht_src')'
     Specifies the short name for the source grid to use in the full
     name of map-files created in MWF mode.  Map-file names include, by
     convention, shortened versions of both the source and destination
     grids.  'ncremap' uses the NM_DST argument to encode the source
     grid name into the output map-file name of this format:
     'map_NM_SRC_to_NM_DST_ALG_TYP.DT_SNG.nc'.  MWF mode requires this
     argument, there is no default.

'--no_cll_msr ('--no_cll_msr', '--no_cll', '--no_cell_measures', '--no_area')'
     This switch (which takes no argument) controls whether 'ncclimo'
     and 'ncremap' add measures variables to the extraction list along
     with the primary variable and other associated variables.  See
     *note CF Conventions:: for a detailed description.

'--no_frm_trm ('--no_frm_trm', '--no_frm', '--no_formula_terms')'
     This switch (which takes no argument) controls whether 'ncclimo'
     and 'ncremap' add formula variables to the extraction list along
     with the primary variable and other associated variables.  See
     *note CF Conventions:: for a detailed description.

'--no_stg_grd ('--no_stg_grd', '--no_stg', '--no_stagger', '--no_staggered_grid')'
     This switch (which takes no argument) controls whether regridded
     output will contain the staggered grid coordinates 'slat', 'slon',
     and 'w_stag' (*note Regridding::).  By default the staggered grid
     is output for all files regridded from a Cap (aka FV) grid, except
     when the regridding is performed as part of splitting (reshaping)
     into timeseries.

'-O OUT_DRC ('--out_drc', '--drc_out', '--dir_out', '--out_dir', '--output')'
     Specifies the output directory, i.e., the directory name to contain
     the output file(s).  If OUT_FL is also specified, then the output
     filepath is constructed by appending a slash and the filename to
     the directory: 'OUT_DRC/OUT_FL'.  Specifying OUT_DRC without OUT_FL
     causes 'ncremap' to name each output file the same as the
     corresponding input file.  There is no namespace conflict because
     the input and output files will be in separate directories.

'-o OUT_FL ('--out_fl', '--output_file', '--out_file')'
     Specifies the output filename, i.e., the name of the file to
     contain the data from IN_FL remapped to the destination grid.  If
     OUT_FL already exists it will be overwritten.  Specifying OUT_FL
     when there are multiple input files (i.e., from using '-I IN_DRC'
     or standard input) generates an error (output files will be named
     the same as input files).  Two positional arguments are interpreted
     as a single INPUT-FILE and its corresponding OUTPUT-FILE.

'-P PRC_TYP ('--prc_typ', '--pdq_typ', '--prm_typ', '--procedure')'
     Specifies the permutation mode desired.  As of NCO version 4.5.5
     (February, 2016), one can tell 'ncremap' to invoke special
     processing procedures for different types of input data.  For
     instance, to automatically permute the dimensions in the data file
     prior to regridding for a limited (though growing) number of
     data-file types that encounter the 'ncremap' limitation concerning
     dimension ordering.  Valid procedure types are 'airs' for NASA AIRS
     satellite data, 'elm' or 'clm' DOE ELM and NCAR CLM model data,
     'cice' for CICE ice model data on 2D grids, 'mpas' for MPAS
     ocean/ice model data on unstructured grids, 'mod04' for Level 2
     MODIS MOD04 product, 'mwf' for making all weight-files for a pair
     of grids, 'sgs' for datasets containing sub-gridscale (SGS) data
     (such as CLM/CTSM/ELM land model data and CICE/MPAS-Seaice sea-ice
     model data), and 'nil' (for none).  The default PRC_TYP is 'nil',
     which means 'ncremap' does not perform any special procedure prior
     to regridding.  The AIRS procedure calls 'ncpdq' to permute
     dimensions from their order in the input file to this order:
     'StdPressureLev,GeoTrack,GeoXTrack'.  The ELM, CLM, and CICE
     procedures set idiosyncratic model values and then invoke the
     Sub-gridscale (SGS) procedure (see below).  The MOD04 procedure
     unpacks input data.  The MPAS procedure permutes input data
     dimensions into this order:
     'Time,depth,nVertLevels,nVertLevelsP1,maxEdges,MaxEdges2,nCategories,R3,ONE,TWO,FOUR,nEdges,nCells',
     and invokes renormalization.  An MPAS dataset that contains any
     other dimensions will fail to regrid until/unless those dimensions
     are added to the 'ncremap' dimension permutation option.

     MWF-mode:
     As mentioned above in other options, 'ncremap' includes an MWF-mode
     (for "Make All Weight Files") that generates and names, with one
     command and in a self-consistent manner, all combinations of (for
     instance, E3SM or CESM) global atmosphere<->ocean maps with both
     ERWG and Tempest.  MWF-mode automates the laborious and error-prone
     process of generating numerous map-files with various switches.
     Its chief use occurs when developing and testing new global
     grid-pairs for the E3SM atmosphere and ocean components.  Invoke
     MWF-mode with a number of specialized options to control the naming
     of the output map-files:
          ncremap -P mwf -s grd_ocn -g grd_atm --nm_src=ocn_nm \
                  --nm_dst=atm_nm --dt_sng=date
     where GRD_OCN is the "global" ocean grid, GRD_ATM, is the global
     atmosphere grid, NM_SRC sets the shortened name for the source
     (ocean) grid as it will appear in the output map-files, NM_DST
     sets, similarly, the shortend named for the destination
     (atmosphere) grid, and DT_SNG sets the date-stamp in the output
     map-file name 'map_${NM_SRC}_to_${NM_DST}_${ALG_TYP}.${DT_SNG}.nc'.
     Setting NM_SRC, NM_DST, and DT_SNG, is optional though highly
     recommended.  For example,
          ncremap -P mwf -s ocean.RRS.30-10km_scrip_150722.nc \
            -g t62_SCRIP.20150901.nc --nm_src=oRRS30to10 --nm_dst=T62 \
            --dt_sng=20180901 
     produces the 10 ERWG map-files:
       1. 'map_oRRS30to10_to_T62_aave.20180901.nc'
       2. 'map_oRRS30to10_to_T62_blin.20180901.nc'
       3. 'map_oRRS30to10_to_T62_ndtos.20180901.nc'
       4. 'map_oRRS30to10_to_T62_nstod.20180901.nc'
       5. 'map_oRRS30to10_to_T62_patc.20180901.nc'
       6. 'map_T62_to_oRRS30to10_aave.20180901.nc'
       7. 'map_T62_to_oRRS30to10_blin.20180901.nc'
       8. 'map_T62_to_oRRS30to10_ndtos.20180901.nc'
       9. 'map_T62_to_oRRS30to10_nstod.20180901.nc'
       10. 'map_T62_to_oRRS30to10_patc.20180901.nc'
     The ordering of source and destination grids is immaterial for ERWG
     maps since MWF-mode produces all map combinations.  However, as
     described above in the TempestRemap section, the Tempest
     overlap-mesh generator must be called with the smaller grid
     preceding the larger grid.  For this reason, always invoke MWF-mode
     with the smaller grid (i.e., the ocean) as the source, otherwise
     some Tempest map-file will fail to generate.  The six optimized
     SE<->FV Tempest maps described above in the TempestRemap section
     will be generated when the destination grid has a '.g' suffix which
     'ncremap' interprets as indicating an Exodus-format SE grid (NB:
     this assumption is an implementation convenience that can be
     modified if necessary).  For example,
          ncremap -P mwf -s ocean.RRS.30-10km_scrip_150722.nc -g ne30.g \
                  --nm_src=oRRS30to10 --nm_dst=ne30np4 --dt_sng=20180901
     produces the 6 TempestRemap map-files:
       1. 'map_oRRS30to10_to_ne30np4_monotr.20180901.nc'
       2. 'map_oRRS30to10_to_ne30np4_highorder.20180901.nc'
       3. 'map_oRRS30to10_to_ne30np4_mono.20180901.nc'
       4. 'map_ne30np4_to_oRRS30to10_mono.20180901.nc'
       5. 'map_ne30np4_to_oRRS30to10_highorder.20180901.nc'
       6. 'map_ne30np4_to_oRRS30to10_intbilin.20180901.nc'
     MWF-mode takes significant time to complete (~20 minutes on my
     MacBookPro) for the above grids.  To accelerate this, consider
     installing the MPI-enabled instead of the serial version of ERWG.
     Then use the '--wgt_cmd' option to tell 'ncremap' the MPI
     configuration to invoke ERWG with, for example:
          ncremap -P mwf --wgt_cmd='mpirun -np 12 ESMF_RegridWeightGen' \
            -s ocean.RRS.30-10km_scrip_150722.nc -g t62_SCRIP.20150901.nc \
            --nm_src=oRRS30to10 --nm_dst=T62 --dt_sng=20180901
     Background and distributed node parallelism (as described above in
     the the Parallelism section) of MWF-mode are possible though not
     yet implemented.  Please let us know if this feature is desired.

     RRG-mode:
     EAM and CAM-SE will produce regional output if requested to with
     the 'finclNlonlat' namelist parameter.  Output for a single region
     can be higher temporal resolution than the host global simulation.
     This facilitates detailed yet economical regional process studies.
     Regional output files are in a special format that we call RRG (for
     "regional regridding").  An RRG file may contain any number of
     rectangular regions.  The coordinates and variables for one region
     do not interfere with other (possibly overlapping) regions because
     all variables and dimensions are named with a per-region suffix
     string, e.g., 'lat_128e_to_134e_9s_to_16s'.  'ncremap' can easily
     regrid RRG output from an atmospheric FV-dycore because 'ncremap'
     can infer (as discussed above) the regional grid from any
     rectangular FV data file.  Regridding regional SE data, however, is
     more complex because SE gridcells are essentially weights without
     vertices and SE weight-generators are not yet flexible enough to
     output regional weights.  To summarize, regridding RRG data leads
     to three SE-specific difficulties (#1-3 below) and two difficulties
     (#4-5) shared with FV RRG files:

       1. RRG files contain only regional gridcell center locations, not
          weights
       2. Global SE grids have well-defined weights not vertices for
          each gridpoint
       3. Grid generation software (ESMF and TempestRemap) only create
          global not regional SE grid files
       4. Non-standard variable names and dimension names
       5. Regional files can contain multiple regions

     'ncremap''s RRG mode resolves these issues to allow trouble-free
     regridding of SE RRG files.  The user must provide two additional
     input arguments, '--dat_glb=DAT_GLB' (or synonynms '--rrg_dat_glb',
     '--data_global', or '--global_data') and '--grd_glb=GRD_GLB' (or
     synonyms '--rrg_grd_glb', '--grid_global', or 'global_grid') that
     point to a global SE dataset and grid, respectively, of the same
     resolution as the model that generated the RRG datasets.  Hence a
     typical RRG regridding invocation is:
          ncremap --dat_glb=dat_glb.nc --grd_glb=grd_glb.nc -g grd_rgn.nc \
                  dat_rgn.nc dat_rgr.nc
     Here 'grd_rgn.nc' is a regional destination grid-file, 'dat_rgn.nc'
     is the RRG file to regrid, and 'dat_rgr.nc' is the regridded
     output.  Typically 'grd_rgn.nc' is a uniform rectangular grid
     covering the same region as the RRG file.  Generate this as
     described in the last example in the section that describes Manual
     Grid-file Generation with the '-G' option.  'grd_glb.nc' is the
     standard dual-grid grid-file for the SE resolution, e.g.,
     'ne30np4_pentagons.091226.nc'.  'ncremap' regrids the global data
     file 'dat_glb.nc' to the global dual-grid in order to produce a
     intermediate global file annotated with gridcell vertices.  Then it
     hyperslabs the lat/lon coordinates (and vertices) from the regional
     domain to use with regridding the RRG file.  A 'grd_glb.nc' file
     with only one 2D field suffices (and is fastest) for producing the
     information needed by the RRG procedure.  One can prepare an
     optimal 'dat_glb.nc' file by subsetting any 2D variable from any
     full global SE output dataset with, e.g., 'ncks -v FSNT in.nc
     dat_glb.nc'.

     'ncremap' RRG mode supports two additional options to override
     internal parameters.  First, the per-region suffix string may be
     set with '--rnm_sng=rnm_sng' (or synonyms '--rrg_rnm_sng' or
     '--rename_string').  RRG mode will, by default, regrid the first
     region it finds in an RRG file.  Explicitly set the desired region
     with RNM_SNG for files with multiple regions, e.g.,
     '--rnm_sng=_128e_to_134e_9s_to_16s'.  Second, the regional
     bounding-box may be explicitly set with
     '--bb_wesn=lon_wst,lon_est,lat_sth,lat_nrt'.  The normal parsing of
     the bounding-box string from the suffix string may fail in (as yet
     undiscovered) corner cases, and the '--bb_wesn' option provides a
     workaround should that occur.  The bounding-box string must include
     the entire RRG region, specified in WESN order.  The two override
     options may be used independently or together, as in:
          ncremap --rnm_sng='_128e_to_134e_9s_to_16s' --bb_wesn='128,134,-16,-9' \
                  --dat_glb=dat_glb.nc --grd_glb=grd_glb.nc -g grd_rgn.nc \
                  dat_rgn.nc dat_rgr.nc

     RRG-mode supports most normal 'ncremap' options, including input
     and output methods and regridding algorithms.

     SGS-mode:
     In sub-grid mode, 'ncremap' performs substantial pre- and
     post-processing to conserve fields that represent fractional parts
     of a gridcell.  The sub-gridscale (SGS) fraction usually changes
     spatially with the geographic distribution of ice, ocean,
     vegetation, and so on.  Spatial fields output by most geophysical
     models are intensive, and so by default the regridder attempts to
     conserve the integral of the area times the field value such that
     the integral is equal on source and destination grids.  However
     some models (like ELM, CLM, and CICE) output gridcell values
     intended to apply to only a fraction SGS_FRC (which stands for
     "sub-gridscale fraction") of the gridcell.  For concreteness
     consider a sub-grid field that represents the land fraction.  Land
     fraction is less than one in gridcells that resolve coastlines or
     islands.  ELM and CLM) happily output temperature values valid only
     for a small (i.e., SGS_FRC << 1) island within the larger gridcell.
     Regridding weights that assume SGS_FRC = 1 everywhere would create
     subtle, difficult-to-notice, biases in coastal regions that, on
     Earth, would alter global mean values (e.g., of temperature,
     fluxes) by up to a few percent at common grid resolutions.  Though
     models like ELM and CLM may run on the same horizontal grid as the
     overlying atmosphere, they cannot be regridded with the
     atmosphere's grid weights that account for the whole gridcell
     instead of the sub-grid land fraction.

     To correctly regrid sub-grid data, specify the names of the SGS_FRC
     and SGS_MSK variables.  (if they differ from their respective
     defaults 'landfrac' and 'landmask').  Trouble will ensue if SGS_FRC
     is a percentage, or an absolute area.  'ncremap' must know the
     normalization factor SGS_NRM by which SGS_FRC must be _divided_
     (not multiplied) to obtain a true, normalized fraction (between
     zero and one).  Datasets (such as those from CICE) that store
     SGS_FRC in percent should specify the option '--sgs_nrm=100' to
     instruct 'ncremap' to normalize the sub-grid area appropriately
     before regridding.  'ncremap' will re-derive SGS_MSK based on the
     regridded values of SGS_FRC: SGS_MSK = 1 is assigned to destination
     gridcells with SGS_FRC > 0.0, and all others SGS_MSK = 0.  As of
     NCO version 4.6.8 (released June, 2017), invoking any of the
     options '--sgs_frc', '--sgs_msk', or '--sgs_nrm', automatically
     triggers SGS-mode, so that also invoking '-P sgs' is redundant
     though legal.  As of NCO version 4.9.0 (released December, 2019),
     the values of the SGS_FRC and SGS_MSK variables should be
     explicitly specified.  In previous versions they defaulted to
     'landfrac' and 'landmask', respectively, when '-P sgs' was
     selected.  This behavior still exists but will likely be deprecated
     in a future version.

     The 'area' and SGS_FRC fields in the regridded file will be in
     units of sterradians and fraction, respectively.  However,
     'ncremap' offers custom options to reproduce the idiosyncratic data
     and metadata format of two particular models, ELM and CICE.  When
     invoked with '-P elm' (or '-P clm'), a final step converts the
     output 'area' from sterradians to square kilometers.  When invoked
     with '-P cice', the final step converts the output 'area' from
     sterradians to square meters, and the output 'sgs_frc' from a
     fraction to a percent.
          # ELM/CLM: output "area" in [sr]
          ncremap --sgs_frc=landfrac --sgs_msk=landmask in.nc out.nc
          ncremap -P sgs in.nc out.nc # Deprecated in 4.9.0
          # ELM/CLM pedantic format: output "area" in [km2]
          ncremap -P elm in.nc out.nc # Same as -P clm, alm, ctsm

          # CICE: output "area" in [sr]
          ncremap --sgs_frc=aice --sgs_msk=tmask --sgs_nrm=100 in.nc out.nc
          # CICE pedantic format: output "area" in [m2], "aice" in [%]
          ncremap -P cice in.nc out.nc

          # MPAS-Seaice: both commands are equivalent
          ncremap -P mpasseaice in.nc out.nc
          ncremap --sgs_frc=timeMonthly_avg_iceAreaCell in.nc out.nc
     It is sometimes convenient to store the SGS_FRC field in an
     external file from the field(s) to be regridded.  For example,
     CMIP-style timeseries are often written with only one variable per
     file.  NCO supports this organization by accepting SGS_FRC
     arguments in the form of a filename followed by a slash and then a
     variable name:
          ncremap --sgs_frc=sgs_landfrac_ne30.nc/landfrac -m map.nc in.nc out.nc

     Files regridded using explicitly specified SGS options will differ
     slightly from those regridded using the '-P elm' or '-P cice'
     options.  The former will have an 'area' field in sterradians, the
     generic units used internally by the regridder.  The latter
     produces model-specific 'area' fields in square kilometers (for
     ELM) or square meters (for CICE), as expected in the raw output
     from these two models.  To convert from angular to areal values,
     NCO assumes a spherical Earth with radius 6,371,220 m or
     6,371,229 m, for ELM) and CICE, respectively.  The ouput SGS_FRC
     field is expressed as a decimal fraction in all cases except for
     '-P cice' which stores the fraction in percent.  Thus the generic
     SGS and model-specific convenience options produce equivalent
     results, and the latter is intended to be indistinguishable (in
     terms of metadata and units) to raw model output.  This makes it
     more interoperable with many existing analysis scripts.

     During development of this procedure, we learned that regridding
     sub-grid values correctly versus incorrectly (i.e., with and
     without '-P sgs', respectively) alters global-mean answers for
     land-based quantities by about 1% for typical grid resolutions on
     Earth.  This is small enough to be easily overlooked.

'-p PAR_TYP ('--par_typ', '--par_md', '--parallel_type', '--parallel_mode', '--parallel')'
     Specifies the parallelism mode desired.  Parallelism accelerates
     throughput when regridding multiple files in one 'ncremap'
     invocation.  Valid types are 'bck' (for Background mode), 'mpi'
     (for MPI mode), and 'nil' (for none).  The default PAR_TYP is
     'nil', which means 'ncremap' will run in serial mode, and process
     one file at a time.  Background and MPI parallelism modes both
     issue all regridding commands at one time.  In Background mode
     these commands are spawned as UNIX background processes on the
     local node.  Nodes with mutiple cores will take advantage of this
     and simultaneously execute regridding commands.  In MPI mode these
     commands are issued round-robin fashion to all the compute nodes
     available to the job.  Typically both parallel modes scale well
     with sufficent CPUs until I/O contention becomes the bottleneck.
     Furthermore, a naming conflict among intermediate files still
     exists (see Limitations, below) so parallel mode is currently only
     supported when all source files share the same grid.

'--preserve=PRS_STT ('--preserve', '--prs_stt', '--preserve_statistic')'
     This is a simple, intuitive option to specify how weight
     application should treat destination gridcells that are not
     completely overlapped by source gridcells with valid values.
     Destination gridcells that are completely overlapped by valid
     source values are unaffected.  The two statistics that can be
     preserved for incompletely overlapped gridcells are the mean and
     the integral of the source values.  Hence the two valid values for
     this option are 'integral' and 'mean'.  Specifying
     '--preserve=INTEGRAL' sets the destination gridcell equal to the
     sum of the source weights times the source values.  This is exactly
     equivalent to setting '--rnr=off', i.e., no renormalization (see
     *note Regridding::).  Specifying '--preserve=INTEGRAL' sets the
     destination gridcell equal to the sum of the source weights times
     the source values.  If the weights were generated by a conservative
     algorithm then the output will be conservative.  This is often
     desired for regridding quantities that should be conserved, e.g.,
     fluxes, and is the default weight application method in 'ncremap'
     (except in MPAS-mode).  Specifying '--preserve=MEAN' sets the
     destination gridcell equal to the mean of the source weights times
     the source values.  This is exactly equivalent to setting
     '--rnr=0.0', i.e., renormalizing the integral value by the
     fractional area covered (see *note Regridding::).  This is often
     desired for regridding state variables, e.g., temperature, though
     it is not the default behavior and must be explicitly requested
     (except in MPAS-mode).

'-R RGR_OPT ('--rgr_opt', '--regrid_options')'
     'ncremap' passes RGR_OPT directly through to the regridder.  This
     is useful to customize output grids and metadata.  One use is to
     rename output variables and dimensions from the defaults provided
     by or derived from input data.  The default value is '--rgr
     lat_nm_out=lat --rgr lon_nm_out=lon', i.e., by default 'ncremap'
     always names latitude and longitude "lat" and "lon", respectively,
     regardless of their input names.  Users might use this option to
     set different canonical axes names, e.g., '--rgr lat_nm_out=y --rgr
     lon_nm_out=x'.

'-r RNR_THR ('--rnr_thr', '--thr_rnr', '--rnr', '--renormalize', '--renormalization_threshold')'
     Use this option to request renormalized (see *note Regridding::)
     weight-application and to specify the weight threshold, if any.
     For example, '-r 0.9' tells the regridder to renormalize with a
     weight threshold of 90%, so that all destination gridcells with at
     least 90% of their area contributed by valid source gridcells will
     be contain valid (not missing) values that are the area-weighted
     mean of the valid source values.  If the weights are conservative,
     then the output gridcells on the destination grid will preserve the
     mean of the input gridcells.  Specifying '-r 0.9' and
     '--rnr_thr=0.9' are equivalent.  Renormalization can be explicitly
     turned-off by setting RNR_THR to either of the values 'off', or
     'none'.  The '--preserve=PRS_STT' option performs the same task as
     this option except it does not allow setting an arbitrary threshold
     fraction.

'--rgn_dst ('--rgn_dst', '--dst_rgn', '--regional_destination')'
'--rgn_src ('--rgn_src', '--src_rgn', '--regional_source')'
     Use these flags which take no argument to indicate that a
     user-supplied (i.e., with '-s GRD_SRC' or '-g GRD_DST') grid is
     regional.  The ERWG weight-generator needs to be told whether the
     source, destination, or both grids are regional or global in order
     to optimize weight production.  'ncremap' supplies this information
     to the regridder for grids it automatically infers from data files.
     However, the regridder needs to be explicitly told if user-supplied
     (i.e., with either '-s GRD_SRC' or '-g GRD_DST') grids are regional
     because the regridder does not examine supplied grids before
     calling ERWG which assumes, unless told otherwise, that grids are
     global in extent.  The sole effect of these flags is to add the
     arguments '--src_regional' and/or '--dst_regional' to ERWG calls.
     Supplying regional grids without invoking these flags may
     dramatically increase the map-file size and time to compute.
     According to E3SM MPAS documentation, ERWG "considers a mesh to be
     regional when the mesh is not a full sphere (including if it is
     planar and does not cover the full sphere).  In other words, all
     MPAS-O and MPAS-LI grids are regional" to ERWG.

'-s GRD_SRC ('--grd_src', '--grid_source', '--source_grid', '--src_grd')'
     Specifies the source gridfile.  NCO will use ERWG or TempestRemap
     weight-generator to combine this with a destination gridfile
     (either inferred from DST_FL, or generated by supplying a '-G
     GRD_SNG' option) to generate remapping weights.  GRD_SRC is not
     modified, and may have read-only permissions.  One appropriate
     circumstance to specify GRD_SRC is when the INPUT-FILE(s) do not
     contain sufficient information for NCO to infer an accurate or
     complete source grid.  (Unfortunately many dataset producers do not
     record information like cell edges/vertices in their datasets.
     This is problematic for non-rectangular grids.)  NCO assumes that
     GRD_SRC, when supplied, applies to every INPUT-FILE.  Thus NCO will
     call the weight generator only once, and will use that MAP_FL to
     regrid every INPUT-FILE.

     Although 'ncremap' usually uses the contents of a pre-existing
     GRD_SRC to create mapping weights, there are some situations where
     'ncremap' creates the file specified by GRD_SRC (i.e., treats it as
     a location for storing output).  When a source grid is inferred or
     created from other user-specified input, 'ncremap' will store it in
     the location specified by GRD_SRC.  This allows users to, for
     example, name the grid on which an input dataset is stored when
     that grid is not known _a priori_.  This functionality is only
     available for SCRIP-format grids.

'--skl_fl=SKL_FL ('--skl_fl', '--skl', '--skl_fl')'
     Normally 'ncremap' only creates a SCRIP-format gridfile named
     GRD_DST when it receives the 'grd_sng' option.  The '--skl' option
     instructs 'ncremap' to also produce a "skeleton" file based on the
     'grd_sng' argument.  A skeleton file is a bare-bones datafile on
     the specified grid.  It contains the complete latitude/longitude
     grid and an area field.  Skeleton files are useful for validating
     that the grid-creation instructions in GRD_SNG perform as expected.

'--no_stdin ('--no_stdin', '--no_inp_std', '--no_redirect', '--no_standard_input')'
     First introduced in NCO version 4.8.0 (released May, 2019), this
     switch (which takes no argument) disables checking standard input
     (aka 'stdin') for input files.  This is useful because 'ncclimo'
     and 'ncremap' may mistakenly expect input to be provided on 'stdin'
     in environments that use 'stdin' for other purposes.  Some
     non-interactive environments (e.g., 'crontab', 'nohup', Azure CI,
     CWL), use standard input for their own purposes, and thus confuse
     NCO into thinking that you provided the input files names via the
     'stdin' mechanism.  In such cases users may disable the automatic
     checks for standard input by explicitly invoking the '--no_stdin'
     flag.  This switch is never required for jobs run in an interactive
     shell.

'-T TMP_DRC ('--tmp_drc', '--drc_tmp', '--tmp_dir', '--dir_tmp', '--tmp_drc')'
     Specifies the directory in which to place intermediate output
     files.  Depending on how it is invoked, 'ncremap' may generate a
     few or many intermediate files (grids and maps) that it will, by
     default, remove upon successful completion.  These files can be
     large, so the option to set TMP_DRC is offered to ensure their
     location is convenient to the system.  If the user does not specify
     TMP_DRC, then 'ncremap' uses the value of '$TMPDIR', if any, or
     else '/tmp' if it exists, or else it uses the current working
     director ('$PWD').

'-t THR_NBR ('--thr_nbr', '--thr', '--thread_number', '--threads')'
     Specifies the number of threads used per regridding process (*note
     OpenMP Threading::).  The NCO regridder scales well up to 8-16
     threads.

'-U ('--unpack', '--upk', '--upk_inp')'
     This switch (which takes no argument) causes 'ncremap' to unpack
     (see *note Packed data::) input data before regridding it.  This
     switch causes unpacking at the regridding stage that occurs after
     map generation.  Hence this switch does not benefit grid inferral.
     Grid inferral examines only the coordinate variables in a dataset.
     If coordinates are packed (a terrible practice) in a file from
     which a grid will be inferred, users should first manually unpack
     the file (this option will not help).  Fortunately, coordinate
     variables are usually not packed, even in files with other packed
     data.

     Many institutions (like NASA) pack datasets to conserve space
     before distributing them.  This option allows one to regrid input
     data without having to manually unpack it first.  Beware that NASA
     uses at least three different and incompatible versions of packing
     in its L2 datasets.  The unpacking algorithm employed by this
     option is the default netCDF algorithm, which is appropriate for
     MOD04 and is inappropriate for MOD08 and MOD13.  See *note Packed
     data:: for more details and workarounds.

'--ugrid_fl=UGRID_FL ('--ugrid_fl', '--ugrid', '--ugrid_fl')'
     Normally 'ncremap' only infers a gridfile named GRD_DST in
     SCRIP-format.  The 'ugrid_fl' option instructs 'ncremap' to infer
     both a SCRIP-format gridfile named GRD_DST and a UGRID-format
     gridfile named UGRID_FL.  This is an experimental feature and the
     UGRID file is only expected to be valid for global rectangular
     grids.

'-u UNQ_SFX ('--unq_sfx', '--unique_suffix', '--suffix')'
     Specifies the suffix used to label intermediate (internal) files
     generated by the regridding workflow.  Unique names are required to
     avoid interference among parallel invocations of 'ncremap'.  The
     default UNQ_SFX is '.pidPID.ncremap.tmp', where PID is the process
     ID. Applications that invoke ncremap can provide more or less
     informative suffixes.  The suffix should be unique so that no two
     simultaneously executing instances of 'ncremap' can generate the
     same file.  For instance, a climatology script that issues a dozen
     'ncremap' commands may find it useful to encode the climatological
     month in the unique suffix.  If UNQ_SFX is 'noclean' then 'ncremap'
     retains (not removes) all intermediate files after completion.

'-v VAR_LST ('--var_lst', '--var', '--vars', '--variables', '--variable_list')'
     The '-v' option causes 'ncremap' to regrid only the variables in
     VAR_LST.  3<It behaves like subsetting (*note Subsetting Files::)
     in the rest of NCO.

'-V VAR_RGR ('--var_rgr', '--rgr_var', '--var_cf', '--cf_var', 'cf_variable')'
     The '-V' option tells 'ncremap' to use the same grid as VAR_RGR in
     the input file.  If VAR_RGR adheres to the CF 'coordinates'
     convention described here
     (http://cfconventions.org/cf-conventions/cf-conventions.html#coordinate-system),
     then 'ncclimo' will infer the grid as represented by those
     coordinate variables.  This option simplifies inferring grids when
     the grid coordinate names are unknown, since 'ncclimo' will follow
     the CF convention to learn the identity of the grid coordinates.

     Until NCO version 4.6.0 (May, 2016), 'ncremap' would not follow CF
     conventions to identify coordinate variables.  Instead, 'ncremap'
     used an internal database of "usual suspects" to identify latitude
     and longitude coordinate variables.  Now, if VAR_RGR is
     CF-compliant, then 'ncremap' will automatically identify the
     horizontal spatial dimensions.  If VAR_RGR is supplied but is not
     CF-compliant, then 'ncremap' will still attempt to identify
     horizontal spatial dimensions using its internal database of
     "likely names".  If both these automated methods fail, manually
     supply 'ncremap' with the names of the horizontal spatial
     dimensions
          # Method used to obtain horizontal spatial coordinates:
          ncremap -V var_rgr -d dst.nc -O ~/rgr in.nc # CF coordinates convention
          ncremap -d dst.nc -O ~/rgr in.nc # Internal database
          ncremap -R "--rgr lat_nm=xq --rgr lon_nm=zj" -d dst.nc -O ~/rgr in.nc # Manual

'--vrb=VRB_LVL ('--vrb_lvl', '--vrb', '--verbosity', '--verbosity_level')'
     Specifies a verbosity level similar to the rest of NCO.  If VRB_LVL
     = 0, 'ncremap' prints nothing except potentially serious warnings.
     If VRB_LVL = 1, 'ncremap' prints the basic filenames involved in
     the remapping.  If VRB_LVL = 2, 'ncremap' prints helpful comments
     about the code path taken.  If VRB_LVL > 2, 'ncremap' prints even
     more detailed information.  Note that VRB_LVL is distinct from
     DBG_LVL which is passed to the regridder ('ncks') for additional
     diagnostics.

'--vrt_fl=VRT_FL ('--vrt_fl', '--vrt', '--vrt_crd', '--vertical_file')'
     The '--vrt_fl=VRT_FL' option instructs 'ncremap' to vertically
     interpolate the input file to the vertical coordinate grid
     contained in the file VRT_FL.  This option first appeared in NCO
     version 4.8.0, released in May, 2019.  The vertical gridfile VRT_FL
     must specify a vertical gridtype that 'ncremap' understands,
     currently either pure-pressure or hybrid-coordinate pressure.  We
     plan to add pure-sigma coordinates in the future.

     Besides the vertical grid-type, the main assumptions, constraints,
     and priorities for future development of vertical regridding are:
       1. Input datasets must have netCDF (and thus C-based)
          dimension-ordering _all other dimensions, a single vertical
          dimension, then one or two horizontal dimensions_ so that the
          horizontal dimension(s) vary more rapidly than the vertical.
          Eliminating this constraint will remain low priority until we
          are lobbied with compelling use-cases.
       2. The vertical interpolation algorithm defaults to linear in
          log(pressure).  This assumption is more natural for gases
          (like the atmosphere) than for condensed media (like oceans or
          Earth's interior).  To instead interpolate linearly in the
          vertical coordinate, use the 'ntp_mth=lin' options (as of NCO
          4.9.0).
       3. Vertical interpolation and horizontal regridding may be
          invoked simultaneously (as of NCO 4.9.0) by the user simply by
          supplying both a map-file and a vertical grid-file to
          'ncremap'.  When this occurs, 'ncremap' internally performs
          the vertical interpolation prior to the horizontal regridding.
       4. The default extrapolation method uses nearest neighbor except
          for temperature and geopotential (those extrapolation methods
          are described below).  These defaults are well-suited to
          extrapolate valid initial conditions from data on older
          vertical grids.  Note that the default approximation used for
          geopotential is inaccurate in cold regions.  As of July 2019
          and NCO version 4.8.1, one may instead set points outside the
          input domain to missing-values with the '--xtr_opt=mss_val'
          option.  Other extrapolation options, not yet exposed to
          user-access, include: dying, setting to 0.0, and linear
          extrapolation.  Supporting these other methods, or improving
          the existing special-case approximations for temperature or
          geopotential, will remain low priority until we are lobbied
          with compelling use-cases for other algorithms.
       5. Missing values are not (yet) treated specially Eliminating
          this constraint is not a priority because atmospheric datasets
          often contain no missing data.  This could become a high
          priority issue if ocean modelers show interest in employing
          this tool to regrid to/from depth coordinates where missing
          values indicate bathymetry.
       6. Time-varying vertical grids are only allowed for hybrid grids
          (not pure pressure grids), and these must store the time
          dimension as a record dimension.  This constraint applies to
          the vertical grid only, not to the other fields in the
          dataset.  Hence this does not preclude interpolating
          timeseries to/from time-invariant vertical grids.  For
          example, time-varying hybrid grid data such as temperature may
          be interpolated to timeseries on a time-invariant pressure
          grid.  Eliminating this constraint will not be a priority
          unless/until an important use-case is identified.
       7. Variable names for input and output vertical grids must match
          E3SM/CESM, ECMWF, and NCEP implementations.  These names
          include 'hyai', 'hyam', 'hybi', 'hybm', 'ilev', 'lev', 'P0',
          and 'PS' (for E3SM/CESM hybrid grids), 'lev', 'lev_2', and
          'lnsp' (for ECMWF hybrid grids only), and 'plev' (for
          pure-pressure grids).  The infrastructure to provide alternate
          names for any of these input/output variables names is
          straightforward, and is heavily used for horizontal spatial
          regridding.  Allowing this functionality will not be a
          priority until we are presented with a compelling use-case.

     The simplest vertical grid-type, a pure-pressure grid, contains the
     horizontally uniform vertical pressure levels in a one-dimensional
     coordinate array named 'plev'.  The 'plev' dimension may have any
     number of levels and the values must monotonically increase or
     decrease.  A 17-level NCEP pressure grid, for example, is easy to
     create:
          # Construct monotonically decreasing 17-level NCEP pressure grid
          ncap2 -O -v -s 'defdim("plev",17);plev[$plev]={100000,92500,85000, \
            70000,60000,50000,40000,30000,25000,20000,15000,10000,7000,5000, \
            3000,2000,1000};' vrt_prs_ncep_L17.nc

     Hybrid-coordinate grids are a hybrid between a sigma-coordinate
     grid (where each pressure level is a fixed fraction of a
     spatiotemporally varying surface pressure) and a pure-pressure grid
     that is spatially invariant (as described above).  The so-called
     hybrid A and B coefficients specify the fractional weight of the
     pure-pressure and sigma-grids, respectively, at each level.  The
     hybrid gridfile must specify A and B coefficients for both layer
     midpoints and interfaces with these standard (as employed by CESM
     and E3SM) names and dimensions: 'hyai(ilev)', 'hybi(ilev)',
     'hyam(lev)', and 'hybm(lev)'.  The reference pressure and surface
     pressure must be named 'P0' and 'PS', respectively.  The pressures
     at all midpoints and interfaces are then defined as
          prs_mdp[time,lev, lat,lon]=hyam*P0+hybm*PS # Midlayer
          prs_ntf[time,ilev,lat,lon]=hyai*P0+hybi*PS # Interface
     The scalar reference pressure 'P0' is typically 100000 Pa (or
     1000 mb) while the surface pressure 'PS' is a (possibly
     time-varying) array with one or two spatial dimensions, and its
     values are in the same dimensional units (e.g., Pa or hPa) as 'P0'.

     It is often useful to create a vertical grid file from existing
     model or reanalysis output.  We call vertical grid files "skinny"
     if they contain only the vertical information.  Skinny grid-files
     are easy to create with 'ncks', e.g.,
          ncks -C -v hyai,hyam,hybi,hybm,P0 in_L128.nc vrt_hyb_L128.nc
     Such files are extremely small and portable, and represent all the
     hybrid files created by the model because the vertical grid
     parameters are time-invariant.  A "fat" vertical grid file would
     also include the time-varying grid information, i.e., the surface
     pressure field.  Fat grid-files are also easy to create with
     'ncks', e.g.,
          ncks -C -v hyai,hyam,hybi,hybm,P0,PS in_L128.nc vrt_hyb_L128.nc
     The full (layer-midpoint) and half (layer-interface) pressure
     fields 'prs_mdp' and 'prs_ntf', respectively, can be reconstructed
     from any fat grid-file with an 'ncap2' command:
          ncap2 -s 'prs_mdp[time,lat,lon,lev]=P0*hyam+PS*hybm' \
                -s 'prs_ntf[time,lat,lon,ilev]=P0*hyai+PS*hybi' in.nc out.nc

     Hybrid-coordinate grids define a pure-sigma or pure-pressure grid
     when either their A or B coefficients are zero, respectively.  For
     example, the following creates the hybrid-coordinate representation
     of a pure-pressure grid with midpoints every 100 mb from 100 mb to
     1000 mb:
          ncap2 -O -v -s 'defdim("ilev",11);defdim("lev",10);P0=100000.0; \
            hyai=array(0.05,0.1,$ilev);hyam=array(0.1,0.1,$lev); \
            hybi=0.0*hyai;hybm=0.0*hyam;' vrt_hyb_prs_L10.nc
     NCO currently has no other means of representing pure sigma
     vertical grids (as opposed to pure pressure grids).

     As of July 2019 and NCO version 4.8.1, NCO supports regridding
     ECMWF datasets in IFS hybrid vertical coordinate format to
     CESM/E3SM-format hybrid vertical grids.  The native IFS hybrid
     datasets that we have seen store pressure coordinates in terms of a
     slightly different formula that employs the log of surface pressure
     ('lnsp') instead of surface pressure 'PS', that redefines 'hyai'
     and 'hyam' to be pure-pressure offsets (rather than coefficients),
     and that omits 'P0':
          prs_mdp[time,lev,  lat,lon]=hyam+hybm*exp(lnsp) # Midlayer
          prs_ntf[time,lev_2,lat,lon]=hyai+hybi*exp(lnsp) # Interface
     Note that ECMWF also alters the names of the vertical half-layer
     coordinate and employs distinct dimensions ('nhym' and 'nhyi') for
     the hybrid variables 'hyai(nhyi)', 'hybi(nhyi)', 'hyam(nhym)', and
     'hybm(nhym)'.  ECMWF uses the vertical coordinates 'lev' and
     'lev_2' for full-layer (i.e., midlayer) and half-layer (i.e.,
     interface) for all other variables.

     The 'lev' and 'ilev' coordinates of a hybrid grid are defined by
     the hybrid coefficients and reference pressure, and are by
     convention stored in millibars (not Pascals) as follows:
          ilev[ilev]=P0*(hyai+hybi)/100.0;
          lev[lev]=P0*(hyam+hybm)/100.0;

     A vertical hybrid grid file VRT_FL must contain at least 'hyai',
     'hybi', 'hyam', 'hybm(lev)' and 'P0'; 'PS', 'lev', and 'ilev' are
     optional.  (Exceptions for ECMWF grids are noted above).  All
     hybrid-coordinate data files must contain 'PS'.  Interpolating a
     pure-pressure coordinate data file to hybrid coordinates requires,
     therefore, that the hybrid-coordinate VRT_FL must contain 'PS'
     and/or the input data file must contain 'PS'.  If both contain 'PS'
     then the 'PS' from the VRT_FL takes precedence and will be used to
     construct the hybrid grid and then copied without to the output
     file.

     In all cases 'lev' and 'ilev' are optional in input
     hybrid-coordinate data files and vertical grid-files.  They are
     diagnosed from the other parameters using the above definitions.
     The minimal requirements--a 'plev' coordinate for a pure-pressure
     grid or five parameters for a hybrid grid--allow vertical gridfiles
     to be much smaller than horizontal gridfiles such as SCRIP files.
     Moreover, data files from ESMs or analyses (NCEP, MERRA2, ERA5) are
     also valid gridfiles.  The flexibility in gridfile structure makes
     it easy to intercompare data from the same or different sources.

     'ncremap' supports vertical interpolation between all combinations
     of pure-pressure and hybrid-pressure grids.  The input and output
     (aka source and destination) pressure grids may monotonically
     increase or decrease independently of eachother (i.e., one may
     increase and the other may decrease).  When an output pressure
     level is outside the input pressure range for that column, then all
     variables must be extrapolated (not interpolated) to that/those
     level(s).  By default 'ncremap' sets all extrapolated values to the
     nearest valid value.

     Temperature and geopotential height are exceptions to this rule.
     Temperature variables (those named 'T' or 'ta', anyway) are
     extrapolated upwards towards space using the nearest neighbor
     assumption, and downwards beneath the surface assuming a moist
     adiabatic lapse rate of 6.5 degrees centigrade per 100 millibars.
     Geopotential variables (those named 'Z3' or 'zg', anyway) are
     extrapolated upwards and downwards using the hypsometric equation
     (4) with constant global mean virtual temperature T = 288K. This
     assumption leads to unrealistic values where T differs
     significantly from the global mean surface temperature.  Using the
     local T itself would be a much better approximation, yet would
     require a time-consuming implementation.  Please let us know if
     accurate surface geopotential extrapolation in cold regions is
     important to you.

     Interpolation to and from hybrid coordinate grids works on both
     midpoint and interface fields (i.e., on variables with 'lev' or
     'ilev' dimensions), while interpolation to and from pure-pressure
     grids applies to fields with, or places output of fields on, a
     'plev' dimension.  All other fields pass through the interpolation
     procedure unscathed.  Input can be rectangular (aka RLL),
     curvilinear, or unstructured.

'--vrt_ntp=VRT_NTP ('--vrt_ntp', '--ntp_mth', '--interpolation_type', '--interpolation_method')'
     Specifies the interpolation method for destination points within
     the vertical range of the input data during vertical interpolation.
     Valid values and their synonyms are 'lin' (synonyms 'linear' and
     'lnr'), and 'log' (synonyms 'logarithmic' and 'lgr').  Default is
     VRT_NTP = 'log'.  The vertical interpolation algorithm defaults to
     linear in log(pressure).  Logarithmic interpolation is more natural
     for gases like the atmosphere, because it is compressible, than for
     condensed media like oceans or Earth's interior, which are
     incompressible.  To instead interpolate linearly in the vertical
     coordinate, use the 'ntp_mth=lin' option.  NCO supports this
     feature as of version 4.9.0 (December, 2019).

'--vrt_xtr=VRT_XTR ('--vrt_xtr', '--xtr_mth', '--extrapolation_type', '--extrapolation_method')'
     Specifies the extrapolation method for destination points outside
     the vertical range of the input data during vertical interpolation.
     Valid values and their synonyms are 'mss_val' (synonyms 'msv' and
     'missing_value'), and 'nrs_ngh' (synonyms 'nn' and
     'nearest_neighbor').  Default is VRT_XTR = 'nrs_ngh'.  NCO supports
     this feature as of version 4.8.1 (July, 2019).

'-W WGT_OPT ('--wgt_opt', '--weight_options', '--esmf_opt', '--esmf_options', '--tps_opt', '--tempest_options')'
     'ncremap' passes WGT_OPT directly through to the weight-generator
     (currently ERWG or TempestRemap's 'GenerateOfflineMap') (and not to
     'GenerateOverlapMesh').  The user-specified contents of WGT_OPT, if
     any, supercede the default contents for the weight-generator.  The
     default option for ERWG is '--ignore_unmapped').  'ncremap' 4.7.7
     and later additionally set the ERWG '--ignore_degenerate' option,
     though if the run-time ERWG reports its version is 7.0 (March,
     2018) or later.  This is done to preserve backwards compatibility
     since, ERWG 7.1.0r and later require '--ignore_degenerate' to
     successfully regrid some datasets (e.g., CICE) that previous ERWG
     versions handle fine.  Users of earlier versions of 'ncremap' that
     call ESMF 7.1.0r and later can explicitly pass the base ERWG
     options with 'ncremap''s '--esmf_opt' option:
          # Use when NCO <= 4.7.6 and ERWG >= 7.1.0r
          ncremap --esmf_opt='--ignore_unmapped --ignore_degenerate' ...

     The ERWG and TempestRemap documentation shows all available
     options.  For example, to cause ERWG to output to a netCDF4 file,
     pass '-W "--netcdf4"' to 'ncremap'.

     By default, 'ncremap' runs 'GenerateOfflineMap' without any
     options.  To cause 'GenerateOfflineMap' to use a '_FillValue' of
     -1, pass '-W '--fillvalue -1.0'' to 'ncremap'.  Other common
     options include enforcing monotonicity (which is not the default in
     TempestRemap) constraints.  To guarantee monotonicity in regridding
     from Finite Volume FV to FV maps (e.g., MPAS-to-rectangular), pass
     '-W '-in_np 1'' to 'ncremap'.  To guarantee monotonicity in
     regridding from Finite Element FE to FV maps, pass '-W '--mono''.
     Common sets of specialized options recommended for TempestRemap are
     collected into six boutique algorithms invokable with '--alg_typ'
     as described above.

'-w WGT_CMD ('--wgt_cmd', '--weight_command', '--wgt_gnr', '--weight_generator')'
     Specifies a (possibly extended) command to use to run the
     weight-generator when a map-file is not provided.  This command
     overrides the default executable executable for the weight
     generator, which is 'ESMF_RegridWeightGen' for ESMF and
     'GenerateOfflineMap' for TempestRemap.  (There is currently no way
     to override 'GenerateOverlapMesh' for TempestRemap).  The WGT_CMD
     must accept the same arguments as the default command.  Examples
     include 'mpirun -np 24 ESMF_RegridWeightGen', 'mpirun-openmpi-mp
     -np 16 ESMF_RegridWeightGen', and other ways of exploiting
     parallelism that are system-dependent.  Specifying WGT_CMD and
     supplying (with '-m') a map-file is not permitted (since the
     weight-generator would not be used).

'--xcl_var ('--xcl_var', '--xcl', '--exclude', '--exclude_variables')'
     This flag (which takes no argument) changes VAR_LST, as set by the
     '--var_lst' option, from an extraction list to an exclusion list so
     that variables in VAR_LST will not be processed, and variables not
     in VAR_LST will be processed.  Thus the option '-v VAR_LST' must
     also be present for this flag to take effect.  Variables explicitly
     specified for exclusion by '--xcl --vars=VAR_LST[,...]' need not be
     present in the input file.

'-x XTN_LST ('--xtn_lst', '--xtn_var', '--var_xtn', '--extensive', '--extensive_variables')'
     The '-x' option causes 'ncremap' to treat the variables in XTN_LST
     as "extensive", meaning that their value depends on the gridcell
     boundaries.  Support for extensive variables during regridding is
     nascent.  Currently variables marked as extensive are summed, not
     regridded.  We are interested in "real-world" situations that
     require regridding extensive variables, please contact us if you
     have one.

Limitations to 'ncremap'
------------------------

'ncremap' has two significant limitations to be aware of.  First, for
two-dimensional input grids the fields to be regridded must have
latitude and longitude, or, in the case of curvilinear data, the two
equivalent horizontal dimensions, as the final two dimensions in IN_FL.
Fields with other dimension orders (e.g., 'lat,lev,lon') will not regrid
properly.  To circumvent this limitation one can employ 'ncpdq' (*note
ncpdq netCDF Permute Dimensions Quickly::) to permute the dimensions
before (and un-permute them after) regridding.  'ncremap' utilizes this
method internally for some common input grids.  For example,
     # AIRS Level2 vertical profiles
     ncpdq -a StdPressureLev,GeoTrack,GeoXTrack AIRS_L2.hdf AIRS_L2_ncpdq.nc
     ncremap -i AIRS_L2_ncpdq.nc -d dst_1x1.nc -O ~/rgr
     # MPAS-O fields
     ncpdq -a Time,nVertLevels,maxEdges,MaxEdges2,nEdges,nCells mpas.nc mpas_ncpdq.nc
     ncremap -R "--rgr col_nm=nCells" -i mpas_ncpdq.nc -m mpas120_to_t62.nc -O ~/rgr
The previous two examples occur so frequently that 'ncremap' has been
specially equipped to handle AIRS and MPAS files.  As of NCO version
4.5.5 (February, 2016), the following 'ncremap' commands with the '-P
PRC_TYP' option automagically perform all required permutation and
renaming necessary:
     # AIRS Level2 vertical profiles
     ncremap -P airs -i AIRS_L2.nc -d dst_1x1.nc -O ~/rgr
     # MPAS-O/I fields
     ncremap -P mpas -i mpas.nc -m mpas120_to_t62.nc -O ~/rgr
The machinery to handle permutations and special options for other
datafiles is relatively easy to extend with new PRC_TYP options.  If you
work with common datasets that could benefit from their own
pre-processing options, contact us and we will try to implement them.

   The second limitation is that 'ncremap' must read weights from an
on-disk mapfile, and cannot yet compute weights itself and use them
directly from RAM.  This makes 'ncremap' an "offline regridder" and
unnecessarily slow compared to an "integrated regridder" that computes
weights and immediately applies them in RAM without any disk-access.  In
practice, the difference is most noticeable when the weights are easily
computable "on the fly", e.g., rectangular-to-rectangular mappings.
Otherwise the weight-generation takes much more time than the
weight-application, at which 'ncremap' is quite fast.  To circumvent
this limitation, build 'ESMF_RegridWeightGen' with parallel capabilities
and ask me to enhance 'ncremap' to call ERWG with the options to exploit
those capabilities.  As of NCO version 4.8.0, released in May, 2019, the
regridder can generate intersection grids and overlap weights for all
finite volume grid combinations.  However these weights are first stored
in an offline mapfile, are not usable otherwise.

   A side-effect of 'ncremap' being an offline regridder is that, when
necessary, it generates intermediate files to store grids and maps.
These files are named, by default, 'ncremap_tmp_grd_dst.nc''${unq_sfx}',
'ncremap_tmp_grd_src.nc''${unq_sfx}',
'ncremap_tmp_gnr_out.nc''${unq_sfx}',
'ncremap_tmp_map_*.nc''${unq_sfx}',
'ncremap_tmp_msh_ovr_*.nc''${unq_sfx}', and
'ncremap_tmp_pdq.nc''${unq_sfx}'.  They are placed in DRC_OUT with the
output file(s).  In general, no intermediate grid or map files are
generated when the map-file is provided.  Intermediate files are always
generated when the '-P PRM_TYP' option is invoked.  By default these
files are automatically removed upon successful completion of the
script.  Early or unexpected termination of 'ncremap' leaves these
intermediate files behind.  Should intermediate files proliferate and/or
annoy you, locate and/or remove all such files under the current
directory with
     find . -name 'ncremap_tmp*'
     rm `find . -name 'ncremap_tmp*'`

EXAMPLES

   Regrid input file 'in.nc' to the spatial grid in file 'dst.nc' and
write the output to 'out.nc':
     ncremap -d dst.nc in.nc out.nc
     ncremap -d dst.nc -i in.nc -o out.nc
     ncremap -d dst.nc -O regrid in.nc out.nc
     ncremap -d dst.nc in.nc regrid/out.nc
     ncremap -d dst.nc -O regrid in.nc # output named in.nc
NCO infers the destination spatial grid from 'dst.nc' by reading its
coordinate variables and CF attributes.  In the first example, 'ncremap'
places the output in 'out.nc'.  In the second and third examples, the
output file is 'regrid/out.nc'.  In the fourth example, 'ncremap' places
the output in the specified output directory.  Since no output filename
is provided, the output file will be named 'regrid/in.nc'.

   Generate a mapfile with 'ncremap' and store it for later re-use.  A
pre-computed mapfile (supplied with '-m MAP_FL') eliminates
time-consuming weight-generation, and thus considerably reduces
wallclock time:
     ncremap -m map.nc in.nc out.nc
     ncremap -m map.nc -I drc_in -O regrid

   As of NCO version 4.7.2 (January, 2018), 'ncremap' supports
"canonical" argument ordering of command line arguments most frequently
desired for one-off regridding, where a single input and output filename
are supplied as command-line positional arguments without switches,
pipes, or redirection:
     ncremap -m map.nc in.nc out.nc # Requires 4.7.2+
     ncremap -m map.nc -i in.nc -o out.nc
     ncremap -m map.nc -o out.nc in.nc
     ncremap -m map.nc -O out_dir in1.nc in2.nc
     ncremap -m map.nc -o out.nc < in.nc
     ls in.nc | ncremap -m map.nc -o out.nc
   These are all equivalent methods, but the canonical ordering shown in
the first example only works in NCO version 4.7.2 and later.

   'ncremap' annotates the gridfiles and mapfiles that it creates with
helpful metadata containing the full provenance of the command.
Consequently, 'ncremap' is a sensible tool for generating mapfiles for
later use.  To generate a mapfile with the specified (non-default) name
'map.nc', and then regrid a single file,
     ncremap -d dst.nc -m map.nc in.nc out.nc

   To test the remapping workflow, regrid only one or a few variables
instead of the entire file:
     ncremap -v T,Q,FSNT -m map.nc in.nc out.nc
   Regridding generally scales linearly with the size of data to be
regridded, so eliminating unnecessary variables produces a snappier
response.

   Regrid multiple input files with a single mapfile 'map.nc' and write
the output to the 'regrid' directory:
     ncremap -m map.nc -I drc_in -O regrid
     ls drc_in/*.nc | ncremap -m map.nc -O regrid
The three ways NCO obtains the destination spatial grid are, in
decreasing order of precedence, from MAP_FL (specified with '-m'), from
GRD_DST (specified with '-g'), and (inferred) from DST_FL (specified
with '-d').  In the first example all likely data files from DRC_IN are
regridded using the same specified mapfile, MAP_FL = 'map.nc'.  Each
output file is written to DRC_OUT = 'regrid' with the same name as the
corresponding input file.  The second example obtains the input file
list from standard input, and uses the mapfile and output directory as
before.

   If multiple input files are on the same grid, yet the mapfile does
not exist in advance, one can still regrid all input files without
incurring the time-penalty of generating multiple mapfiles.  To do so,
provide the (known-in-advance) source gridfile or toggle the '-M'
switch:
     ncremap -M -I drc_in -d dst.nc -O regrid
     ls drc_in/*.nc | ncremap -M -d dst.nc -O regrid
     ncremap -I drc_in -s grd_src.nc -d dst.nc -O regrid
     ls drc_in/*.nc | ncremap -s grd_src.nc -d dst.nc -O regrid
     ncremap -I drc_in -s grd_src.nc -g grd_dst.nc -O regrid
     ls drc_in/*.nc | ncremap -s grd_src.nc -g grd_dst.nc -O regrid
The first two examples explicitly toggle the multi-map-generation switch
(with '-M'), so that 'ncremap' refrains from generating multiple
mapfiles.  In this case the source grid is inferred from the first input
file, the destination grid is inferred from 'dst.nc', and 'ncremap' uses
ERWG to generate a single mapfile and uses that to regrid every input
file.  The next four examples are variants on this theme.  In these
cases, the user provides (with '-s grd_src.nc') the source gridfile,
which will be used directly instead of being inferred.  Any of these
styles works well when each input file is known in advance to be on the
same grid, e.g., model data for successive time periods in a simulation.

   The most powerful, time-consuming (yet simultaneously time-saving!)
feature of 'ncremap' is its ability to regrid multiple input files on
unique grids.  Both input and output can be on any CRUD grid.
     ncremap -I drc_in -d dst.nc -O regrid
     ls drc_in/*.nc | ncremap -d dst.nc -O regrid
     ncremap -I drc_in -g grd_dst.nc -O regrid
     ls drc_in/*.nc | ncremap -g grd_dst.nc -O regrid
There is no pre-supplied MAP_FL or GRD_SRC in these examples, so
'ncremap' first infers the output grid from 'dst.nc' (first two
examples), or directly uses the supplied gridfile 'grd_dst' (second two
examples), and calls ERWG to generate a new mapfile for each input file,
whose grid it infers.  This is necessary when each input file is on a
unique grid, e.g., swath-like data from satellite observations or models
with time-varying grids.  These examples require remarkably little
input, since 'ncremap' automates most of the work.

   Finally, 'ncremap' uses the parallelization options '-p PAR_TYP' and
'-j JOB_NBR' to help manage high-volume workflow.  On a single node such
as a local workstation, use Background mode to regrid multiple files in
parallel
     ls drc_in/*.nc | ncremap -p bck -d dst.nc -O regrid
     ls drc_in/*.nc | ncremap -p bck -j 4 -d dst.nc -O regrid
Both examples will eventually regrid all input files.  The first example
regrids two at a time because two is the default batch size 'ncremap'
employs.  The second example regrids files in batches of four at a time.
Increasing JOB_NBR will increase throughput so long as the node is not
I/O-limited.

   Multi-node clusters can exploit inter-node parallelism in MPI-mode:
     qsub -I -A CLI115 -V -l nodes=4 -l walltime=03:00:00 -N ncremap
     ls drc_in/*.nc | ncremap -p mpi -j 4 -d dst.nc -O regrid
This example shows a typical request for four compute nodes.  After
receiving the login prompt from the interactive master node, execute the
'ncremap' command with '-p mpi'.  'ncremap' will send regridding jobs in
round-robin fashion to all available compute nodes until all jobs
finish.  It does this by internally prepending an MPI execution command,
like 'mpirun -H NODE_NAME -npernode 1 -n 1', to the usual regridding
command.  MPI-mode typically has excellent scaling because most nodes
have independent access to hard storage.  This is the easiest way to
speed your cumbersome job by factors of ten or more.  As mentioned above
under Limitations, parallelism is currently only supported when all
regridding uses the same map-file.

   ---------- Footnotes ----------

   (1) This means that newer (including user-modified) versions of
'ncremap' work fine without re-compiling NCO.  Re-compiling is only
necessary to take advantage of new features or fixes in the NCO
binaries, not to improve 'ncremap'.  One may download and give
executable permissions to the latest source at
<https://github.com/nco/nco/tree/master/data/ncremap> without
re-installing the rest of NCO.

   (2) Install the Conda NCO package with 'conda install -c conda-forge
nco'.

   (3) As of version 4.7.6 (August, 2018)), NCO's syntax for gridfile
generation is much improved and streamlined, and is the syntax described
here.  This is also called "Manual Grid-file Generation".  An earlier
syntax (described at *note Grid Generation::) accessed through 'ncks'
options still underlies the new syntax, though it is less user-friendly.
Both old and new syntax work well and produce finer rectangular grids
than any other software we know of.

   (4) Z_2-Z_1=(R_d*T_v/g_0)*ln(p_1/p_2)=(R_d*T_v/g_0)*(ln(p_1)-ln(p_2))

